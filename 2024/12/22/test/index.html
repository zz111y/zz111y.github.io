<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>test | zz111y&#39;s stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="以torchvision中的CIFAR10数据集为例，介绍一个简单网络模型的训练方法。 CIFAR10介绍CIFAR10是torchvision中给出的数据集，包含了60000张32x32的3通道彩色图像，这些图片被分为了10类，每类有6000张。其中，train集有50000张，test集有10000张。官方文档 网络模型构建这里以现有模型结构进行构建，以下是现有模型结构的示意图（来源：Stru">
<meta property="og:type" content="article">
<meta property="og:title" content="test">
<meta property="og:url" content="http://example.com/2024/12/22/test/index.html">
<meta property="og:site_name" content="zz111y&#39;s stack">
<meta property="og:description" content="以torchvision中的CIFAR10数据集为例，介绍一个简单网络模型的训练方法。 CIFAR10介绍CIFAR10是torchvision中给出的数据集，包含了60000张32x32的3通道彩色图像，这些图片被分为了10类，每类有6000张。其中，train集有50000张，test集有10000张。官方文档 网络模型构建这里以现有模型结构进行构建，以下是现有模型结构的示意图（来源：Stru">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2024/12/22/test/images/2024/06/3798127767.png">
<meta property="article:published_time" content="2024-12-22T07:31:40.000Z">
<meta property="article:modified_time" content="2024-12-22T08:29:41.409Z">
<meta property="article:author" content="zz111y">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2024/12/22/test/images/2024/06/3798127767.png">
  
    <link rel="alternate" href="/atom.xml" title="zz111y's stack" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zz111y&#39;s stack</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/12/22/test/" class="article-date">
  <time class="dt-published" datetime="2024-12-22T07:31:40.000Z" itemprop="datePublished">2024-12-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/ML/">ML</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      test
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>以torchvision中的CIFAR10数据集为例，介绍一个简单网络模型的训练方法。</p>
<h2 id="CIFAR10介绍"><a href="#CIFAR10介绍" class="headerlink" title="CIFAR10介绍"></a>CIFAR10介绍</h2><p>CIFAR10是torchvision中给出的数据集，包含了60000张32x32的3通道彩色图像，这些图片被分为了10类，每类有6000张。其中，train集有50000张，test集有10000张。<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~kriz/cifar.html">官方文档</a></p>
<h2 id="网络模型构建"><a href="#网络模型构建" class="headerlink" title="网络模型构建"></a>网络模型构建</h2><p>这里以现有模型结构进行构建，以下是现有模型结构的示意图（来源：<a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Structure-of-CIFAR10-quick-model_fig2_312170477">Structure of CIFAR10-quick model.</a>）<br><img src="images/2024/06/3798127767.png" alt="Structure of CIFAR10-quick model"><br>在此模型中，输入以3channel 32x32的格式输入，依次经过以下变化过程：</p>
<ol>
<li>经过一个<strong>5x5 kernel的Convolution</strong>，转化为32channel 32x32；</li>
<li>经过一个<strong>2x2 kernel的Max-Pooling</strong>，转化为32channel 16x16；</li>
<li>经过一个<strong>5x5 kernel的Convolution</strong>，转化为32channel 32x32；</li>
<li>经过一个<strong>2x2 kernel的Max-Pooling</strong>，转化为32channel 8x8；</li>
<li>经过一个<strong>5x5 kernel的Convolution</strong>，转化为64channel 8x8；</li>
<li>经过一个<strong>2x2 kernel的Max-Pooling</strong>，转化为64channel 4x4；</li>
<li>经过<strong>flatten</strong>操作后，展平为一个64x4x4的线性数据；</li>
<li>经过一个<strong>Linear</strong>，输出为64；</li>
<li>经过一个<strong>Linear</strong>，输出为10（也即<strong>对应类别的概率</strong>）。</li>
</ol>
<p>根据以上模型的构建，可以定义一个model类（需要继承nn.module)，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">testModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(testModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br>[scode type=”yellow”]模型的输入必须是一个<strong>3 channel 32x32的batch</strong>。如果不满足尺寸要求或不是一个batch，都将报错！[/scode]</p>
<h2 id="数据集的获取"><a href="#数据集的获取" class="headerlink" title="数据集的获取"></a>数据集的获取</h2><p>使用torchvision中的datasets可以获取指定模型。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>对参数的理解为：</p>
<ul>
<li>root：数据集存放的位置；</li>
<li>train：为True时加载train数据集，为False时加载test数据集；</li>
<li>transform：对数据集中每个元素进行的初始化操作，这里将每个元素转化为了Tensor；</li>
<li>download：为True时，当本地不存在数据集时，会自动下载；为False时，不会自动下载。</li>
</ul>
<p>然后，将数据集分为多个batch，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><br>至此，数据集初始化部分完成。</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>首先定义简单模型参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model = testModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数——交叉熵</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率和优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前epoch和总epoch</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义tensorboard，记录训练过程</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br></pre></td></tr></table></figure><br>模型训练的步骤如下（对于每个epoch）：<br>首先将模型调整为train模式，对于某些层（如Dropout和BatchNorm）在train模式和eval模式下作为会有所不同，因此好的习惯是不管包含哪些层，都注意train模式和eval模式的改变<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br></pre></td></tr></table></figure><br>然后对训练集dataLoader中的每一个batch进行训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="comment"># 取出一个batch的数据</span></span><br><span class="line">    imgs, label = data</span><br><span class="line">    <span class="comment"># 将batch传给模型，得到batch的输出</span></span><br><span class="line">    outputs = model(imgs)</span><br><span class="line">    <span class="comment"># 计算此batch的损失</span></span><br><span class="line">    loss = loss_fn(outputs, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清空梯度，防止梯度积累</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播，计算损失对模型参数的梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 根据计算得到的梯度更新模型参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 写入tensorboard</span></span><br><span class="line">    total_train_step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;, train loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, epochs, loss.item()))</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;loss&quot;</span>, loss.item(), total_train_step)</span><br></pre></td></tr></table></figure><br>模型验证的步骤如下（对于每个epoch）：<br>首先将模型调整为train模式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><br>然后对测试集dataloader中每一个batch进行验证：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定当前batch总损失和准确率</span></span><br><span class="line">total_test_loss = <span class="number">0</span></span><br><span class="line">total_test_accuracy = <span class="number">0</span></span><br><span class="line"><span class="comment"># 验证时不需要反向传播，禁用梯度计算，加快训练速度</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        img, label = data</span><br><span class="line">        outputs = model(img)</span><br><span class="line">        loss = loss_fn(outputs, label)</span><br><span class="line">        total_test_loss += loss.item()</span><br><span class="line">        <span class="comment"># 计算当前批次中预测正确的样本数</span></span><br><span class="line">        accuracy = (outputs.argmax(<span class="number">1</span>) == label).<span class="built_in">sum</span>()</span><br><span class="line">        total_test_accuracy += accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入tensorboard</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total test loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total test accuracy: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_accuracy))</span><br><span class="line">writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_train_step)</span><br><span class="line">writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_test_accuracy, total_train_step)</span><br><span class="line">total_test_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><br>最后，保存模型。这里我们每个epoch保存一个模型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, <span class="string">&quot;test_model&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;save model&quot;</span>)</span><br></pre></td></tr></table></figure><br>至此，一个简单模型的训练结束。下面给出整体代码：<br>model.py文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">testModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(testModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = testModel()</span><br><span class="line">    _<span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    output = model(_<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>train.py文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train data size:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test data size:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">model = testModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, epochs))</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        imgs, label = data</span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        loss = loss_fn(outputs, label)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;, train loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, epochs, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_test_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            img, label = data</span><br><span class="line">            outputs = model(img)</span><br><span class="line">            loss = loss_fn(outputs, label)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == label).<span class="built_in">sum</span>()</span><br><span class="line">            total_test_accuracy += accuracy</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total test loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total test accuracy: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_accuracy))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_train_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_test_accuracy, total_train_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    torch.save(model, <span class="string">&quot;test_model&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;save model&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="利用gpu进行训练"><a href="#利用gpu进行训练" class="headerlink" title="利用gpu进行训练"></a>利用gpu进行训练</h2><p>运行上面程序效率较低，这是因为我们单纯使用cpu进行计算。cpu对于处理大规模简单计算的效率远远不如gpu，因此接下来介绍使用gpu的训练方法。此方法需要设备配备nvidia显卡、安装cuda以及cudnn。<br>检测当前环境是否能使用gpu进行训练，使用以下语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><br>支持gpu训练，返回True，否则返回False。</p>
<h3 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h3><p>要使用gpu进行训练，需要对上述代码作出以下修改：<br>模型初始化部分原始代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = testModel()</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><br>利用gpu版本的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = testModel()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()</span><br></pre></td></tr></table></figure><br>模型训练部分及验证部分原始代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img, label = data</span><br></pre></td></tr></table></figure><br>利用gpu版本代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">imgs, label = data</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    imgs = imgs.cuda()</span><br><span class="line">    label = label.cuda()</span><br></pre></td></tr></table></figure></p>
<h3 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h3><p>首先指定训练设备：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure><br>然后在对应部分加上如下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型定义部分</span></span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 损失函数部分</span></span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"><span class="comment"># 获取dataloader中batch部分</span></span><br><span class="line">imgs = imgs.to(device)</span><br><span class="line">label = label.to(device)</span><br></pre></td></tr></table></figure><br>使用gpu加速训练后，可以发现，训练效率明显加速。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/12/22/test/" data-id="cm4zannya0000ip3k3zj3547u" data-title="test" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/12/22/test/">test</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 zz111y<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
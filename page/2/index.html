<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>zz111y&#39;s stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ">
<meta property="og:type" content="website">
<meta property="og:title" content="zz111y&#39;s stack">
<meta property="og:url" content="https://zz111y.github.io/page/2/index.html">
<meta property="og:site_name" content="zz111y&#39;s stack">
<meta property="og:description" content="ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zz111y">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="zz111y's stack" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zz111y&#39;s stack</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zz111y.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-week3-Why-Deep" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/week3-Why-Deep.html" class="article-date">
  <time class="dt-published" datetime="2024-07-31T11:22:00.000Z" itemprop="datePublished">2024-07-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/week3-Why-Deep.html">[week3]Why Deep</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>week2我们讲到这样一张图：<br><img src="images/2024/07/2792548885.png" alt=""><br>本文将探讨<strong>Deep Learning</strong>是如何做到<strong>在一个小的|ℋ|的情况下，仍做到有一个很小的Loss（即$L(h^{all},D_{all})$）</strong>。</p>
<h1 id="Why-Hidden-Layer"><a href="#Why-Hidden-Layer" class="headerlink" title="Why Hidden Layer"></a>Why Hidden Layer</h1><p>我们知道，ML的任务是找到一个function，而这个function可以近似为一个<strong>分段Linear的function</strong>，而<strong>只要Activate function（例如sigmoid、ReLU）足够多，就能表示这样一个function</strong>。那么看起来似乎只要<strong>一层非常fat的structure就足够了</strong>，为什么要deep呢？<br><img src="images/2024/07/2416590996.png" alt="Fat + Short v.s. Thin + Tall"><br>下面来看实验结果：<br><img src="images/2024/07/1564636475.png" alt="SeideFrank, Gang Li, and Dong Yu. &quot;Conversational Speech Transcription Using Context-Dependent Deep Neural Networks.&quot;Interspeech.2011."><br>可以看到，在<strong>parameter数量相同</strong>的情况下，deep network确实要比fat network表现的要好很多。这是为什么呢？</p>
<p>观察下面这张图片：<br><img src="images/2024/07/223762964.png" alt="deep network"><br>以此类推，假设我们有<strong>$2K$个neural</strong>，那么我们就能represent一条<strong>$2^K pieces$组成的function</strong>。可是如果只使用一层Layer，那么每个neural只能表示一个piece，那么要represent同样的function，需要<strong>$2^K$个neural</strong>。<br><img src="images/2024/07/3615953620.png" alt="deep v.s. shallow"><br>[scode type=”green”]虽然一个Hidden Layer能represent任何function<br>但<strong>deep structure更加effective</strong>[/scode]<br>同时，由于deep structure的<strong>neural少</strong>，那么<strong>parameter也会更少</strong>，train起来就<strong>需要较小的|ℋ|</strong>。</p>
<hr>
<p>但是这时候又有一个疑问：对于复杂的function，deep structure的效果是好，但是对于简单的function，会不会shallow structure效果更好呢？<br>答案是不会的。有人做过实验：就算是$y=x^2$，deep structure的效果还是要更好。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/week3-Why-Deep.html" data-id="cm4zg7wzd000cwe3k0vok74dc" data-title="[week3]Why Deep" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-week3-extra-CNN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/week3-extra-CNN.html" class="article-date">
  <time class="dt-published" datetime="2024-07-30T16:00:00.000Z" itemprop="datePublished">2024-07-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/week3-extra-CNN.html">[week3 extra]CNN</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>考虑做<strong>影像辨识</strong>，对于一张<strong>彩色照片</strong>，在PyTorch中的表达为一个<strong>3-D Tensor</strong>（宽-高-channel，通常三个channel代表RGB）。如果使用fully-connected-network，由于其feature非常多，parameter的数量也会非常大，很容易导致<strong>overfitting</strong>。因此，需要对model做出修改。</p>
<h2 id="Receptive-Field"><a href="#Receptive-Field" class="headerlink" title="Receptive Field"></a>Receptive Field</h2><p>考虑如下方法：<br><img src="images/2024/07/1611002021.png" alt="Receptive Field"><br>我们让<strong>每个neural只关心特定的一个区域</strong>（Receptive Field），如下图所示：<br><img src="images/2024/07/4282853410.png" alt="Receptive Field"><br>对于Receptive Field，可以有很多<strong>设计方法</strong>：不同大小、不同形状、不同channel…完全取决于个人需求。<br>[scode type=”green”]最常见的Receptive Field：<strong>all channels,3×3 kernel size</strong>[/scode]<br><img src="images/2024/07/3962272193.png" alt="typical setting"><br>其中的stride是kernel的<strong>移动步长</strong>，padding为kernel移出image后补的数据（通常为0）。</p>
<h2 id="Parameter-Sharing"><a href="#Parameter-Sharing" class="headerlink" title="Parameter Sharing"></a>Parameter Sharing</h2><p>某一个特征（比如鸟喙）可能<strong>出现在图片的不同位置</strong>，因此需要在每个位置都需要设置一个“鸟喙detector”，这两个detector的<strong>parameter是完全一样的</strong>，确保它们能detect到同样的东西。这就叫<strong>Parameter Sharing</strong>。<br>对于每个Perceptive Field，有<strong>多个neural负责detect</strong>，每个Perceptive Field的neural共享相同的parameter；同一个Perceptive Field的neural的parameter是不同的。<br>每一组共享parameter的neural叫做一个<strong>filter</strong>。<br><img src="images/2024/07/1704772374.png" alt="Parameter Sharing"></p>
<h2 id="Convolutional-Layer"><a href="#Convolutional-Layer" class="headerlink" title="Convolutional Layer"></a>Convolutional Layer</h2><p>总结一下，Fully Connected Network是最没有限制的Model；加上Receptive Field会增加一定的限制，缩小Model的范围；再加上Parameter Sharing后，其Model的范围会更小。<br>[scode type=”yellow”]Convolutional Layer的output叫做<strong>feature map</strong>[/scode]<br><img src="images/2024/07/2171939834.png" alt="Convolutional Layer"><br>虽然加上Convolutional Layer，虽然Model bias更大了，但其是专门为<strong>影像辨识</strong>产生的，因此用在影像辨识上，效果很好。<br>[scode type=”green”]<strong>Receptive Field</strong> + <strong>Parameter Sharing</strong> = <strong>Convolutional Layer</strong><br>使用了Convolutional Layer的network就叫做<strong>CNN</strong>[/scode]</p>
<h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>对于一张image，对其做<strong>pixel subsampling</strong>后（例如去除偶数的行列pixel），不会影响它是什么东西，subsampling就是<strong>Pooling</strong>。我们可以发现，Pooling的操作不需要学习什么parameter，因此很多人觉得它更像一个Activate Function。<br>以<strong>Max Pooling</strong>举例，它会在一个范围内选择其中最大的元素作为代表：<br><img src="images/2024/07/2464747740.png" alt="Max Pooling"></p>
<h2 id="CNN实际应用"><a href="#CNN实际应用" class="headerlink" title="CNN实际应用"></a>CNN实际应用</h2><p>在实际进行影像辨识时，通常会选择<strong>Convolutional Layer + Pooling</strong>的方法。举例来说，对于一张输入的image，先通过一系列的Convolutional Layer和Pooling的组合，在丢进一个Fully Connected Network，最后经过一个Flatten和softmax，得到一个列向量，表示辨识结果。<br>实际上，<strong>Alpha Go</strong>也是使用了CNN。具体原理可以查看原论文<a target="_blank" rel="noopener" href="https://www.nature.com/articles/nature16961">《Mastering the game of Go with deep neural networks and tree search》</a></p>
<h2 id="Spatial-Transformer"><a href="#Spatial-Transformer" class="headerlink" title="Spatial Transformer"></a>Spatial Transformer</h2><p>CNN对于<strong>缩放和旋转</strong>图片的效果并不好，而Spatial Transformer能够解决CNN的这一问题。<br><img src="images/2024/07/2978398766.png" alt="Spatial Transformer"><br>由于Spatial Transformer也是network种的一层，因此training的同时，同样可以使用Back Propagation的方法学习到其中的parameters。</p>
<p>那么Spatial Transformer是如何操作的呢？举个例子：<br><img src="images/2024/07/904148656.png" alt="Spatial Transformer"><br>更普遍的来说，Spatial Transformer的原理如下：<br><img src="images/2024/07/1679205279.png" alt="Spatial Transformer"><br>Spatial Transformer会<strong>output六个parameters</strong>$a,b,c,d,e,f$，这六个parameters决定了要<strong>对当前input做何种变换</strong>。<br>但实际中，output$a,b,c,d,e,f$的值不会是整数，这就导致$x^\prime,y^\prime$不会得到一个整数，解决方法为：<strong>Interpolation</strong>。<br><img src="images/2024/07/2123913151.png" alt="2024-07-30T15:47:10.png"><br>这样做相比于<strong>直接使用近的坐标</strong>的好处是：可以进行Gradient Descent。</p>
<p>实际应用举例：<br><img src="images/2024/07/2588050740.png" alt="RNN+Spatial Transformer"><br><img src="images/2024/07/1670807104.png" alt="RNN+Spatial Transformer"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/week3-extra-CNN.html" data-id="cm4zg7wzd000fwe3kao2m4q6u" data-title="[week3 extra]CNN" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-算法-二分" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86.html" class="article-date">
  <time class="dt-published" datetime="2024-07-30T12:48:00.000Z" itemprop="datePublished">2024-07-30</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Algorithm/">Algorithm</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86.html">[算法] 二分</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>二分算法是一种在<strong>有序序列</strong>种查找目标值的高效算法，每次迭代都会将区间<strong>缩短一半</strong>，因此假设序列长度为N，其时间复杂度则为<em>log(N)</em><br>当<strong>问题的答案具有单调性</strong>时，就可以通过二分<strong>把求解转化为判定</strong>。本文重点讨论<strong>整数域上的二分</strong>，因为整数二分有很多细节需要处理，且只使用<strong>一种常见的二分实现方法</strong>进行讨论。</p>
<h1 id="模板"><a href="#模板" class="headerlink" title="模板"></a>模板</h1><p>假设问题保证答案在闭区间[l,r]以内，循环以l=r结束，有两种常见的二分需求：</p>
<ol>
<li>在单<strong>单调递增序列a</strong>中找到<strong>≥x的第一个</strong>（即<strong>x或x的后继</strong>）：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">while (l &lt;= r)</span><br><span class="line">&#123;</span><br><span class="line">    int mid = (l + r) &gt;&gt; 1;</span><br><span class="line">    if (a[mid] &gt;= x) r = mid;</span><br><span class="line">    else l = mid + 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<strong>解释</strong>：若<code>a[mid] &gt;= x</code>，那么mid之后的数会更大，则≥x的第一个数不会出现在mid之后，因此答案只会出现在[l,mid]中（包括mid）；反之同理，答案只会出现在(mid,r]中（不包括mid）。</li>
<li>在单<strong>单调递增序列a</strong>中找到<strong>≤x的最后一个</strong>（即<strong>x或x的前驱</strong>）：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">while (l &lt;= r)</span><br><span class="line">&#123;</span><br><span class="line">    int mid = (l + r + 1) &gt;&gt; 1;</span><br><span class="line">    if (a[mid] &lt;= x) l = mid;</span><br><span class="line">    else r = mid - 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<strong>解释</strong>：若<code>a[mid] &lt;= x</code>，那么mid之前的数会更小，则≤x的组后一个数不会出现在mid之前，因此答案只会出现在[mid,r]中（包括mid）；反之同理，答案只会出现在[l,mid)中（不包括mid）。</li>
</ol>
<p>以上两种方法可以看作是：<br>第一种是<strong>将区间划分为了[l,mid]和[mid+1,r]</strong>；<br>第二种是<strong>将区间划分为了[l,mid-1]和[mid,r]</strong>。<br>[scode type=”yellow”]第二种方法必须使用<code>mid = l + r + 1 &gt;&gt; 1</code><br>因为当r-l=1时，如果接下来进入<code>l = mid</code>分支，如果<code>mid = l + r &gt;&gt; 1</code><br>那么更新完后就相当于<code>l = l</code>，陷入死循环[/scode]<br>[scode type=”yellow”]<strong>使用&gt;&gt;1的原因</strong><br>/2是向零取整，&gt;&gt;1是向下取整。如果二分区间包含负数，那么/2无法正常运作[/scode]</p>
<p>实际操作的思路是：</p>
<ol>
<li>分析具体问题，确定左右半段<strong>哪一个是解存在的区间</strong>，以及<strong>mid归属哪一半</strong>；</li>
<li>根据分析结果，选择上述模板之一（mid加不加一取决于<strong>区间的划分情况</strong>，<code>l = mid</code>的情况下一定要加一）；</li>
<li>二分终止条件<code>l==r</code>，此即答案所在位置。</li>
</ol>
<p>二分的难点在于：</p>
<ul>
<li>check函数的写法（即模板中<code>a[mid] &lt;= x</code>部分）</li>
<li>区间划分的选择</li>
</ul>
<p>要真正理解二分，需要通过做题来体会。下面放几道例题</p>
<h1 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h1><h2 id="洛谷P2678-NOIP2015-提高组-跳石头"><a href="#洛谷P2678-NOIP2015-提高组-跳石头" class="headerlink" title="洛谷P2678 NOIP2015 提高组 跳石头"></a><a target="_blank" rel="noopener" href="https://www.luogu.com.cn/problem/P2678">洛谷P2678 NOIP2015 提高组 跳石头</a></h2><p><strong>大致题意</strong><br>给一个数轴，给出上面几个点的坐标，现规定能移走其中M个点，要使得数轴上<strong>最近的两个点距离尽可能远</strong>。</p>
<p><strong>分析</strong><br>假设起点终点距离为L，我们可以发现，答案是在一个范围内的，也就是(0,L]。因此可以遍历这个范围内就能找到答案。但数据范围$1≤L≤10^9$，暴搜明显超时了，因此需要优化一下。<br>我们发现，对于这道题目，解空间存在一个<strong>阈值</strong>，即阈值一侧的所有解<strong>都是可行的</strong>，另一侧的所有解<strong>都是非法的</strong>。更具体的来说，<strong>最近的两个点距离</strong>有一个值，比这个值小的都能做到，比这个值大的都做不到（这个值也可以做到）。做不做得到就取决于<strong>拿走点的数量是否合乎规定</strong>，那么就可以对其进行<strong>二分</strong>。</p>
<p>首先考虑<strong>check函数</strong>：</p>
<ul>
<li>如果在规定数量内可以做到就返回True；</li>
<li>如果在规定数量内无法实现就返回False。</li>
</ul>
<p>再考虑<strong>区间的划分</strong>：</p>
<ul>
<li>如果一个值返回True，那么代表比他小的值都能做到，我们要找<strong>最大的能做到的值</strong>，因此<strong>目标一定在这个值右侧包括这个值</strong>，因此<code>l = mid</code>；</li>
<li>如果一个值返回False，那么代表<strong>可行解一定比他小</strong>，即在这个值左侧，因此<code>r = mid - 1</code>；</li>
</ul>
<p>同时，注意到我们使用了<code>l = mid</code>，因此mid的取法要<strong>配套使用</strong><code>mid = l + r + 1 &gt;&gt; 1</code></p>
<p>下面就具体考虑check函数如何实现。对于一个给定的<strong>最近点的距离</strong>，我们只需要遍历一遍所有点，对于所有<strong>距离小于给定值的两点</strong>，移走靠后的点，如果移走的数量大于规定值，就返回False。</p>
<p>经过以上分析，可以写出<strong>代码</strong>：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">const int N = 50010;</span><br><span class="line"></span><br><span class="line">int l, n, m;</span><br><span class="line">int d[N];</span><br><span class="line"></span><br><span class="line">bool check(int x)</span><br><span class="line">&#123;</span><br><span class="line">    int cnt = 0, last = 0;</span><br><span class="line">    for (int i = 1; i &lt;= n + 1; ++i)</span><br><span class="line">        if (d[i] - d[last] &lt; x) ++cnt;</span><br><span class="line">        else last = i;</span><br><span class="line">    </span><br><span class="line">    return cnt &lt;= m;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    cin &gt;&gt; l &gt;&gt; n &gt;&gt; m;</span><br><span class="line">    </span><br><span class="line">    for (int i = 1; i &lt;= n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        scanf(&quot;%d&quot;, &amp;d[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    d[n + 1] = l;</span><br><span class="line">    </span><br><span class="line">    int r = l;</span><br><span class="line">    l = 0;</span><br><span class="line">    while (l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        int mid = l + r + 1 &gt;&gt; 1;</span><br><span class="line">        if (check(mid)) l = mid;</span><br><span class="line">        else r = mid - 1;</span><br><span class="line">    &#125;</span><br><span class="line">    cout &lt;&lt; l &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="AcWing-896-最长上升子序列II"><a href="#AcWing-896-最长上升子序列II" class="headerlink" title="AcWing 896.最长上升子序列II"></a><a target="_blank" rel="noopener" href="https://www.acwing.com/problem/content/898/">AcWing 896.最长上升子序列II</a></h2><p><strong>大致题意</strong><br>给定一个长度为N的数列，求<strong>数值严格单调递增的子序列</strong>的长度最长是多少。<br><strong>分析</strong><br>这是一道非常简单的dp问题，但这个题$1≤N≤100000$，正常dp $O(n^2)$的复杂度爆掉了，因此需要优化。<br>我们发现这样一个性质：</p>
<p><div align="center">
<img src="http://8.130.10.253:12345/usr/uploads/2024/07/3237557560.png" width="70%">
</div><br>横轴代表<strong>最长子序列的长度</strong>，纵轴代表<strong>所有这个长度的子序列最小的结尾数字</strong>，性质为：这两个量是正相关的。<br>[scode type=”share”]<strong>证明</strong><br>设长度为5的序列最小的结尾数字是$a$，长度为6的序列最小的结尾数字是$b$<br>假设$b&lt;a$，那么在<strong>长度为6的序列</strong>中<strong>长度为5的子序列</strong>的结尾数字一定小于$a$<br>那么全局长度为5的子序列最小结尾数字就不是$a$，与假设矛盾。[/scode]</p>
<p>我们只需要维护一个序列，这个序列记录了所有长度子序列<strong>最小的结尾数字</strong>，遍历数组的时候只需要对每个元素，二分得到<strong>一个序列长度</strong>，这个序列是<strong>这个元素能接在后面的最长的序列</strong>，起始就是找到<strong>小于$x$的最后一个数的下标</strong>。优化后$O(nlogn)$，不会爆掉。</p>
<p><strong>代码</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;iostream&gt;</span><br><span class="line"></span><br><span class="line">using namespace std;</span><br><span class="line"></span><br><span class="line">const int N = 100010;</span><br><span class="line"></span><br><span class="line">int n;</span><br><span class="line">int a[N], m[N];</span><br><span class="line">int len;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    cin &gt;&gt; n;</span><br><span class="line">    </span><br><span class="line">    for (int i = 0; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        scanf(&quot;%d&quot;, &amp;a[i]);</span><br><span class="line">        int l = 0, r = len;</span><br><span class="line">        while (l &lt; r)</span><br><span class="line">        &#123;</span><br><span class="line">            int mid = l + r + 1 &gt;&gt; 1;</span><br><span class="line">            if (m[mid] &lt; a[i]) l = mid;</span><br><span class="line">            else r = mid - 1;</span><br><span class="line">        &#125;</span><br><span class="line">        len = max(len, l + 1);</span><br><span class="line">        m[l + 1] = a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    cout &lt;&lt; len &lt;&lt; endl;</span><br><span class="line">    </span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86.html" data-id="cm4zg7wze000nwe3kdjty29in" data-title="[算法] 二分" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-HW-2-Classification" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/HW-2-Classification.html" class="article-date">
  <time class="dt-published" datetime="2024-07-25T03:35:00.000Z" itemprop="datePublished">2024-07-25</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/HW-2-Classification.html">[HW 2] Classification</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本次HW要求完成一个语音资料的辨识问题。给出的data是将每个语音段分成多个frame后的tensor，每个frame对应一个phoneme（类似于音标），一共有41种phoneme，要predict的是给出一个语音段的多个frame的tensor，预测每个frame的phoneme。</p>
<ul>
<li>HW介绍：<a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/HW01.pdf">HW2投影片介绍</a>；</li>
<li>data获取：<a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/ml2022spring-hw2/data">kaggle</a>；</li>
<li>示例代码：<a target="_blank" rel="noopener" href="https://github.com/virginiakm1988/ML2022-Spring/blob/main/HW02/HW02.ipynb">github</a></li>
</ul>
<p>做了两次HW后，也算是大体掌握了一种train model的结构。这里只记录本次HW主要部分，其他用法的解释见HW1。</p>
<h1 id="导入package"><a href="#导入package" class="headerlink" title="导入package"></a>导入package</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># numerical operation</span><br><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"># data i\o</span><br><span class="line">import csv</span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># garbage collect</span><br><span class="line">import gc</span><br><span class="line"></span><br><span class="line"># progress bar</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"></span><br><span class="line"># PyTorch</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.utils.data import Dataset, DataLoader, random_split</span><br><span class="line"></span><br><span class="line"># plotting learning curve</span><br><span class="line">from torch.utils.tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line"># run time</span><br><span class="line">from time import time</span><br></pre></td></tr></table></figure>
<h1 id="固定随机数种子"><a href="#固定随机数种子" class="headerlink" title="固定随机数种子"></a>固定随机数种子</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def same_seed(seed):</span><br><span class="line">    torch.backends.cudnn.deterministic = True</span><br><span class="line">    torch.backends.cudnn.benchmark = False</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    random.seed(seed)</span><br><span class="line">    if torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>
<h1 id="定义Hyperparameter"><a href="#定义Hyperparameter" class="headerlink" title="定义Hyperparameter"></a>定义Hyperparameter</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    &#x27;concat_feat_num&#x27;: 19,</span><br><span class="line">    &#x27;train_ratio&#x27;: 0.8,</span><br><span class="line">    &#x27;seed&#x27;: 5201314,</span><br><span class="line">    &#x27;batch_size&#x27;: 2048,</span><br><span class="line">    &#x27;lr&#x27;: 1e-5,</span><br><span class="line">    &#x27;epochs&#x27;: 50,</span><br><span class="line">    &#x27;save_path&#x27;: &#x27;./model/model.pth&#x27;,</span><br><span class="line">    &#x27;early_stop&#x27;: 10</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里其实是一边写一边往里面加的</p>
<h1 id="定义Dateset"><a href="#定义Dateset" class="headerlink" title="定义Dateset"></a>定义Dateset</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class hw2_Dataset(Dataset):</span><br><span class="line">    def __init__(self, x, y=None):</span><br><span class="line">        if y is None:</span><br><span class="line">            self.y = y</span><br><span class="line">        else:</span><br><span class="line">            self.y = torch.LongTensor(y)</span><br><span class="line">        self.x = x</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, index):</span><br><span class="line">        if self.y is None:</span><br><span class="line">            return self.x[index]</span><br><span class="line">        else:</span><br><span class="line">            return self.x[index], self.y[index]</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.x)</span><br></pre></td></tr></table></figure>
<h1 id="预处理数据"><a href="#预处理数据" class="headerlink" title="预处理数据"></a>预处理数据</h1><p>这部分要<strong>连接多个frame作为一个frame的feature</strong>。因为一个phoneme的前后phoneme可能都会对当前phoneme产生影响，换句话说，<strong>在一定程度上，可以根据前一句话和后一句话推断出当前这一句话</strong>，这是非常合理的。</p>
<p>这段代码在示例中已经写好了，只需要我们传入连接frame的数量即可：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def shift(x, n):</span><br><span class="line">    if n &lt; 0:</span><br><span class="line">        left = x[0].repeat(-n, 1)</span><br><span class="line">        right = x[:n]</span><br><span class="line"></span><br><span class="line">    elif n &gt; 0:</span><br><span class="line">        right = x[-1].repeat(n, 1)</span><br><span class="line">        left = x[n:]</span><br><span class="line">    else:</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    return torch.cat((left, right), dim=0)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def concat_feat(x, concat_n):</span><br><span class="line">    assert concat_n % 2 == 1  # n must be odd</span><br><span class="line">    if concat_n &lt; 2:</span><br><span class="line">        return x</span><br><span class="line">    seq_len, feature_dim = x.size(0), x.size(1)</span><br><span class="line">    x = x.repeat(1, concat_n)</span><br><span class="line">    x = x.view(seq_len, concat_n, feature_dim).permute(1, 0, 2)  # concat_n, seq_len, feature_dim</span><br><span class="line">    mid = (concat_n // 2)</span><br><span class="line">    for r_idx in range(1, mid + 1):</span><br><span class="line">        x[mid + r_idx, :] = shift(x[mid + r_idx], r_idx)</span><br><span class="line">        x[mid - r_idx, :] = shift(x[mid - r_idx], -r_idx)</span><br><span class="line"></span><br><span class="line">    return x.permute(1, 0, 2).view(seq_len, concat_n * feature_dim)</span><br></pre></td></tr></table></figure><br>下面是对数据的导入：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">def pretreatment_feat(data, label_dict, concat_num, mode=&#x27;train&#x27;):</span><br><span class="line">    max_len = 3000000</span><br><span class="line">    x = torch.empty(max_len, 39 * config[&#x27;concat_feat_num&#x27;])</span><br><span class="line">    if mode == &#x27;train&#x27;:</span><br><span class="line">        y = torch.empty(max_len, dtype=torch.long)</span><br><span class="line">    idx = 0</span><br><span class="line">    for i, fname in enumerate(data):</span><br><span class="line">        feat = torch.load(os.path.join(path, &#x27;feat&#x27;, mode, f&#x27;&#123;fname&#125;.pt&#x27;))</span><br><span class="line">        cur_len = len(feat)</span><br><span class="line">        feat = concat_feat(feat, concat_num)</span><br><span class="line">        x[idx:idx + cur_len, :] = feat</span><br><span class="line">        if mode == &#x27;train&#x27;:</span><br><span class="line">            label = torch.LongTensor(label_dict[fname])</span><br><span class="line">            y[idx:idx + cur_len] = label</span><br><span class="line">        idx += cur_len</span><br><span class="line"></span><br><span class="line">    x = x[:idx, :]</span><br><span class="line">    if mode == &#x27;train&#x27;:</span><br><span class="line">        y = y[:idx]</span><br><span class="line">        return x, y</span><br><span class="line">    else:</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">path = &#x27;./libriphone&#x27;</span><br><span class="line"># train_label_dict</span><br><span class="line">train_label = open(os.path.join(path, &#x27;train_labels.txt&#x27;)).readlines()</span><br><span class="line">label_dict = &#123;&#125;</span><br><span class="line">for line in train_label:</span><br><span class="line">    line = line.strip(&#x27;\n&#x27;).split(&#x27; &#x27;)</span><br><span class="line">    label_dict[line[0]] = [int(p) for p in line[1:]]</span><br><span class="line"></span><br><span class="line"># train_data_list</span><br><span class="line">train_data = open(os.path.join(path, &#x27;train_split.txt&#x27;)).readlines()</span><br><span class="line">train_data = [line.strip(&#x27;\n&#x27;) for line in train_data]</span><br><span class="line">random.shuffle(train_data)</span><br><span class="line"></span><br><span class="line"># split train data to train set and valid set</span><br><span class="line">x_train_data = train_data[:int(config[&#x27;train_ratio&#x27;] * len(train_data))]</span><br><span class="line">x_valid_data = train_data[int(config[&#x27;train_ratio&#x27;] * len(train_data)):]</span><br><span class="line"></span><br><span class="line"># pretreatment</span><br><span class="line">x_train, y_train = pretreatment_feat(x_train_data, label_dict, config[&#x27;concat_feat_num&#x27;])</span><br><span class="line">x_valid, y_valid = pretreatment_feat(x_valid_data, label_dict, config[&#x27;concat_feat_num&#x27;])</span><br><span class="line"></span><br><span class="line"># test data, same operation as train data</span><br><span class="line">test_data = open(os.path.join(path, &#x27;test_split.txt&#x27;)).readlines()</span><br><span class="line">test_data = [line.strip(&#x27;\n&#x27;) for line in test_data]</span><br><span class="line">x_test = pretreatment_feat(test_data, label_dict, config[&#x27;concat_feat_num&#x27;], mode=&#x27;test&#x27;)</span><br><span class="line"></span><br><span class="line">print(f&quot;&quot;&quot;train data size: &#123;len(x_train)&#125;,</span><br><span class="line">valid data size: &#123;len(x_valid)&#125;,</span><br><span class="line">test data size: &#123;len(x_test)&#125;&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line">train_dataset, valid_dataset, test_dataset = hw2_Dataset(x_train, y_train), hw2_Dataset(x_valid, y_valid), \</span><br><span class="line">    hw2_Dataset(x_test)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(dataset=train_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=True, pin_memory=True)</span><br><span class="line">valid_loader = DataLoader(dataset=valid_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=True, pin_memory=True)</span><br><span class="line">test_loader = DataLoader(dataset=test_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=False, pin_memory=True)</span><br><span class="line"></span><br><span class="line">del train_data, x_train_data, label_dict, x_valid_data, train_label, test_data, x_train, \</span><br><span class="line">    y_train, x_valid, y_valid, x_test</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure><br>最终形成了train set、valid set、test set的DataLoader，即可feed给model进行训练。</p>
<h1 id="定义model"><a href="#定义model" class="headerlink" title="定义model"></a>定义model</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class hw2_model(nn.Module):</span><br><span class="line">    def __init__(self, input_dim, output_dim=41):</span><br><span class="line">        super(hw2_model, self).__init__()</span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, 1024),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.BatchNorm1d(1024),</span><br><span class="line">            nn.Dropout(0.35),</span><br><span class="line">            nn.Linear(1024, 1024),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.BatchNorm1d(1024),</span><br><span class="line">            nn.Dropout(0.35),</span><br><span class="line">            nn.Linear(1024, 1024),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.BatchNorm1d(1024),</span><br><span class="line">            nn.Dropout(0.35),</span><br><span class="line">            nn.Linear(1024, 1024),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.BatchNorm1d(1024),</span><br><span class="line">            nn.Dropout(0.35),</span><br><span class="line">            nn.Linear(1024, output_dim),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>
<p>由于作业介绍上提到，达到更高的分数需要加上<strong>batchnorm和dropout</strong>，因此这里的<code>nn.BatchNorm1d(1024)</code>和<code>nn.Dropout(0.35)</code>实现了这两个功能。下面分别介绍一下这两个方法。<br>属实没想到多加一个hidden layer居然能让score从0.69长到0.75…</p>
<h2 id="batchnorm"><a href="#batchnorm" class="headerlink" title="batchnorm"></a>batchnorm</h2><p>batchnorm是一种<strong>用于加速neural network训练速度，提高模型稳定性</strong>的技术。他会对每一层的input进行<strong>归一化</strong>处理，从而减小<strong>梯度消失</strong>、<strong>梯度爆炸</strong>问题。<br><strong>作用</strong>：训练neural network的过程中，<strong>数据的分布</strong>会影响学习过程。每一层input分布变化后会引起后面层的input也发生变化，这种现象叫做<strong>协变量偏移</strong>。使用batchnorm可以使input变得更加稳定，减少协变量偏移的影响。<br><strong>方法</strong>：</p>
<ol>
<li><strong>计算均值和方差</strong>：计算当前批次数据的均值和方差。</li>
<li><strong>归一化</strong>：将每个数据点减去均值并除以方差，使其变成标准正态分布。</li>
<li><strong>缩放和平移</strong>：通过可学习的参数对归一化后的数据进行缩放和平移。</li>
</ol>
<p>PyTorch提供了三个Batch Normalization模块：nn.BatchNorm1d、nn.BatchNorm2d 和 nn.BatchNorm3d，分别用于1D、2D和3D数据的归一化。</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>Dropout是一种正则化技术，通过<strong>在训练过程中随机地将一部分神经元的输出设为零，以防止过拟合并增强模型的泛化能力</strong>。<br><code>nn.Dropout</code>模块在训练期间会随机地将输入张量的一部分元素设为零，并按比例缩放剩余的元素，以保持整体的期望值不变。<br><strong>作用</strong>：</p>
<ol>
<li><strong>防止过拟合</strong>：通过随机丢弃一部分神经元，Dropout 减少了神经元之间的相互依赖，增强了模型的泛化能力。</li>
<li><strong>增强模型的鲁棒性</strong>：Dropout 使得神经网络在训练过程中学会更为鲁棒的特征表示，能够更好地适应不同的输入数据。</li>
<li><strong>简化模型的复杂度</strong>：通过随机丢弃一部分神经元，Dropout 有效地降低了模型的复杂度，从而减少了过拟合的风险。</li>
</ol>
<h1 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h1><p>接下来就是定义traning loop并开始训练：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;training loop&quot;&quot;&quot;</span><br><span class="line">def trainer(train_loader, valid_loader, model, config, device, train_len, valid_len):</span><br><span class="line">    criterion = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimizer = torch.optim.Adam(model.parameters(), lr=config[&#x27;lr&#x27;])</span><br><span class="line">    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,</span><br><span class="line">                                                                     T_0=10, T_mult=2, eta_min=config[&#x27;lr&#x27;] / 2)</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter()</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(&#x27;model&#x27;):</span><br><span class="line">        os.mkdir(&#x27;./model&#x27;)</span><br><span class="line"></span><br><span class="line">    n_epochs, best_acc, step, early_stop_count = config[&#x27;epochs&#x27;], -math.inf, 0, 0</span><br><span class="line"></span><br><span class="line">    for epoch in range(n_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        train_acc, train_loss, valid_acc, valid_loss = 0, 0, 0, 0</span><br><span class="line">        train_pbar = tqdm(train_loader, position=0, leave=True)</span><br><span class="line"></span><br><span class="line">        for x, y in train_pbar:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            outputs = model(x)</span><br><span class="line">            loss = criterion(outputs, y)</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            _, train_pred = torch.max(outputs, 1)  # get index</span><br><span class="line">            train_acc += (train_pred.detach() == y.detach()).sum().item()</span><br><span class="line">            train_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            step += 1</span><br><span class="line">            train_pbar.set_description(f&#x27;Epoch [&#123;epoch + 1&#125;/&#123;n_epochs&#125;]&#x27;)</span><br><span class="line">            train_pbar.set_postfix(&#123;&#x27;loss&#x27;: loss.detach().item()&#125;)</span><br><span class="line"></span><br><span class="line">        train_acc_rate = train_acc / train_len</span><br><span class="line">        writer.add_scalar(&#x27;acc_rate/train&#x27;, train_acc_rate, step)</span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        for x, y in valid_loader:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                outputs = model(x)</span><br><span class="line">                loss = criterion(outputs, y)</span><br><span class="line">                _, valid_pred = torch.max(outputs, 1)</span><br><span class="line">                valid_acc += (valid_pred.cpu() == y.cpu()).sum().item()</span><br><span class="line">                valid_loss += loss</span><br><span class="line"></span><br><span class="line">        valid_acc_rate = valid_acc / valid_len</span><br><span class="line">        writer.add_scalar(&#x27;acc_rate/valid&#x27;, valid_acc_rate, step)</span><br><span class="line"></span><br><span class="line">        print(f&#x27;Epoch [&#123;epoch + 1&#125;/&#123;n_epochs&#125;]: Train acc: &#123;train_acc_rate&#125;, Valid acc: &#123;valid_acc_rate&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">        if valid_acc &gt; best_acc:</span><br><span class="line">            best_acc = valid_acc</span><br><span class="line">            torch.save(model.state_dict(), config[&#x27;save_path&#x27;])</span><br><span class="line">            print(&#x27;save model with acc: &#123;:.3f&#125;&#x27;.format(valid_acc_rate))</span><br><span class="line">            early_stop_count = 0</span><br><span class="line">        else:</span><br><span class="line">            early_stop_count += 1</span><br><span class="line"></span><br><span class="line">        if early_stop_count &gt;= config[&#x27;early_stop&#x27;]:</span><br><span class="line">            print(&#x27;\n model is not improving, so we halt the training&#x27;)</span><br><span class="line">            return</span><br><span class="line">        scheduler.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;start training&quot;&quot;&quot;</span><br><span class="line">st_time = time()</span><br><span class="line">model = hw2_model(39 * config[&#x27;concat_feat_num&#x27;], 41).to(device)</span><br><span class="line">trainer(train_loader, valid_loader, model, config, device, len(train_dataset), len(valid_dataset))</span><br><span class="line">end_time = time()</span><br><span class="line">print(f&#x27;Total train time: &#123;end_time - st_time&#125;&#x27;)</span><br></pre></td></tr></table></figure><br>这里用到了<strong>Adam</strong>作为Optimizer，用<strong>余弦退火</strong>的方法来对learning rate进行schedule。都是PyTorch中自带的，方便的很。<br>[scode type=”green”]至于为什么要用Adam和余弦退火，用李宏毅老师的话说：<strong>这是古圣先贤的意思，用就完事了</strong>。[/scode]</p>
<h1 id="预测并保存结果"><a href="#预测并保存结果" class="headerlink" title="预测并保存结果"></a>预测并保存结果</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&quot;&quot;&quot;get model&quot;&quot;&quot;</span><br><span class="line">model = hw2_model(39 * config[&#x27;concat_feat_num&#x27;], 41).to(device)</span><br><span class="line">model.load_state_dict(torch.load(config[&#x27;save_path&#x27;]))</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;predict test and save result&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict(test_loader, model, device):</span><br><span class="line">    preds = []</span><br><span class="line">    model.eval()</span><br><span class="line">    for x in tqdm(test_loader):</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            y = model(x)</span><br><span class="line">            _, pred = torch.max(y, 1)</span><br><span class="line">            preds.append(pred.detach().cpu())</span><br><span class="line">    preds = torch.cat(preds, dim=0).numpy()</span><br><span class="line">    return preds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def save_pred(preds, file):</span><br><span class="line">    with open(file, &#x27;w&#x27;) as fp:</span><br><span class="line">        writer = csv.writer(fp)</span><br><span class="line">        writer.writerow([&#x27;Id&#x27;, &#x27;Class&#x27;])</span><br><span class="line">        for i, p in enumerate(preds):</span><br><span class="line">            writer.writerow([i, p])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">preds = predict(test_loader, model, device)</span><br><span class="line">save_pred(preds, &#x27;pred.csv&#x27;)</span><br></pre></td></tr></table></figure>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>完成了本次HW，主要收获有：</p>
<ul>
<li>更加熟悉了train的过程，慢慢养成自己的训练套路；</li>
<li>学习到了<strong>Batchnorm、Dropout</strong>的方法，特别有用；</li>
<li>学习到了<strong>余弦退火</strong>这种learning rate schedule的方法，看来古圣先贤是不会骗人的；</li>
<li>放心大胆的去尝试更复杂的structure，本次HW多加了一个hidden layer，直接从Simple baseline干到Strong baseline…</li>
</ul>
<p>但我对Hyperparameter的调整策略还是一知半解，难道真的只是凭intuition去调整吗…前路漫漫<br>HW代码：<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1Gn2bgc9BUE6qHqX-Aozavihk-x8R3MKB?usp=drive_open#scrollTo=7NfkWnq88un2">colab</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/HW-2-Classification.html" data-id="cm4zg7wz60000we3k9j041934" data-title="[HW 2] Classification" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-week2-How-to-get-a-Good-Train-Data" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/week2-How-to-get-a-Good-Train-Data.html" class="article-date">
  <time class="dt-published" datetime="2024-07-22T14:55:00.000Z" itemprop="datePublished">2024-07-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/week2-How-to-get-a-Good-Train-Data.html">[week2] How to get a Good Train Data</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>在train的过程中，如果我们能拿到世界上所有的data（记作$D_{all}$），那么在方法合适的情况下，自然就能train出一组最佳参数（记作$h^{all}$）：</p>
<script type="math/tex; mode=display">
h^{all}=\mathop{\arg\min}\limits_hL(h,D_{all})</script><p>但问题在于我们无法获取$D_{all}$，只能获取部分数据$D_{train}={(x^1,\hat{y}^1),(x^2,\hat{y}^2),…,(x^N,\hat{y}^N)}$，其中$(x^n,\hat{y}^n)~D_{all}$。这里我们假设$(x^n,\hat{y}^n)$是<strong>independently and identically distributed</strong>（i.i.d)。那么我们可以得到一组参数：</p>
<script type="math/tex; mode=display">
h^{train}=\mathop{\arg\min}\limits_hL(h,D_{train})</script><p>我们希望$L(h^{train},D_{all})$接近于$L(h^{all},D_{all})$（在$D_{all}$上，$L(h^{all},D_{all})$一定是所有$h$中最小的）。<br>那么我们可以把目标记作：</p>
<script type="math/tex; mode=display">
L(h^{train},D_{all})-L(h^{all},D_{all})\leq\sigma</script><p>什么样的$D_{train}$训练出的$h^{train}$满足这个目标呢？答案是：</p>
<script type="math/tex; mode=display">
\forall h∈ℋ,|L(h,D_{train})-L(h,D_{all})|\leq\frac{\sigma}{2}</script><p>其中|ℋ|=number of candidate functions (model “complexity”)<br><strong>证明</strong><br>根据上式结论，有：</p>
<script type="math/tex; mode=display">
L(h^{train},D_{all})\leq L(h^{train},D_{train})+\frac{\sigma}{2}\leq L(h^{all},D_{train})+\frac{\sigma}{2}</script><p>而：</p>
<script type="math/tex; mode=display">
L(h^{all},D_{train})\leq L(h^{all},D_{all})+\frac{\sigma}{2}</script><p>联立以上两式：</p>
<script type="math/tex; mode=display">
L(h^{train},D_{all})\leq L(h^{all},D_{all})+\sigma</script><p>我们将$\frac{\sigma}{2}$记作ε，得：</p>
<script type="math/tex; mode=display">
\forall h∈ℋ,|L(h,D_{train})-L(h,D_{all})|\leq ε</script><p>那么sample出bad $D_{train}$的probability是多少呢？<br>如果一个$D_{train}$是bad的，那么至少有一个$h$使得$|L(h,D_{train})-L(h,D_{all})|&gt; ε$。因此：</p>
<script type="math/tex; mode=display">
P(D_{train} is bad)=\mathop{\cup}\limits_{h∈ℋ} P(D_{train}\ is\ bad\ due\ to\ h)\leq\sum\limits_{h∈ℋ} P(D_{train}\ is\ bad\ due\ to\ h)</script><p>根据<strong>Hoeffding’s Inequality</strong>（在$L∈[0,1]$的情况下）：</p>
<script type="math/tex; mode=display">
P(D_{train}\ is\ bad\ due\ to\ h)\leq 2exp(-2Nε^2)</script><p>其中$N$是$D_{train}$种的样本数量。<br>而$2exp(-2Nε^2)$和$h$没有关系，因此$\sum\limits_{h∈ℋ}2exp(-2Nε^2)=|ℋ|2exp(-2Nε^2)$<br>那么最终得到：</p>
<script type="math/tex; mode=display">
P(D_{train}\ is\ bad)\leq|ℋ|2exp(-2Nε^2)</script><p>综上所述，就得到了<strong>降低sample到bad$ D_{train}$的概率</strong>的方法：</p>
<ul>
<li>增大N；</li>
<li>减小ℋ。</li>
</ul>
<p>还有一个问题就是|ℋ|如何计算。解决方法是使用<strong>VC-dimension</strong>，这里不进行过多展开。</p>
<p>下面我们要面临的是ℋ如何选择的问题：</p>
<ul>
<li>当选择一个小的ℋ时，Model得到的<strong>Loss会很大</strong>，但<strong>$L(h^{train},D_{all})$和$L(h^{all},D_{all})$会很接近</strong>；</li>
<li>当选择一个大的ℋ时，Model得到的<strong>Loss较小</strong>，但<strong>$L(h^{train},D_{all})$和$L(h^{all},D_{all})$会相差较大</strong>；</li>
</ul>
<p><img src="images/2024/07/2306880246.png" alt="2024-07-22T14:54:20.png"><br>后面的课程会讲解鱼与熊掌兼得的方法。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/week2-How-to-get-a-Good-Train-Data.html" data-id="cm4zg7wzc000bwe3k5w9x5qmb" data-title="[week2] How to get a Good Train Data" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Week2-extra-General-Guide-in-training" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/Week2-extra-General-Guide-in-training.html" class="article-date">
  <time class="dt-published" datetime="2024-07-22T11:54:00.000Z" itemprop="datePublished">2024-07-22</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/Week2-extra-General-Guide-in-training.html">[Week2 extra] General Guide in training</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>李宏毅老师给出的<strong>General Guide</strong>：<br><img src="images/2024/07/2727764113.png" alt="General Guide"><br><strong>Model Bias</strong>指的是<strong>Model too simple</strong>，这时候我们只需要增加模型的复杂程度即可。<br>但往往有些时候，增加模型复杂程度后其train的效果反而更差了，此时就出现了<strong>Optimization issue</strong>，解决方法放在后面。<br>如果在<strong>train的时候loss</strong>小，而在<strong>test的时候loss很大</strong>，很可能发生了<strong>overfitting</strong>。对此有以下解决方法：</p>
<ol>
<li><strong>增加training data</strong><ul>
<li>搜集更多data</li>
<li>根据自己对data的理解<strong>创造data</strong></li>
</ul>
</li>
<li><strong>给Model增加限制</strong><ul>
<li>Less parameters or Sharing parameters</li>
<li>Dropout</li>
<li>Less features</li>
<li>Early stopping</li>
<li>Regularization</li>
</ul>
</li>
</ol>
<p>那么如何在train的时候选择一个<strong>兼顾train loss和test loss</strong>的Model呢？使用<strong>Cross Validation</strong>。<br>简单原理就是，将training data分为<strong>training set</strong>和<strong>validation set</strong>，用<strong>validation set模拟testing data</strong>，以<strong>validation set的loss</strong>作为评判模型好坏的标准。<br>[scode type=”green”]为避免<strong>划分出不好的validation set</strong>，推荐使用<strong>N-fold Cross Validation</strong>。[/scode]</p>
<h1 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h1><p>问题：为什么要<strong>分batch训练</strong>？<br><img src="images/2024/07/2668724858.png" alt=""><br>这是一张对比图，可以看到分batch后的区别。<br>在考虑<strong>GPU并行计算</strong>后，有以下对比图：<br><img src="images/2024/07/2491029082.png" alt=""><br>可以发现，在一定范围内，反而<strong>大的batch计算效率更快</strong>。下面看看在traning上的表现：<br><img src="images/2024/07/3952806775.png" alt=""><br>分析可知，batch size越大，在<strong>training的时候效果会交叉</strong>。问题在于：<strong>Optimization</strong>。<br>一个可能的解释是：<br><img src="images/2024/07/2031386315.png" alt=""><br>由图可知，大的batch更容易卡住，而小的batch由于<strong>batch不同，loss function不同</strong>，所以不容易被单一Loss卡住。<br>另外，小batch在testing data上的表现往往也更好，见下图：<br><img src="images/2024/07/394981967.png" alt=""><br>由于testing data和training data的分布是<strong>有微小差异的</strong>，因此其<strong>Loss也会有微小差异</strong>。在平坦的地方，training data的minima对应到testing data上时，差异不会很大；而在陡峭的地方，差异就会很大，如图所示。一个可能的解释是：<strong>小batch容易找到平坦地区的minima，大batch容易进入峡谷minima</strong>。</p>
<p>下面是一张大小batch优缺点对比图：<br><img src="images/2024/07/709608771.png" alt="large batch v.s. small batch"><br>[scode type=”yellow”]<strong>batch size是一个hyperparameter</strong>，需要我们自己去调[/scode]</p>
<h1 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h1><p>简单理解为<strong>之前的动作</strong>会对<strong>当前的动作</strong>产生影响，即Movement<strong>不只取决于gradient</strong>，<strong>之前的Movement</strong>也会产生影响。举例如下：</p>
<ul>
<li><strong>Strating at</strong> $\theta^0$</li>
<li><strong>Movemoent</strong> $m^0=0$</li>
<li><strong>Compute gradient</strong> $g^0$</li>
<li><strong>Movement</strong> $m^1=\lambda m^0-\eta g^0$</li>
<li><strong>Move</strong> $\theta^1=\theta^0+m^1$</li>
<li><strong>Compute gradient</strong> $g^1$</li>
<li><strong>Movement</strong> $m^2=\lambda m^1-\eta g^1$</li>
</ul>
<p><div align="center">
<img src="images/2024/07/679543323.png" width="50%">
</div><br>使用momentum在有些情况下可以<strong>帮助走出local minima</strong>。将momentum应用到SGD（stochastic gradient descent)上，叫做SGDM。<br>[scode type=”share”]Gradient Descent是<strong>每次计算所有data的Loss的gradient并更新</strong>，而SGD是在其中<strong>选取部分data计算gradient并更新</strong>。<br>PyTorch中默认提供了SGD的方法，因为训练的时候一般都是<strong>分batch训练并且shuffle</strong>的，相当于每次<strong>随机抽样</strong>。如果<strong>只有一个batch并且不shuffle</strong>，那么虽然调用SGD，但实际相当于普通的Gradient Descent[/scode]</p>
<h1 id="How-to-solve-Optimization-issue"><a href="#How-to-solve-Optimization-issue" class="headerlink" title="How to solve Optimization issue"></a>How to solve Optimization issue</h1><h2 id="Hessian-Matrix"><a href="#Hessian-Matrix" class="headerlink" title="Hessian Matrix"></a>Hessian Matrix</h2><p>Optimization停下的原因可能是陷入了<strong>gradient接近于0</strong>的点（<strong>critical point</strong>），而这个点并不是universal minima（可能是<strong>local minima</strong>或<strong>saddle point</strong>）。<br><img src="images/2024/07/3654125339.png" alt="local minima and saddle point"><br>我们能够通过一些方法知道某个点是local minima还是saddle point，即通过$L(\theta^\prime)$估计出其附近的值$L(\theta)$：</p>
<script type="math/tex; mode=display">
L(\theta)=L(\theta^\prime)+(\theta-\theta^\prime)^Tg+\frac{1}{2}(\theta-\theta^\prime)^TH(\theta-\theta^\prime)</script><p>其中：</p>
<script type="math/tex; mode=display">
g=\nabla L(\theta^\prime),g_i=\frac{\partial L(\theta^\prime)}{\partial \theta_i}</script><p>H为Hessian Matrix：</p>
<script type="math/tex; mode=display">
H_{ij}=\frac{\partial^2}{\partial \theta_i\theta_j}L(\theta^\prime)</script><p>我们把(\theta-\theta^\prime)记作$v$，那么在critical point上，$g=0$：</p>
<script type="math/tex; mode=display">
L(\theta)=L(\theta^\prime)+\frac{1}{2}v^THv</script><p>$v^THv$的正负就能说明是local minima还是saddle point：</p>
<ul>
<li>如果对于某些$v$，$v^THv&gt;0$，说明$L(\theta)&gt;L(\theta^\prime)$，那么$L(\theta^\prime)$是local minima。此时<strong>$H$是positive definite的</strong>，即其<strong>所有eigenvalues都是正的</strong>；</li>
<li>如果对于某些$v$，$v^THv&gt;0$，某些$v^THv&lt;0$，那么$L(\theta^\prime)$是saddle point。$H$<strong>有些eigenvalues为正，有些eigenvalue为负</strong>。</li>
</ul>
<p>当遇到saddle point时，$H$可以指明我们该<strong>如何更新</strong>。<br>假设$u$是$H$的一个eigenvector，$\lambda$是$u$对应的eigenvalue，那么有：</p>
<script type="math/tex; mode=display">
u^THu=u^T(\lambda u)=\lambda ||u||^2</script><p>假如$\lambda&lt;0$，那么$\lambda ||u||^2=u^THu&lt;0$，那么在$\theta-\theta^\prime=u$的条件下：</p>
<script type="math/tex; mode=display">
L(\theta)=L(\theta^\prime)+\frac{1}{2}u^THu</script><p>那么$L(\theta)&lt;L(\theta^\prime)$，此时可知：$\theta=\theta^\prime+u$<br>[scode type=”yellow”]实际应用中，由于计算量太大，这个方法<strong>很少用到</strong>[/scode]</p>
<p>[scode type=”share”]在实际中，由于parameter数量很大，所以在这样的一个条件下很难找到一个<strong>所有eigenvalues都是正的Hessian Matrix</strong>，因此local minima非常少见，卡住训练的critical point一般都是<strong>saddle point</strong>。[/scode]</p>
<h2 id="adaptive-learning-rate"><a href="#adaptive-learning-rate" class="headerlink" title="adaptive learning rate"></a>adaptive learning rate</h2><p>在实际train的过程中，卡住训练的往往不是critical point，有这样一种情况：</p>
<p><div align="center">
<img src="images/2024/07/3198263582.png" width="50%">
</div><br>可以看到，实际上陷入了这样一种情况。如果选定一个较大的learning rate，那么在山谷里会遇到这种情况；如果选定一个较小的learning rate，那么在平原上会被阻塞住。由此可见，<strong>learning rate需要根据gradient进行调整</strong>，才能更好的完成optimization。</p>
<h3 id="Root-Mean-Square"><a href="#Root-Mean-Square" class="headerlink" title="Root Mean Square"></a>Root Mean Square</h3><p>普通的Gradient Descent方法为：$\theta_i^{t+1}=\theta_i^t-\eta g_i^t$<br>Root Mean Square的方法为：</p>
<script type="math/tex; mode=display">
\theta_i^{t+1}=\theta_i^t-\frac{\eta}{\sigma_i^t}g_i^t</script><p>其中:</p>
<script type="math/tex; mode=display">
\sigma_i^0=\sqrt{(g_i^0)^2}=|g_i^0|</script><script type="math/tex; mode=display">
\sigma_i^1=\sqrt{\frac{1}{2}[(g_i^0)^2+(g_i^1)^2]}</script><script type="math/tex; mode=display">
...</script><script type="math/tex; mode=display">
\sigma_i^t=\sqrt{\frac{1}{t+1}\sum\limits_{j=0}^t(g_i^j)^2}</script><p>这个方法能常用在<strong>Adagrad</strong>中，针对不同的parameter，其learning rate也不同（陡的地方learning rate小、平缓的地方learning rate大）。但并没有考虑到<strong>同一个parameter的gradient变化可能很大</strong>的问题。</p>
<h3 id="Exploding-Gradient"><a href="#Exploding-Gradient" class="headerlink" title="Exploding Gradient"></a>Exploding Gradient</h3><p><img src="images/2024/07/1555348955.png" alt="Exploding Gradient"><br>在Root Mean Square中，$\sigma_i^t=\sqrt{\frac{1}{t+1}\sum\limits_{j=0}^t(g_i^j)^2}$，当<strong>某一时期的gradient都很小</strong>时，$\frac{1}{t+1}\sum\limits_{j=0}^t(g_i^j)^2$就会累积变得很小，那么$\frac{\eta}{\sigma_i^t}$就会变得很大，此时gradient就会暴走。</p>
<h3 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h3><p><strong>RMSProp</strong>的方法为：</p>
<script type="math/tex; mode=display">
\theta_i^{t+1}=\theta_i^t-\frac{\eta}{\sigma_i^t}g_i^t</script><p>其中:</p>
<script type="math/tex; mode=display">
\sigma_i^0=\sqrt{(g_i^0)^2}=|g_i^0|</script><script type="math/tex; mode=display">
\sigma_i^1=\sqrt{\alpha(\sigma_i^0)^2+(1-\alpha)(g_i^1)^2}</script><script type="math/tex; mode=display">
...</script><script type="math/tex; mode=display">
\sigma_i^t=\sqrt{\alpha(\sigma_i^{t-1})^2+(1-\alpha)(g_i^t)^2}</script><p>其中$0&lt;\alpha&lt;1$，是一个<strong>hyperparameter</strong>。</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adam可以看作是<strong>SGDM+RMSProp</strong>。具体如下：<br>SGDM的原理为：</p>
<script type="math/tex; mode=display">
\theta^{t+1}=\theta^{t}-\eta m^t</script><script type="math/tex; mode=display">
m^{t}=\beta_1m^{t-1}+(1-\beta_1)g^{t}</script><p>RMSProp的原理为：</p>
<script type="math/tex; mode=display">
\theta_i^{t+1}=\theta_i^t-\frac{\eta}{\sigma_i^t}g_i^t</script><script type="math/tex; mode=display">
\sigma_i^0=\sqrt{(g_i^0)^2}=|g_i^0|</script><script type="math/tex; mode=display">
\sigma_i^t=\sqrt{\alpha(\sigma_i^{t-1})^2+(1-\alpha)(g_i^t)^2}</script><p>而<strong>Adam</strong>的方法为：</p>
<script type="math/tex; mode=display">
\theta^{t+1}=\theta^t-\frac{\eta}{\hat{\sigma}^t+\epsilon}\hat{m}^t</script><p>其中：</p>
<script type="math/tex; mode=display">
\hat{m}^t=\frac{m^t}{1-\beta_1^t}</script><script type="math/tex; mode=display">
\hat{\sigma}^t=\frac{\sigma^t}{\sqrt{1-\beta_2^t}}</script><p>一般情况下，$\beta_1=0.9,\eta_2=0.999$。$\epsilon$的作用是防止$\hat{\sigma}^t=0$而暴走，一般取$10^{-8}$。</p>
<p>[scode type=”green”]目前最常用的Optimization就是<strong>Adam</strong>。[/scode]</p>
<h3 id="Learning-Rate-Schedualing"><a href="#Learning-Rate-Schedualing" class="headerlink" title="Learning Rate Schedualing"></a>Learning Rate Schedualing</h3><p>Adaptive Learning Rate的基本方法是：</p>
<script type="math/tex; mode=display">
\theta_i^{t+1}=\theta_i^t-\frac{\eta}{\sigma_i^t}g_i^t</script><p>上面提到的<strong>gradient暴走</strong>问题在这几种方法中都会遇到，解决方法是使用<strong>Learning Rate Schedualing</strong>：</p>
<script type="math/tex; mode=display">
\theta_i^{t+1}=\theta_i^t-\frac{\eta^t}{\sigma_i^t}g_i^t</script><p>一种方法是<strong>Learning Rate Decay</strong></p>
<p><div align="center">
<img src="images/2024/07/2946435415.png" width="50%">
</div><br>随着train的进行，我们<strong>越来越接近destination</strong>，那么此时就可以减小$\eta$，使更新变得平稳。</p>
<p>另一种方法是<strong>Warm Up</strong></p>
<p><div align="center">
<img src="images/2024/07/2391515901.png" width="50%">
</div><br>一种可能的解释是：一开始，$\sigma_i^t$并不精准，所以先使用小的$\eta^t$在初始位置附近<strong>进行探索</strong>。</p>
<h2 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h2><p>train的时候，<strong>给Loss加上一个parameter的惩罚</strong>，就叫做L2 Regularization：</p>
<script type="math/tex; mode=display">
L_{l2}(\theta)=L(\theta)+\gamma||\theta||^2</script><p>对于SGD，有：</p>
<script type="math/tex; mode=display">
\theta^t=\theta^{t-1}-\nabla L_{l2}(\theta^{t-1})=\theta^{t-1}-\nabla L(\theta^{t-1})-\gamma\theta^{t-1}</script><p>对于SGDM，有：</p>
<script type="math/tex; mode=display">
\theta^t=\theta^{t-1}-\lambda m^{t-1}-\eta(\nabla L(\theta^{t-1})+\gamma\theta^{t-1})</script><p>那么：</p>
<script type="math/tex; mode=display">
m^t=\lambda m^{t-1}+\eta(\nabla L(\theta^{t-1})+\gamma\theta^{t-1})</script><p>对于Adam，有：</p>
<script type="math/tex; mode=display">
m^t=\lambda m^{t-1}+\eta(\nabla L(\theta^{t-1})+\gamma\theta^{t-1})</script><script type="math/tex; mode=display">
\sigma^t=\beta_2\sigma^{t-1}+(1-\beta_2)(\nabla(L(\theta^{t-1})+\gamma\theta^{t-1})</script><p>上述方法计算$m^t、\sigma^t$时，<strong>将$\gamma\theta^{t-1}$算作其中的一部分</strong>，叫做<strong>L2 regularization</strong>，<strong>否则叫做weight decay</strong>。</p>
<h2 id="Optimization总结"><a href="#Optimization总结" class="headerlink" title="Optimization总结"></a>Optimization总结</h2><p><img src="images/2024/07/379032510.png" alt="常见的Optimization"><br><img src="images/2024/07/397304109.png" alt="SGDM v.s. Adam"><br><img src="images/2024/07/2734923171.png" alt="Optimization advice"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/Week2-extra-General-Guide-in-training.html" data-id="cm4zf9dd50004y73k0h6l8t8m" data-title="[Week2 extra] General Guide in training" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-HW-1-Regreesion" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/HW-1-Regreesion.html" class="article-date">
  <time class="dt-published" datetime="2024-07-19T10:54:00.000Z" itemprop="datePublished">2024-07-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/HW-1-Regreesion.html">[HW 1]Regreesion</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>本次HW要求使用DNN完成一个COVID19的Regression，feature包括地区、相似症状、行为、精神状态，Regression目标为检测为tested positive cases。training data给出五天的资料，testing data给出四天的资料以及第五天的feature，要Regression第五天的tested positive cases。</p>
<ul>
<li>HW介绍：<a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/ml/ml2022-course-data/HW01.pdf">HW1 投影片介绍</a>；</li>
<li>data获取：<a target="_blank" rel="noopener" href="https://www.kaggle.com/competitions/ml2022spring-hw1/data">kaggle</a>；</li>
<li>示例代码：<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1FTcG6CE-HILnvFztEFKdauMlPKfQvm5Z#scrollTo=YdttVRkAfu2t">colab</a>；</li>
</ul>
<p>记录一下完成本次HW的总结与收获，最终在Kaggle的score为0.9左右。</p>
<h1 id="导入必要的包"><a href="#导入必要的包" class="headerlink" title="导入必要的包"></a>导入必要的包</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># numerical operation</span><br><span class="line">import numpy as np</span><br><span class="line">import math</span><br><span class="line"># reading/writing data</span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">import csv</span><br><span class="line"># progress bar</span><br><span class="line">from tqdm import tqdm</span><br><span class="line"># pytorch</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.utils.data import Dataset, DataLoader, random_split</span><br><span class="line"># plotting learning curve</span><br><span class="line">from torch.utils.Tensorboard import SummaryWriter</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line">import scipy.stats as stats</span><br></pre></td></tr></table></figure>
<h1 id="定义一些必要的函数"><a href="#定义一些必要的函数" class="headerlink" title="定义一些必要的函数"></a>定义一些必要的函数</h1><h2 id="固定随机数种子"><a href="#固定随机数种子" class="headerlink" title="固定随机数种子"></a>固定随机数种子</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def same_seed(seed):</span><br><span class="line">    torch.backends.cudnn.deterministic = True</span><br><span class="line">    torch.backends.cudnn.benchmark = False</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    if torch.cuda.is_available():</span><br><span class="line">        torch.cuda.manual_seed_all(seed)</span><br></pre></td></tr></table></figure>
<p>其作用是确保<strong>实验的可重复性</strong>，便于<strong>复现实验结果</strong>。</p>
<ul>
<li>‘torch.backends.cudnn.deterministic = True’：强制cuDNN使用<strong>确定性算法</strong>，使得同样的输入会产生<strong>同样的输出</strong>；</li>
<li>‘torch.backends.cudnn.benchmark = False’：使得cuDNN使用一种<strong>预定义的选择算法</strong>来选择合适的内核，而不是基于当前配置<strong>选择最佳内核</strong>，可以进一步确保<strong>结果的一致性</strong>，但会降低效率；</li>
<li>‘np.random.seed(seed)’：设置<strong>numpy的随机数生成种子</strong>，使numpy生成的随机数序列是可重复的；</li>
<li>‘torch.manual_seed(seed)’：设置<strong>PyTorch的CPU随机数生成种子</strong>，确保<strong>PyTorch在CPU上生成的随机数</strong>是可重复的；</li>
<li>‘torch.cuda.manual_seed_all(seed)’：设置<strong>PyTorch的GPU随机数生成种子</strong>，作用类似于上。</li>
</ul>
<h2 id="划分train-set和valid-set"><a href="#划分train-set和valid-set" class="headerlink" title="划分train_set和valid_set"></a>划分train_set和valid_set</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def train_valid_split(data_set, valid_ratio, seed):</span><br><span class="line">    valid_set_size = int(valid_ratio * len(data_set)) </span><br><span class="line">    train_set_size = len(data_set) - valid_set_size</span><br><span class="line">    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))</span><br><span class="line">    return np.array(train_set), np.array(valid_set)</span><br></pre></td></tr></table></figure>
<p>以<strong>valid_ratio为比例</strong>将整个train data划分为<strong>train set和valid set</strong>。在训练的时候，使用train set进行训练，并使用valid set对本次训练进行验证，目的是<strong>让模型在未见过的data上进行验证，提高模型泛化能力</strong>。这在模型训练中十分重要：</p>
<ul>
<li>能帮助<strong>选择最佳Model和hyperparameters</strong>；</li>
<li>能防止Model<strong>过拟合</strong>；</li>
<li>可以用于<strong>早停</strong>。</li>
</ul>
<h2 id="预测test-data"><a href="#预测test-data" class="headerlink" title="预测test data"></a>预测test data</h2><p>两种方法：</p>
<ul>
<li>将test data转化为<strong>tensor</strong>后feed给Model进行predict；</li>
<li>将test data转化为<strong>DataLoader</strong>后feed给Model进行predict。</li>
</ul>
<p>下面介绍一下两种方法的差异：</p>
<h3 id="直接转化为tensor"><a href="#直接转化为tensor" class="headerlink" title="直接转化为tensor"></a>直接转化为tensor</h3><p>假设将test data的有效feature提取出到一个numpy数组x_test中，由于<strong>model接收的参数是tensor</strong>，因此需要对其进行类型转化：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_tensor = torch.from_numpy(x_test)</span><br></pre></td></tr></table></figure><br>另外，<strong>输入tensor的数据类型要和model的parameters的类型保持一致</strong>，假设模型parameter是float（float32）类型，而numpy数组转化默认是double（float64）类型，因此需要再进行一步转化：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_tensor = test_tensor.float()</span><br></pre></td></tr></table></figure><br>如果是在gpu进行训练，还要进一步<strong>将tensor转移到gpu上</strong>：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_tensor = test_tensor.to(device)</span><br></pre></td></tr></table></figure><br>随后即可将其feed给model，得到预测结果：<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pred = model(test_tensor).numpy()</span><br></pre></td></tr></table></figure><br>这样做的优点：</p>
<ul>
<li>代码简单；</li>
<li>对于<strong>小型数据集</strong>，简单的步骤可以减少<strong>潜在的延迟</strong>。</li>
</ul>
<p>这样做的缺点：</p>
<ul>
<li>对于<strong>大型数据集</strong>，直接加载可能导致<strong>内存问题</strong>；</li>
<li>不能方便的控制<strong>批大小、异步加载和数据增强</strong>。</li>
</ul>
<h3 id="转化为DataLoader后feed给model"><a href="#转化为DataLoader后feed给model" class="headerlink" title="转化为DataLoader后feed给model"></a>转化为DataLoader后feed给model</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def predict(test_loader, model, device):</span><br><span class="line">    model.eval()  # 将model转化为evaluate模式</span><br><span class="line">    preds = []</span><br><span class="line">    for x in tqdm(test_loader):</span><br><span class="line">        x = x.to(device)</span><br><span class="line">        with torch.no_grad():  # 禁用梯度计算</span><br><span class="line">            pred = model(x)</span><br><span class="line">            preds.append(pred.detach().cpu())</span><br><span class="line">    preds = torch.cat(preds, dim=0).numpy()  # concatenate</span><br><span class="line">    return preds</span><br></pre></td></tr></table></figure>
<p>解释一下’preds.append(pred.detach().cpu())’：model输出的pred是一个tensor，这里的目的是将<strong>每个batch的ouput放到一个list中最后拼接为一个大的tensor</strong>。其中:</p>
<ul>
<li><strong>detach方法</strong>的目的是将pred<strong>从计算图中分离出来</strong>，其原因是<strong>pytorch会在训练过程中跟踪计算图</strong>，以便在<strong>Back Pass时计算梯度</strong>。而predict阶段不需要Back Pass，因此将结果分离出来可以<strong>节省内存</strong>。</li>
<li><strong>cpu方法</strong>的目的是将pred从gpu移动到cpu。其原因是在多数情况下，模型的预测结果需要进一步保存或处理，<strong>这些操作需要在cpu上进行</strong>。另外，<strong>cpu上的数据更容易与其他库（如numpy）进行兼容</strong>。</li>
</ul>
<p>这样做的优点：</p>
<ul>
<li>适合处理<strong>大规模数据</strong>，DataLoader会<strong>分批加载数据</strong>，避免内存爆掉；</li>
<li>DataLoader可以设置批大小和并行数据加载，<strong>提高数据处理效率</strong>。</li>
</ul>
<p>这样做的缺点：</p>
<ul>
<li>代码更加复杂；</li>
<li>对于<strong>小型数据</strong>，其初始化和批处理可能导致<strong>微小延迟</strong>。</li>
</ul>
<p>鉴于本次hw的test data数量大，所以使用了后者。</p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class hw1_Dataset(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, x, y=None):</span><br><span class="line">        if y is None:</span><br><span class="line">            self.y = y</span><br><span class="line">        else:</span><br><span class="line">            self.y = torch.FloatTensor(y)</span><br><span class="line">        self.x = torch.FloatTensor(x)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        if self.y is None:</span><br><span class="line">            return self.x[idx]</span><br><span class="line">        else:</span><br><span class="line">            return self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.x)</span><br></pre></td></tr></table></figure>
<p>[scode type=”yellow”]自己的Dataset要继承pytorch里的Dataset，<strong>必须重写<strong>init</strong>、<strong>getitem</strong>、<strong>len</strong>这几个方法</strong>！[/scode]<br>由于本次hw是将test data转化为DataLoader后feed给model的，因此也需要test data的Dataset。而<strong>test data是没有y（Regression目标）的</strong>，因此对其进行了特殊处理。</p>
<h1 id="选择feature"><a href="#选择feature" class="headerlink" title="选择feature"></a>选择feature</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(&#x27;./covid.train.csv&#x27;).values</span><br><span class="line">district = [&#x27;AL&#x27;,&#x27;AK&#x27;,&#x27;AZ&#x27;,&#x27;AR&#x27;,&#x27;CA&#x27;,&#x27;CO&#x27;,&#x27;CT&#x27;,&#x27;FL&#x27;,&#x27;GA&#x27;,&#x27;ID&#x27;,&#x27;IL&#x27;,&#x27;IN&#x27;,&#x27;IA&#x27;,&#x27;KS&#x27;,&#x27;KY&#x27;,&#x27;LA&#x27;,&#x27;MD&#x27;,&#x27;MA&#x27;,\</span><br><span class="line">       &#x27;MI&#x27;,&#x27;MN&#x27;,&#x27;MS&#x27;,&#x27;MO&#x27;,&#x27;NE&#x27;,&#x27;NV&#x27;,&#x27;NJ&#x27;,&#x27;NM&#x27;,&#x27;NY&#x27;,&#x27;NC&#x27;,&#x27;OH&#x27;,&#x27;OK&#x27;,&#x27;OR&#x27;,&#x27;RI&#x27;,&#x27;SC&#x27;,&#x27;TX&#x27;,&#x27;UT&#x27;,&#x27;VA&#x27;,&#x27;WA&#x27;]</span><br><span class="line">district_data = train_data[:,1:38]</span><br><span class="line">grouped_data = &#123;&#125;</span><br><span class="line">for i in range(train_data.shape[0]):</span><br><span class="line">    idx = np.argmax(district_data[i,:])</span><br><span class="line">    if district[idx] not in grouped_data:</span><br><span class="line">        grouped_data[district[idx]] = []</span><br><span class="line">    grouped_data[district[idx]].append(train_data[i,-1])</span><br><span class="line">h, p = stats.kruskal(*grouped_data.values())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_effect_size(h, n, k):</span><br><span class="line">    return (h * (n + 1) - 2 * (k - 1)) / (n - k)</span><br><span class="line"></span><br><span class="line">n_total = sum(len(e) for e in grouped_data.values())</span><br><span class="line">k_groups = len(grouped_data)</span><br><span class="line">effect_size = compute_effect_size(h, n_total, k_groups)</span><br><span class="line">print(f&#x27;kruskal-wallis effect size &#123;effect_size&#125;&#x27;)</span><br><span class="line"></span><br><span class="line"># 选择有效feature</span><br><span class="line">feature_list = []</span><br><span class="line">p_list = []</span><br><span class="line">y = train_data[:,-1]</span><br><span class="line">for i in range(38, train_data.shape[1] - 1):</span><br><span class="line">    x = train_data[:,i]</span><br><span class="line">    corr, p = stats.spearmanr(x, y)</span><br><span class="line">    if math.fabs(corr) &gt; 0.8:  # 找到相关系数大于0.8的feature</span><br><span class="line">        feature_list.append(i)</span><br><span class="line"></span><br><span class="line">print(feature_list)</span><br></pre></td></tr></table></figure>
<p>地区是<strong>分类变量</strong>，使用了onehot编码，这里使用了Kruskal-Wallis检验，但是偷了个懒，并没有检查合不合理，只是说<strong>这种检验可以检验分类变量和连续变量的相关性</strong>，就用了。<br>其他feature都是连续变量，用了spearman相关系数检验了一下，选择了相关系数大于0.8的feature（就是随便取了一个数…）。</p>
<h1 id="将feature和y从data中分离出来"><a href="#将feature和y从data中分离出来" class="headerlink" title="将feature和y从data中分离出来"></a>将feature和y从data中分离出来</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def select_feature(train_data, valid_data, test_data, select_all=True):</span><br><span class="line">    &#x27;&#x27;&#x27;Selects useful features to perform regression&#x27;&#x27;&#x27;</span><br><span class="line">    y_train, y_valid = train_data[:, -1], valid_data[:, -1]</span><br><span class="line">    raw_x_train, raw_x_valid, raw_x_test = train_data[:, :-1], valid_data[:, :-1], test_data</span><br><span class="line"></span><br><span class="line">    if select_all:</span><br><span class="line">        feat_idx = list(range(raw_x_train.shape[1]))</span><br><span class="line">    else:</span><br><span class="line">        feat_idx = list(range(1, 38)) + [38, 39, 40, 41, 53, 54, 55, 56, 57, 69, 70, 71, 72, 73, 85, 86, 87, 88, 89,</span><br><span class="line">                                         101, 102, 103, 104, 105]</span><br><span class="line"></span><br><span class="line">    return raw_x_train[:, feat_idx], raw_x_valid[:, feat_idx], raw_x_test[:, feat_idx], y_train, y_valid</span><br></pre></td></tr></table></figure>
<p>这里<strong>select_all的设计</strong>还是比较精妙的，为True就选取所有feature，为False就选取指定feature，改起来也比较方便直观。</p>
<h1 id="定义Model"><a href="#定义Model" class="headerlink" title="定义Model"></a>定义Model</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class hw1_model(nn.Module):</span><br><span class="line">    # define model</span><br><span class="line">    def __init__(self, input_dim):</span><br><span class="line">        super(hw1_model, self).__init__()</span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, 64),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(64, 8),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(8, 4),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(4, 1)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.layers(x)</span><br><span class="line">        x = x.squeeze(1)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>
<p>这里就是随便设计了一个structure。</p>
<h1 id="Hyperparameters的设置"><a href="#Hyperparameters的设置" class="headerlink" title="Hyperparameters的设置"></a>Hyperparameters的设置</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># config:contains hyper-parameters for training and the path to save your model.</span><br><span class="line">device = &#x27;cuda&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;</span><br><span class="line">config = &#123;</span><br><span class="line">    &#x27;seed&#x27;: 5201314,</span><br><span class="line">    &#x27;select_all&#x27;: False,</span><br><span class="line">    &#x27;valid_ratio&#x27;: 0.2,</span><br><span class="line">    &#x27;n_epochs&#x27;: 3000,</span><br><span class="line">    &#x27;batch_size&#x27;: 128,</span><br><span class="line">    &#x27;learning_rate&#x27;: 1e-5,</span><br><span class="line">    &#x27;early_stop&#x27;: 400,  # 如果模型400 epoch没有优化就break</span><br><span class="line">    &#x27;save_path&#x27;: &#x27;./models/model.ckpt&#x27;,</span><br><span class="line">    &#x27;weight_decay&#x27;: 1e-3</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这个设计也是非常精妙的，<strong>将所有的Hyperparameters放到一个dictionary中</strong>，方便后面调参，整个代码框架看起来也比较顺眼。</p>
<h1 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">same_seed(config[&#x27;seed&#x27;])</span><br><span class="line"></span><br><span class="line">train_data, test_data = pd.read_csv(&#x27;./covid.train.csv&#x27;).values, pd.read_csv(&#x27;./covid.test.csv&#x27;).values</span><br><span class="line">train_data, valid_data = train_valid_split(train_data, config[&#x27;valid_ratio&#x27;], config[&#x27;seed&#x27;])</span><br><span class="line"></span><br><span class="line">print(f&quot;&quot;&quot;train_data size: &#123;train_data.shape&#125;</span><br><span class="line">valid_data size: &#123;valid_data.shape&#125;</span><br><span class="line">test_data size: &#123;test_data.shape&#125;&quot;&quot;&quot;)</span><br><span class="line"></span><br><span class="line">x_train, x_valid, x_test, y_train, y_valid = select_feature(train_data, valid_data, test_data, config[&#x27;select_all&#x27;])</span><br><span class="line"></span><br><span class="line">print(f&#x27;number of features: &#123;x_train.shape[1]&#125;&#x27;)</span><br><span class="line"></span><br><span class="line">train_dataset, valid_dataset, test_dataset = hw1_Dataset(x_train, y_train), \</span><br><span class="line">    hw1_Dataset(x_valid, y_valid), \</span><br><span class="line">    hw1_Dataset(x_test)</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=True, pin_memory=True)</span><br><span class="line">valid_loader = DataLoader(valid_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=True, pin_memory=True)</span><br><span class="line">test_loader = DataLoader(test_dataset, batch_size=config[&#x27;batch_size&#x27;], shuffle=False, pin_memory=True)</span><br></pre></td></tr></table></figure>
<p>这部分用来对数据进行与处理，将其包装到DataLoader中。小细节：pin_memory=True时，PyTorch会把数据加载到内存的固定位置，<strong>提高传输到gpu的效率</strong>。<br>[scode type=”yellow”]一定要注意：<strong>别把test_loader</strong>给shuffle了**，不然submission的时候会有惊喜。[/scode]</p>
<h1 id="Training-Loop"><a href="#Training-Loop" class="headerlink" title="Training Loop"></a>Training Loop</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">def trainer(train_loader, valid_loader, model, config, device):</span><br><span class="line">    criterion = nn.MSELoss(reduction=&#x27;mean&#x27;)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.SGD(model.parameters(), lr=config[&#x27;learning_rate&#x27;], momentum=0.95,</span><br><span class="line">                                weight_decay=config[&#x27;weight_decay&#x27;])</span><br><span class="line"></span><br><span class="line">    writer = SummaryWriter()  # Writer of tensoboard.</span><br><span class="line"></span><br><span class="line">    if not os.path.isdir(&#x27;./models&#x27;):</span><br><span class="line">        os.mkdir(&#x27;./models&#x27;)  # Create directory of saving models.</span><br><span class="line"></span><br><span class="line">    n_epochs, best_loss, step, early_stop_count = config[&#x27;n_epochs&#x27;], math.inf, 0, 0</span><br><span class="line"></span><br><span class="line">    for epoch in range(n_epochs):</span><br><span class="line">        model.train()  # Set your model to train mode.</span><br><span class="line">        loss_record = []</span><br><span class="line"></span><br><span class="line">        # tqdm is a package to visualize your training progress.</span><br><span class="line">        train_pbar = tqdm(train_loader, position=0, leave=True)</span><br><span class="line"></span><br><span class="line">        for x, y in train_pbar:</span><br><span class="line">            optimizer.zero_grad()  # Set gradient to zero.</span><br><span class="line">            x, y = x.to(device), y.to(device)  # Move your data to device.</span><br><span class="line">            pred = model(x)</span><br><span class="line">            loss = criterion(pred, y)</span><br><span class="line">            loss.backward()  # Compute gradient(backpropagation).</span><br><span class="line">            optimizer.step()  # Update parameters.</span><br><span class="line">            step += 1</span><br><span class="line">            loss_record.append(loss.detach().item())</span><br><span class="line"></span><br><span class="line">            # Display current epoch number and loss on tqdm progress bar.</span><br><span class="line">            train_pbar.set_description(f&#x27;Epoch [&#123;epoch + 1&#125;/&#123;n_epochs&#125;]&#x27;)</span><br><span class="line">            train_pbar.set_postfix(&#123;&#x27;loss&#x27;: loss.detach().item()&#125;)</span><br><span class="line"></span><br><span class="line">        mean_train_loss = sum(loss_record) / len(loss_record)</span><br><span class="line">        writer.add_scalar(&#x27;Loss/train&#x27;, mean_train_loss, step)</span><br><span class="line"></span><br><span class="line">        model.eval()  # Set your model to evaluation mode.</span><br><span class="line">        loss_record = []</span><br><span class="line">        for x, y in valid_loader:</span><br><span class="line">            x, y = x.to(device), y.to(device)</span><br><span class="line">            with torch.no_grad():</span><br><span class="line">                pred = model(x)</span><br><span class="line">                loss = criterion(pred, y)</span><br><span class="line"></span><br><span class="line">            loss_record.append(loss.item())</span><br><span class="line"></span><br><span class="line">        mean_valid_loss = sum(loss_record) / len(loss_record)</span><br><span class="line">        print(f&#x27;Epoch [&#123;epoch + 1&#125;/&#123;n_epochs&#125;]: Train loss: &#123;mean_train_loss:.4f&#125;, Valid loss: &#123;mean_valid_loss:.4f&#125;&#x27;)</span><br><span class="line">        writer.add_scalar(&#x27;Loss/valid&#x27;, mean_valid_loss, step)</span><br><span class="line"></span><br><span class="line">        if mean_valid_loss &lt; best_loss:</span><br><span class="line">            best_loss = mean_valid_loss</span><br><span class="line">            torch.save(model.state_dict(), config[&#x27;save_path&#x27;])  # Save your best model</span><br><span class="line">            print(&#x27;Saving model with loss &#123;:.3f&#125;...&#x27;.format(best_loss))</span><br><span class="line">            early_stop_count = 0</span><br><span class="line">        else:</span><br><span class="line">            early_stop_count += 1</span><br><span class="line"></span><br><span class="line">        if early_stop_count &gt;= config[&#x27;early_stop&#x27;]:</span><br><span class="line">            print(&#x27;\nModel is not improving, so we halt the training session.&#x27;)</span><br><span class="line">            return</span><br></pre></td></tr></table></figure>
<p>这里要注意的就是在Optimizer中加了一个L2正则化（权重衰减，weight decay），即<strong>optimizer定义中中的weight_decay部分所实现的功能</strong>。目前只了解到其是<strong>用于防止机器学习模型过拟合的一种常用技术</strong>。L2正则化<strong>通过在损失函数中增加一个正则化项来限制模型的权重大小，从而减少模型的复杂度</strong>。</p>
<h1 id="开始训练并保存结果"><a href="#开始训练并保存结果" class="headerlink" title="开始训练并保存结果"></a>开始训练并保存结果</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># strat training</span><br><span class="line">start_time = time.time()</span><br><span class="line">model = hw1_model(input_dim=x_train.shape[1]).to(device)</span><br><span class="line">trainer(train_loader, valid_loader, model, config, device)</span><br><span class="line">end_time = time.time()</span><br><span class="line">print(f&#x27;程序运行时间 &#123;end_time - start_time&#125;&#x27;)</span><br><span class="line"></span><br><span class="line"># save result</span><br><span class="line">def save_pred(preds, file):</span><br><span class="line">    &#x27;&#x27;&#x27; Save predictions to specified file &#x27;&#x27;&#x27;</span><br><span class="line">    with open(file, &#x27;w&#x27;) as fp:</span><br><span class="line">        writer = csv.writer(fp)</span><br><span class="line">        writer.writerow([&#x27;id&#x27;, &#x27;tested_positive&#x27;])</span><br><span class="line">        for i, p in enumerate(preds):</span><br><span class="line">            writer.writerow([i, p])</span><br><span class="line"></span><br><span class="line">model = hw1_model(input_dim=x_train.shape[1]).to(device)</span><br><span class="line">model.load_state_dict(torch.load(config[&#x27;save_path&#x27;]))</span><br><span class="line">preds = predict(test_loader, model, device)</span><br><span class="line">save_pred(preds, &#x27;pred.csv&#x27;)</span><br></pre></td></tr></table></figure>
<p>最后就是进行模型的训练，这里用time方法记录了一下训练过程的用时。<br>保存结果的部分，首先就是save_pred中的<strong>enumerate</strong>，是一个内置函数，可以<strong>迭代一个容器的同时把下标和元素同时返回</strong>。然后就是加载已经保存的最优model，进行predict，保存结果。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>做了这次的hw收获还是很多的，对示例中的code进行了理解、调整，学了很多实现的写法，了解到了很多之前不留意的小细节，也算是第一次自己跑正式一点的模型吧。不得不说，专业的代码框架设计确实非常精妙，可读性高，易修改，仿照这个框架做一下子就牛逼起来了。</p>
<p>本次hw代码：<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1k9rBeMGZF6FSBYqflJ2gzSFYm_BHU4Vr#scrollTo=T6Dl0CrRbUzJ">colab</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/HW-1-Regreesion.html" data-id="cm4zf9dd20000y73khv9q7sdj" data-title="[HW 1]Regreesion" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Week1-Extra-用Logistic-Regression实现classification" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/Week1-Extra-%E7%94%A8Logistic-Regression%E5%AE%9E%E7%8E%B0classification.html" class="article-date">
  <time class="dt-published" datetime="2024-07-15T05:14:00.000Z" itemprop="datePublished">2024-07-15</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/Week1-Extra-%E7%94%A8Logistic-Regression%E5%AE%9E%E7%8E%B0classification.html">[Week1 Extra] 用Logistic Regression实现classification</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Generative-Model"><a href="#Generative-Model" class="headerlink" title="Generative Model"></a>Generative Model</h1><p>假设有两个类别class1和class2，从class1中抽取一个样本的概率为$P(C_1)$，从class2中抽取一个样本的概率为$P(C_2)$。现<strong>给定一个x</strong>，求其属于哪个class，即求$P(C_1|x)$或$P(C_2|x)$（<strong>posterior probability</strong>）。<br><strong>根据Bayes公式</strong>，有：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}</script><p>其中$P(C_1)$和$P(C_2)$叫做<strong>Prior</strong>；$P(x|C_1)$为$C_1$产生$x$的概率，$P(x|C_2)$为$C_2$产生$x$的概率。如果$P(C_1|x)&gt;0.5$，则代表这个$x$属于$C_1$；反之则属于$C_2$。</p>
<p>在这个公式中，$P(C_1)$和$P(C_1)$都是非常容易求得的，重点在于$P(x|C_1)$和$P(x|C_2)$。下面我们<strong>假设$C_1$和$C_2$都是从一个Gaussion Distribution中sample出来的</strong>。<br>[scode type=”share”]<strong>Gaussion Ditribution</strong></p>
<script type="math/tex; mode=display">
f_{\mu,\Sigma}(x)=\frac{1}{(2\pi)^{(D/2)}}\frac{1}{|\Sigma|^{(1/2)}}exp\{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\}</script><p>其中$\mu$为样本mean，$\Sigma$为样本covariance matrix[/scode]<br>只要我们找到了这样的一个Gaussion Ditribution，然后将$x$的代入概率密度函数，即可得到$P(x|C_1)$和$P(x|C_2)$。</p>
<h2 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h2><p>那么如何找到它的密度函数呢？使用<strong>Maximum Likelihood</strong>的方法。<br>一个mean为$\mu$、covariance matrix为$\Sigma$的Gaussion Distribution的Likelihood为<strong>它sample出$x^1,x^2,…,x^N$的概率</strong>。我们用$L(\mu,\Sigma)$来表示Likelihood，有：</p>
<script type="math/tex; mode=display">
L(\mu,\Sigma)=f_{\mu,\Sigma}(x^1)f_{\mu,\Sigma}(x^2)...f_{\mu,\Sigma}(x^N)</script><p>那么我们要找到$\mu^<em>,\Sigma^</em>$，使得$L(\mu,\Sigma)$最大，即：</p>
<script type="math/tex; mode=display">
\mu^*,\Sigma^*=\mathop{\arg\max}\limits_{\mu,\Sigma}L(\mu,\Sigma)</script><p>最终解为：</p>
<script type="math/tex; mode=display">
\mu^*=\frac{1}{N}\sum\limits_{n=1}^Nx^n</script><script type="math/tex; mode=display">
\Sigma^*=\frac{1}{N}\sum\limits_{n=1}^N(x^n-\mu^*)(x^n-\mu^*)^T</script><p>那么此时，将$x$分别代入class1和class2的密度函数，即可得到$P(x|C_1)$和$P(x|C_2)$：</p>
<script type="math/tex; mode=display">
P(x|C_1)=f_{\mu^1,\Sigma^1}(x)=\frac{1}{(2\pi)^{(D/2)}}\frac{1}{|\Sigma^1|^{(1/2)}}exp\{-\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)\}</script><script type="math/tex; mode=display">
P(x|C_2)=f_{\mu^2,\Sigma^2}(x)=\frac{1}{(2\pi)^{(D/2)}}\frac{1}{|\Sigma^2|^{(1/2)}}exp\{-\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)\}</script><p>实际使用中，往往<strong>多个class的Gaussion Distribution采用相同的$\Sigma$</strong>，因为covariance matrix的元素个数是class feature数量的平方，当feature较多时，如果采用不同的covariance matirx，会使得parameter数量增长非常快，<strong>容易导致over fitting</strong>。<br>这时，计算Likelihood的方法为：</p>
<script type="math/tex; mode=display">
L(\mu^1,\mu^2,\Sigma)=f_{\mu^1,\Sigma}(x^1)f_{\mu^1,\Sigma}(x^2)...f_{\mu^1,\Sigma}(x^k)+f_{\mu^2,\Sigma}(x^{k+1})f_{\mu^2,\Sigma}(x^{k+2})...f_{\mu^2,\Sigma}(x^N)</script><p>其中$x^1,x^2,…,x^k$来自class1；$x^{k+1},x^{k+2},…,x^N$来自class2。<br>此时$\mu^<em>$的算法同上，$\Sigma^</em>$的解为：</p>
<script type="math/tex; mode=display">
\Sigma^*=\frac{k}{N}\Sigma^1+\frac{N-k}{N}\Sigma^2</script><p><strong>这样得到的两个class的boundary是linear的</strong>，而使用不同的covariance matrix得到的boundary是non-linear的。</p>
<p>以上计算以Gaussion Distribution为例，但实际使用中需要根据背景选择合适的distribution。例如如果<strong>某一个feature是binary</strong>的，那么就应该选择Bernoulli distribution；如果<strong>所有dimension都是独立产生</strong>的，那么就应该选择Naive Bayes Classifier。</p>
<h2 id="对于Posterior-Probability的思考"><a href="#对于Posterior-Probability的思考" class="headerlink" title="对于Posterior Probability的思考"></a>对于Posterior Probability的思考</h2><p>我们对Posterior Probability的计算公式进行进一步推导：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1)+P(x|C_2)P(C_2)}</script><script type="math/tex; mode=display">
=\frac{1}{1+\frac{P(x|C_2)P(C_2)}{P(x|C_1)P(C_1)}}=\frac{1}{1+exp(-z)}=\sigma(z)</script><p>其中：</p>
<script type="math/tex; mode=display">
z=ln\frac{P(x|C_1)P(C_1)}{P(x|C_2)P(C_2)}=ln\frac{P(x|C_1)}{P(x|C_2)}+ln\frac{P(C_1)}{P(C_2)}</script><p>有：</p>
<script type="math/tex; mode=display">
\frac{P(C_1)}{P(C_2)}=\frac{\frac{N_1}{N_1+N_2}}{\frac{N_2}{N_1+N_2}}=\frac{N_1}{N_2}</script><script type="math/tex; mode=display">
P(x|C_1)=f_{\mu^1,\Sigma^1}(x)=\frac{1}{(2\pi)^{(D/2)}}\frac{1}{|\Sigma^1|^{(1/2)}}exp\{-\frac{1}{2}(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)\}</script><script type="math/tex; mode=display">
P(x|C_2)=f_{\mu^2,\Sigma^2}(x)=\frac{1}{(2\pi)^{(D/2)}}\frac{1}{|\Sigma^2|^{(1/2)}}exp\{-\frac{1}{2}(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)\}</script><p>那么可以得到：</p>
<script type="math/tex; mode=display">
ln\frac{P(x|C_1)}{P(x|C_2)}=ln\frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}}-\frac{1}{2}[(x-\mu^1)^T(\Sigma^1)^{-1}(x-\mu^1)-(x-\mu^2)^T(\Sigma^2)^{-1}(x-\mu^2)]</script><p>展开后代入$z$得：</p>
<script type="math/tex; mode=display">
z=ln\frac{|\Sigma^2|^{1/2}}{|\Sigma^1|^{1/2}}-\frac{1}{2}x^T(\Sigma^1)^{-1}+(\mu^1)^T(\Sigma^1)^{-1}x-\frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1</script><script type="math/tex; mode=display">
+\frac{1}{2}x^T(\Sigma^2)^{-1}-(\mu^2)^T(\Sigma^2)^{-1}x+\frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2+ln\frac{N_1}{N_2}</script><p>在我们取相同的covariance matrix的情况下，上式可以化简为：</p>
<script type="math/tex; mode=display">
z=(\mu^1)^T(\Sigma^1)^{-1}x-\frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1-(\mu^2)^T(\Sigma^2)^{-1}x+\frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2+ln\frac{N_1}{N_2}</script><p>进一步化简为：</p>
<script type="math/tex; mode=display">
z=(\mu^1-\mu^2)^T\Sigma^{-1}x-\frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2+ln\frac{N_1}{N_2}</script><p>我们将$(\mu^1-\mu^2)^T\Sigma^{-1}$记作$W^T$，由于$-\frac{1}{2}(\mu^1)^T(\Sigma^1)^{-1}\mu^1+\frac{1}{2}(\mu^2)^T(\Sigma^2)^{-1}\mu^2+ln\frac{N_1}{N_2}$是一个scalar，记作$b$。于是我们得到了最终的表达式：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(w\cdot x+b)</script><p>从这里也能看出，为什么共用$\Sigma$的时候，boundary是linear的。</p>
<hr>
<p>上述Classification叫做<strong>Generative Model</strong>。在Generative Model中，我们会计算$N_1,N_2,\mu^1,\mu^2,\Sigma$，得到结果后，代入上式，即可得到结果。</p>
<h1 id="Discriminative-Model"><a href="#Discriminative-Model" class="headerlink" title="Discriminative Model"></a>Discriminative Model</h1><h2 id="Two-class-Classification"><a href="#Two-class-Classification" class="headerlink" title="Two-class Classification"></a>Two-class Classification</h2><p>上文我们最后总结了Posterior Probability的表达式：</p>
<script type="math/tex; mode=display">
P(C_1|x)=\sigma(w\cdot x+b)</script><p>并且用数学推导的方式得到了最终结果，下文我们将使用Gradient Descent方法求解。<br>给出如下training data：<br>|$x^1$|$x^2$|$x^3$|…|$x^N$|<br>|:-:|:-:|:-:|:-:|:-:|<br>|$C_1$|$C_1$|$C_2$|…|$C_1$|</p>
<p><strong>training data的形式为：$(x^n,\hat{y}^n$，其中$\hat{y}^n=1$代表其属于$C_1$，$\hat{y}^n=0$代表其属于$C_2$。</strong><br>我们假设这组data是基于$f_{w,b}=P_{w,b}(C_1|x)$产生的，现在要找到一组$w,b$，使得其最有可能产生这组data。<br>根据Generative Model的学习，我们可以定义Likelihood函数：</p>
<script type="math/tex; mode=display">
L(w,b)=f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^1))...f_{w,b}(x^N)</script><p>我们要求：</p>
<script type="math/tex; mode=display">
w^*,b^*=\mathop{\arg\max}\limits_{w,b}L(w,b)</script><p>对其进行变形，可得：</p>
<script type="math/tex; mode=display">
w^*,b^*=\mathop{\arg\min}\limits_{w,b}-lnL(w,b)</script><p>进一步推导，得到：</p>
<script type="math/tex; mode=display">
-lnL(w,b)=-lnf_{w,b}(x^1)-lnf_{w,b}(x^2)-ln(1-f_{w,b}(x^1))-...</script><script type="math/tex; mode=display">
=-[\hat{y^1}lnf(x^1)+(1-\hat{y^1})ln(1-f(x^1))]-[\hat{y^2}lnf(x^2)+(1-\hat{y^2})ln(1-f(x^2))]</script><script type="math/tex; mode=display">
-[\hat{y^3}lnf(x^3)+(1-\hat{y^3})ln(1-f(x^3))]-...</script><p>即：</p>
<script type="math/tex; mode=display">
-lnL(w,b)=\sum\limits_n-[\hat{y^n}lnf_{w,b}(x^n)+(1-\hat{y^n})ln(1-f_{w,b}(x^n))]</script><p>综上所述，我们定义了Model：$f_{w,b}(x)=\sigma(\sum\limits_iw_ix_i+b)=\frac{1}{1+exp(-z)}$，其中$z=wx+b=\sum\limits_iw_ix_i+b$。然后，我们可以使用Gradient Descent对其进行Optimization。接下来求解$\frac{\partial -lnL(w,b)}{\partial w_i}$：</p>
<script type="math/tex; mode=display">
\frac{-lnL(w,b)}{\partial w_i}=\sum\limits_n-[\hat{y^n}\frac{lnf_{w,b}(x^n)}{\partial w_i}+(1-\hat{y^n})\frac{ln(1-f_{w,b}(x^n))}{\partial w_i}]</script><p>其中：</p>
<script type="math/tex; mode=display">
\frac{lnf_{w,b}(x^n)}{\partial w_i}=\frac{\partial lnf_{w,b}(x^n)}{\partial z}\frac{\partial z}{\partial w_i}</script><p>而：</p>
<script type="math/tex; mode=display">
\frac{\partial lnf_{w,b}(x^n)}{\partial z}=\frac{1}{\sigma(z)}\frac{\partial\sigma(z)}{\partial z}=\frac{1}{\sigma(z)}\sigma(z)(1-\sigma(z))</script><script type="math/tex; mode=display">
\frac{\partial z}{\partial w_i}=x_i</script><p>则：</p>
<script type="math/tex; mode=display">
\frac{\partial lnf_{w,b}(x^n)}{\partial z}=\frac{\partial lnf_{w,b}(x^n)}{\partial z}\frac{\partial z}{\partial w_i}=(1-f(x^n))x_i^n</script><p>同理可求得：</p>
<script type="math/tex; mode=display">
\frac{ln(1-f_{w,b}(x^n))}{\partial w_i}=f(x^n)x_i^n</script><p>最后，将其代入原式，得到：</p>
<script type="math/tex; mode=display">
\frac{-lnL(w,b)}{\partial w_i}=\sum\limits_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n</script><p>至此，我们得到了<strong>Gradient Descent</strong>的公式：</p>
<script type="math/tex; mode=display">
w_i=w_i-\eta\sum\limits_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n</script><p>从式子中观察知：<strong>$\hat{y}^n$和$f_{w,b}(x^n))x_i^n$的差距越大，则更新的就越大</strong>。<br>[scode type=”yellow”]对于Logistic Regression，应使用<strong>cross entropy</strong>作为Loss，使用Square Error效果不好。[/scode]</p>
<hr>
<p>上面的Model叫做<strong>Discriminative Model</strong>，同<strong>Generative Model</strong>相比，虽然其使用的是同一个function set，但最终会得到不同的$w,b$。</p>
<p>Generative Model的优势：</p>
<ul>
<li>With the assumption of probability distribution,<strong>less training data</strong> is needed;</li>
<li>With the assumption of probability distribution,more robust to the noise;</li>
<li>Priors and class-dependent probability can be estimated from different source.（Discriminative Model首先假设一个概率分布，然后直接找到其中的parameters；而Generative Model将整个过程拆分为Priors和class-dependent probability，可以分别求解。例如在语音辨识中，Priors可以由文字data得到，class-dependent probability需要结合文字data和语音data得到）。</li>
</ul>
<h2 id="Multi-class-Classification"><a href="#Multi-class-Classification" class="headerlink" title="Multi-class Classification"></a>Multi-class Classification</h2><p>以三个类别$C_1,C_2,C_3$为例，其中$C_1$对应parameters为$w^1,b^1$，$C_2$对应parameters为$w^2,b^2$，$C_3$对应parameters为$w^3,b^3$。给定一个$x$，有：</p>
<script type="math/tex; mode=display">
z_1=w^1\cdot x+b^1</script><script type="math/tex; mode=display">
z_2=w^2\cdot x+b^2</script><script type="math/tex; mode=display">
z_3=w^3\cdot x+b^3</script><p>然后将output经过一个<strong>softmax</strong>：<br><img src="images/2024/07/600706388.png" alt="softmax"><br>将softmax的output与traning data的label做Cross Entropy，得到：<br><img src="images/2024/07/1038336687.png" alt="Cross Entropy"><br>其中，如果$x∈C_1$，y=$\begin{bmatrix}1 \ 0 \ 0\end{bmatrix}$；如果$x∈C_2$，y=$\begin{bmatrix}0 \ 1 \ 0\end{bmatrix}$；如果$x∈C_3$，y=$\begin{bmatrix}0 \ 0 \ 1\end{bmatrix}$。</p>
<h1 id="Limitation-of-Logistic-Regression"><a href="#Limitation-of-Logistic-Regression" class="headerlink" title="Limitation of Logistic Regression"></a>Limitation of Logistic Regression</h1><p>以二分类为例，其boundary是一条直线。考虑如下例子：<br><img src="images/2024/07/3596427211.png" alt=""><br>Logistic Regression无法合理的区分这两个Class。因为样本点不是<strong>线性可分的</strong>。<br>解决方法：Feature Transformation(Cascading Logistic Regression Model)<br>如下图所示：<br><img src="images/2024/07/1410495640.png" alt="Feature Transformation"><br>通过Feature Transformation，可以将原来线性不可分的样本点转化为<strong>线性可分的</strong>，然后就能使用Logistic Regression进行classification了。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/Week1-Extra-%E7%94%A8Logistic-Regression%E5%AE%9E%E7%8E%B0classification.html" data-id="cm4zf9dd40001y73kfl9304as" data-title="[Week1 Extra] 用Logistic Regression实现classification" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Week1-extra-Backpropogation" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/Week1-extra-Backpropogation.html" class="article-date">
  <time class="dt-published" datetime="2024-07-13T05:21:00.000Z" itemprop="datePublished">2024-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/Week1-extra-Backpropogation.html">[Week1 extra] Backpropogation</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Backpropagation是一种快速计算gradient的方法。</p>
<h2 id="前置知识——Chain-Rule"><a href="#前置知识——Chain-Rule" class="headerlink" title="前置知识——Chain Rule"></a>前置知识——Chain Rule</h2><ol>
<li>$y=g(x),z=h(y)$，则$\frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}$</li>
<li>$x=g(s),y=h(s),z=k(x,y)$，则$\frac{dz}{ds}=\frac{\partial z}{\partial x}\frac{dx}{ds}+\frac{\partial z}{\partial y}\frac{dy}{ds}$</li>
</ol>
<h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation"></a>Backpropagation</h2><p>首先假设有下图这样的Model：<br><img src="images/2024/07/4046648254.png" alt="Model"><br>那么对于损失函数$L$，有：</p>
<script type="math/tex; mode=display">
L(\theta)=\sum\limits_{n=1}^NC^n(\theta)</script><p>对于$L$的gradient，有：</p>
<script type="math/tex; mode=display">
\frac{\partial L(\theta)}{\partial w}=\sum\limits_{n=1}^N\frac{\partial C^n(\theta)}{\partial w}</script><p>以下图neuron为例：<br><img src="images/2024/07/1517802908.png" alt="neuron"><br>根据<strong>Chain Rule</strong>，$\frac{\partial C}{\partial w}=\frac{\partial z}{\partial w}\frac{\partial C}{\partial z}$，因此需要计算的就是两部分：$\frac{\partial z}{\partial w}$和$\frac{\partial C}{\partial z}$，有如下定义：</p>
<ol>
<li><strong>Forward Pass</strong>：Compute $\frac{\partial z}{\partial w}$for all parameters</li>
<li><strong>Back Pass</strong>：Compute $\frac{\partial C}{\partial z}$ for all activation functions inputs $z$</li>
</ol>
<h3 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h3><p>计算$\frac{\partial z}{\partial w}$非常容易，根据上图neuron所示，$\frac{\partial z}{\partial w_1}$即是$w_1$的系数即$x_1$，$\frac{\partial z}{\partial w_2}$即是$w_2$的系数即$x_2$。<br>对于更多的hidden Layer也是如此：<br><img src="images/2024/07/1956241098.png" alt="Forward Pass"><br>如图所示，由于$\frac{\partial z}{\partial w}$是从Input Layer开始逐层前向计算的，因此这个过程就叫做<strong>Forward Pass</strong>。</p>
<h3 id="Back-Pass"><a href="#Back-Pass" class="headerlink" title="Back Pass"></a>Back Pass</h3><p>考虑如下Model：<br><img src="images/2024/07/3080548224.png" alt="Model"><br>有$\frac{\partial C}{\partial z}=\frac{\partial a}{\partial z}\frac{\partial C}{\partial a}$，其中$\frac{\partial a}{\partial z}$容易求得，即$\sigma^{\prime}(z)$。<br>而$\frac{\partial C}{\partial a}=\frac{\partial z^{\prime}}{\partial a}\frac{\partial C}{\partial z^{\prime}}+\frac{\partial z^{\prime\prime}}{\partial a}\frac{\partial C}{\partial z^{\prime\prime}}$，其中$\frac{\partial z^{\prime}}{\partial a}$和$\frac{\partial z^{\prime\prime}}{\partial a}$容易求得，分别为$w_3、w_4$，我们假设$\frac{\partial C}{\partial z^{\prime}}$和$\frac{\partial C}{\partial z^{\prime\prime}}$已知，那么得到最终结果：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial z}=\sigma^{\prime}(z)[w_3\frac{\partial C}{\partial z^{\prime}}+w_4\frac{\partial C}{\partial z^{\prime\prime}}]</script><p>我们可以将其用一个neuron表示：<br><img src="images/2024/07/2510136609.png" alt=""><br>其中，由于$z$是一个在Forward Pass过程中已经被计算出的确定的值，因此$\sigma’(z)$是一个常数。因此，最终的问题归结为$\frac{\partial C}{\partial z^{\prime}}$和$\frac{\partial C}{\partial z^{\prime\prime}}$如何计算。这里分两种情况来进行讨论：</p>
<ol>
<li>如果$z’$和$z’’$所在的layer<strong>是Output Layer</strong>，那么有：<script type="math/tex; mode=display">
\frac{\partial C}{\partial z^{\prime}}=\frac{\partial y_1}{\partial z^{\prime}}\frac{\partial C}{\partial y_1}</script><script type="math/tex; mode=display">
\frac{\partial C}{\partial z^{\prime\prime}}=\frac{\partial y_2}{\partial z^{\prime\prime}}\frac{\partial C}{\partial y_2}</script>其中各个部分均容易求得；</li>
<li>如果$z’$和$z’’$所在的layer<strong>不是Output Layer</strong>，如下所示：<br><img src="images/2024/07/3090028825.png" alt=""><br>那么对于$\frac{\partial C}{\partial z^{\prime}}$，有：<script type="math/tex; mode=display">
\frac{\partial C}{\partial z^{\prime}}=\frac{\partial z_a}{\partial z^{\prime}}\frac{\partial C}{\partial z_a}+\frac{\partial z_b}{\partial z^{\prime}}\frac{\partial C}{\partial z_b}</script></li>
</ol>
<p>到这里我们就会发现，计算的方法会形成一个递归，直到我们计算到Output Layer。<br>举个例子：假设有如下Model<br><img src="images/2024/07/131783293.png" alt="Model"><br>要计算$\frac{\partial C}{\partial z_1}、\frac{\partial C}{\partial z_2}$，就要首先计算$\frac{\partial C}{\partial z_3}、\frac{\partial C}{\partial z_4}$；要得到$\frac{\partial C}{\partial z_3}、\frac{\partial C}{\partial z_4}$，就要首先计算$\frac{\partial C}{\partial z_5}、\frac{\partial C}{\partial z_6}$。因此，我们的计算顺序为：</p>
<script type="math/tex; mode=display">
\frac{\partial C}{\partial z_5}、\frac{\partial C}{\partial z_6}\rightarrow\frac{\partial C}{\partial z_3}、\frac{\partial C}{\partial z_4}\rightarrow\frac{\partial C}{\partial z_1}、\frac{\partial C}{\partial z_2}</script><p>我们用下图直观地表示这一过程：<br><img src="images/2024/07/4017796866.png" alt="Back Pass"><br>可以发现，这是一个<strong>从Output Layer开始反向逐层计算的过程</strong>，因此叫做<strong>Back Pass</strong>。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>用一张图来表示整个Backpropagation的过程<br><img src="images/2024/07/3173305372.png" alt="Backpropagation"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/Week1-extra-Backpropogation.html" data-id="cm4zf3c6900007y3k6he4fulk" data-title="[Week1 extra] Backpropogation" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-Week1-Introduction-of-Deep-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/Week1-Introduction-of-Deep-Learning.html" class="article-date">
  <time class="dt-published" datetime="2024-07-12T16:07:00.000Z" itemprop="datePublished">2024-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/Week1-Introduction-of-Deep-Learning.html">[Week1] Introduction of Deep Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><p>ML就是让机器<strong>找到一个function</strong>。<br>例如：向function输入一段语音，输出语音对应的文字；向function输入一张图片，输出图片是什么；向function输入一张棋盘，输出下一步最佳落子位置……</p>
<p>根据function的不同，ML主要分为两种：<strong>Regression</strong>和<strong>Classification</strong></p>
<ul>
<li>Regression：function的output是一个<strong>scalar</strong>；</li>
<li>Classification：function的output是一个给定的<strong>option(class)</strong>。</li>
</ul>
<p>但在简单的Regression和Classification之外，ML还有更为广泛的领域：<strong>Structured Learning</strong>，也即让机器学会创作，产生一个<strong>有结构的事物</strong>，例如一篇文章，一张图……</p>
<h2 id="如何实现ML"><a href="#如何实现ML" class="headerlink" title="如何实现ML"></a>如何实现ML</h2><p>要实现ML，通常需要以下步骤：</p>
<h3 id="一、构建function"><a href="#一、构建function" class="headerlink" title="一、构建function"></a>一、构建function</h3><p>我们往往需要根据对应领域的知识，构建出一个<strong>带有未知parameters的function</strong>。以$y=b+wx_1$为例，该functino就叫做一个<strong>Model</strong>，其中$x_1$作为已知量输入，叫做<strong>feature</strong>，w（weight)和b（bias）就是未知的parameters。</p>
<h3 id="二、定义Loss"><a href="#二、定义Loss" class="headerlink" title="二、定义Loss"></a>二、定义Loss</h3><p>Loss是一个<strong>关于parameters的function</strong>。Loss通过比较Model的output与真实值之间的差异，来表示某一组parameter的好坏。常见的Loss定义为：$L=\frac{1}{N}\sum\limits_ne_n$，其中e可以是$|y-\hat{y}|$（此时L叫做mean absolute error，MAE）或$(y-\hat{y})^2$（此时L叫做mean square error，MSE）等，需要根据具体情况进行选择。例如如果$y$和$\hat{y}$是一个概率分布的时候，往往选择Cross-entorpy。</p>
<h3 id="三、Optimization"><a href="#三、Optimization" class="headerlink" title="三、Optimization"></a>三、Optimization</h3><ul>
<li>目标：找到$w^<em>,b^</em>=\mathop{\arg\min}\limits_{w,b}L$。</li>
<li>方法：<strong>Gradient Descent</strong>（暂不考虑local optimum）。</li>
<li>步骤（以$w$为例）：<ol>
<li>随机选取一组随机初始值$w^0,b^0$；</li>
<li>计算$\frac{\partial L}{\partial w}|_{w=w^0}$；</li>
<li>为了让Loss的值变小，则更新$w$为：$w^1=w^0-\eta \frac{\partial L}{\partial w}|_{w=w^0}$，其中$\eta$叫做<strong>learning rate</strong>，是一个<strong>Hyperparameter</strong></li>
<li>迭代进行2、3两步，直到终止（不想继续进行了，或者计算出$\frac{\partial L}{\partial w}|_{w=w^0}=0$）</li>
</ol>
</li>
</ul>
<p><strong>Hyperparameter</strong>：人为设置的参数，不是机器需要学习的parameter</p>
<h2 id="更加复杂的情况"><a href="#更加复杂的情况" class="headerlink" title="更加复杂的情况"></a>更加复杂的情况</h2><h3 id="Sophisticated-Model"><a href="#Sophisticated-Model" class="headerlink" title="Sophisticated Model"></a>Sophisticated Model</h3><p>实际中，function往往并非线性，而是十分复杂的，这时，应用简单的线性Model并不能解决问题，我们需要更加复杂的Model。在ML中，解决方法一般如下：<br><img src="images/2024/07/3743548032.png" alt="sophisticated Model"><br>可以看到，要获得一个如图所示的红色曲线，需要三个类似形状的<strong>蓝色曲线</strong>（hard-sigmoid function）和一个<strong>常数项</strong>相加得到。以此为例，不难得出：任何一个复杂的函数，都可以由多个如图所示的蓝色曲线和一个常数项相加<strong>近似得到</strong>。</p>
<h3 id="sigmoid-function"><a href="#sigmoid-function" class="headerlink" title="sigmoid function"></a>sigmoid function</h3><p>可是，由于蓝色曲线并非光滑的，所以对其进行Optimization非常困难，因此，这里采用<strong>sigmoid函数</strong>去逼近蓝色曲线。<strong>sigmoid函数</strong>如下图所示：<br><img src="images/2024/07/548121614.png" alt="Structure of CIFAR10-quick model"><br>其函数表达式为：$y=c\frac{1}{1+e^{-b+wx_1}}$，简单记作$y=c sigmoid(b+wx_1)$。<br>那么，上面的sophisticated Model就可以转化为：$y=b+\sum\limits_ic_isigmoid(b_i+w_ix_1)$<br>对于一个sigmoid function来说，改变其参数可以得到不同形状的sigmoid function：<br><img src="images/2024/07/1242558349.png" alt="different sigmoid function"><br>有了这些<strong>不同的sigmoid function</strong>，将其叠加后就可以<strong>逼近不同的non-linear Model</strong>。</p>
<h3 id="multi-feature-Model"><a href="#multi-feature-Model" class="headerlink" title="multi-feature Model"></a>multi-feature Model</h3><p>到目前为止，这样的Model还有一个不足：无法应对多feature的情况。对此，我们将其进行改进，给Model<strong>添加多个feature</strong>后，得到如下Model：$y=b+\sum\limits_ic_isigmoid(b_i+\sum\limits_jw_{ij}x_{j})$，其中$j$为feature的编号。<br>假设某Model需要三个sigmoid function，那么我们可以通过下图<strong>直观的观察到其中的关系</strong>：<br><img src="images/2024/07/4222411239.png" alt=""><br>接下来，使用<strong>矩阵</strong>来<strong>简洁的表达其中的运算</strong>：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{bmatrix}
 r_1 \\ r_2 \\ r_3
\end{bmatrix}
=
\begin{bmatrix}
 b_1 \\ b_2 \\ b_3
\end{bmatrix}
+
\begin{bmatrix}
 w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \\ w_{31} & w_{32} & w_{33}
\end{bmatrix}
\begin{bmatrix}
 x_1 \\ x_2 \\ x_3
\end{bmatrix}
\end{equation}</script><p>用符号表示为：$r=b+Wx$<br>整个Model由下图给出：<br><img src="images/2024/07/257902148.png" alt=""></p>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>对于$y=b+c^Ta$中所有未知的parameter（包括$b、c、W、b$），将其中元素拼成一个列向量</p>
<script type="math/tex; mode=display">
\begin{equation}
\theta=
\begin{bmatrix}
 \theta_1 \\ \theta_2 \\ \vdots
\end{bmatrix}
\end{equation}</script><p>要进行Optimization，即找到$\theta^*=\mathop{\arg\min}\limits_\theta L$。步骤和最基本的Model类似，先随机选取一个初始值$\theta^0$，然后计算</p>
<script type="math/tex; mode=display">
\begin{equation}
g=
\begin{bmatrix}
\frac{\partial L}{\partial \theta_1}|_{\theta=\theta^0} \\
\frac{\partial L}{\partial \theta_2}|_{\theta=\theta^0} \\
\vdots
\end{bmatrix}
\end{equation}</script><p>记作$g=\nabla L(\theta^0)$。最后更新参数即可：$\theta^1=\theta^0-\eta g$</p>
<h3 id="train-as-batch"><a href="#train-as-batch" class="headerlink" title="train as batch"></a>train as batch</h3><p>实际Optimization的过程中，往往将train data<strong>分为多个batch</strong>，记每个batch对parameter的一次Optimization为<strong>一次update</strong>，所有batch串行进行一次update叫做<strong>一个epoch</strong>。如下图所示：<br><img src="images/2024/07/425677454.png" alt="epoch-update"><br>那么<strong>batch size和epoch</strong>也就成为了<strong>Hyperparameter</strong>。</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>这样的Model有一个变体，我们使用<strong>ReLU</strong>（Rectified Linear Unit）来代替sigmoid function，ReLU如下图所示：<br><img src="images/2024/07/2218813359.png" alt="ReLU"><br>只要将两个ReLU叠加，就可以得到hard-sigmoid funccion：<br><img src="images/2024/07/2679937844.png" alt=""><br>那么对于上述Model，就可以表示为：$y=b+\sum\limits_{2i}c_imax(b_i+\sum\limits_jw_{ij}x_{j})$</p>
<p>对于sigmoid function和ReLU这样的函数，我们统称为<strong>Activate function</strong>。</p>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><p>我们可以将上述Model进行套娃，如下图所示：<br><img src="images/2024/07/548161856.png" alt=""><br>每个Activate function叫做一个<strong>Neuron</strong>，所有Neuron的总和叫做<strong>Neural Network</strong>；每一列Neuron叫做一个<strong>hidden layer</strong>，<strong>包含多个hidden layer的Model</strong>即是<strong>Deep Learning</strong>。<br><img src="images/2024/07/1753846494.png" alt="Deep Learning"><br>其中，$x_1,x_2,…$叫做<strong>Input Layer</strong>，Output前的最后一层叫做<strong>Output Layer</strong>。</p>
<p>大多数情况下，Deep Learning的表现要比单层的Model更好。但hidden layer过多会导致<strong>over fitness</strong>。</p>
<p>network的structure是非常重要的，需要自己设计好stucture，才能获得好的效果。设计的依据：试错+直觉。</p>
<hr>
<p>那么，相对于Machine Learning，Deep Learning究竟做了什么呢？<br>其实就是<strong>把一个问题转化为了另一个问题</strong>。对于ML，往往需要<strong>找一组好的feature</strong>，这在有些场景下是非常困难的。但在DL中，只需要将一组不是那么好的feature放到一个<strong>设计好的structure</strong>中，就能得到较好的效果。因此问题从<strong>find feature</strong>转化为了<strong>design a network</strong>，而DL和ML的优劣，就归结于<strong>这两个问题哪个较为容易解决</strong>（例如在NLP领域，抽一组好的feature往往是更容易的；而对于影像辨识和语音辨识，抽一组好的feature是比较困难的）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/Week1-Introduction-of-Deep-Learning.html" data-id="cm4zd1vkq00006r3kf3ul4lw2" data-title="[Week1] Introduction of Deep Learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/3/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Env-Setting/">Env Setting</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NTU-ML2022/">NTU-ML2022</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/articles/linux%E9%85%8D%E7%BD%AEgit%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0github.html">linux配置git并部署到github</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E4%B8%8B%E7%94%A8qemu%E6%A8%A1%E6%8B%9Ffreedos%E7%BC%96%E5%86%9916%E4%BD%8D%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80.html">ubuntu下用qemu模拟freedos编写16位汇编语言</a>
          </li>
        
          <li>
            <a href="/articles/week12-extra-Q-learning.html">[week12 extra] Q-learning</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%A4%9A%E7%89%88%E6%9C%ACcuda%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%88%87%E6%8D%A2.html">ubuntu环境下多版本cuda的安装与切换</a>
          </li>
        
          <li>
            <a href="/articles/week12-Reinforcement-Learning.html">[week12]Reinforcement Learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 zz111y<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
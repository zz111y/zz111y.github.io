<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>[week5]Transformer and Variable Attention | zz111y&#39;s stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Sequence-to-sequence(Seq2seq)seq2seqï¼šinputæ˜¯ä¸€ä¸ªseqï¼Œoutputä¹Ÿæ˜¯ä¸€ä¸ªseqï¼Œä¸”outputçš„seqé•¿åº¦ç”±modelå†³å®šã€‚seq2seq Modelåœ¨å¾ˆå¤šé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼Œä¾‹å¦‚NLPçš„å¤§éƒ¨åˆ†é¢†åŸŸï¼ˆè¯­éŸ³è¾¨è¯†ã€ç¿»è¯‘ã€è¯­éŸ³ç¿»è¯‘ç­‰ï¼‰ã€multi-label classificationã€Object Detectionç­‰ã€‚è™½ç„¶seq2seq Modelåœ¨è¿™äº›é¢†åŸŸ">
<meta property="og:type" content="article">
<meta property="og:title" content="[week5]Transformer and Variable Attention">
<meta property="og:url" content="https://zz111y.github.io/articles/week5-Transformer-and-Variable-Attention.html">
<meta property="og:site_name" content="zz111y&#39;s stack">
<meta property="og:description" content="Sequence-to-sequence(Seq2seq)seq2seqï¼šinputæ˜¯ä¸€ä¸ªseqï¼Œoutputä¹Ÿæ˜¯ä¸€ä¸ªseqï¼Œä¸”outputçš„seqé•¿åº¦ç”±modelå†³å®šã€‚seq2seq Modelåœ¨å¾ˆå¤šé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼Œä¾‹å¦‚NLPçš„å¤§éƒ¨åˆ†é¢†åŸŸï¼ˆè¯­éŸ³è¾¨è¯†ã€ç¿»è¯‘ã€è¯­éŸ³ç¿»è¯‘ç­‰ï¼‰ã€multi-label classificationã€Object Detectionç­‰ã€‚è™½ç„¶seq2seq Modelåœ¨è¿™äº›é¢†åŸŸ">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4252600933.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3500171285.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1168952771.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/641826901.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1218346287.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1102089619.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1102159223.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3406154327.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3141551010.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1875163849.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3885036104.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2401978987.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/195258900.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4096320065.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3137948287.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1695904727.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2298068304.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2379751845.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2197013624.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3616426535.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3257410960.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2526671714.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2821130049.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4001522953.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4099759351.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1109922326.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/644508994.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3138545184.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/512070726.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1220587946.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1526852039.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3533257680.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/962417182.png">
<meta property="article:published_time" content="2024-08-11T02:56:00.000Z">
<meta property="article:modified_time" content="2024-12-29T08:59:27.069Z">
<meta property="article:author" content="zz111y">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zz111y.github.io/articles/images/2024/08/4252600933.png">
  
    <link rel="alternate" href="/atom.xml" title="zz111y's stack" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zz111y&#39;s stack</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ã„…ã„†ã„‡ã„ˆã„‰ã„Šã„‹ã„Œå·œã„ã„</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zz111y.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-week5-Transformer-and-Variable-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/week5-Transformer-and-Variable-Attention.html" class="article-date">
  <time class="dt-published" datetime="2024-08-11T02:56:00.000Z" itemprop="datePublished">2024-08-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      [week5]Transformer and Variable Attention
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Sequence-to-sequence-Seq2seq"><a href="#Sequence-to-sequence-Seq2seq" class="headerlink" title="Sequence-to-sequence(Seq2seq)"></a>Sequence-to-sequence(Seq2seq)</h1><p>seq2seqï¼šinputæ˜¯ä¸€ä¸ªseqï¼Œoutputä¹Ÿæ˜¯ä¸€ä¸ªseqï¼Œä¸”outputçš„seqé•¿åº¦ç”±modelå†³å®šã€‚<br>seq2seq Modelåœ¨å¾ˆå¤šé¢†åŸŸéƒ½æœ‰åº”ç”¨ï¼Œä¾‹å¦‚NLPçš„å¤§éƒ¨åˆ†é¢†åŸŸï¼ˆè¯­éŸ³è¾¨è¯†ã€ç¿»è¯‘ã€è¯­éŸ³ç¿»è¯‘ç­‰ï¼‰ã€multi-label classificationã€Object Detectionç­‰ã€‚è™½ç„¶seq2seq Modelåœ¨è¿™äº›é¢†åŸŸéƒ½èƒ½åº”ç”¨ï¼Œä½†<strong>ä¸ºæ¯ä¸ªé—®é¢˜å®¢åˆ¶åŒ–å®šåˆ¶Model</strong>æ‰èƒ½è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚</p>
<p>Seq2seqçš„ç»“æ„ä¸º<strong>ä¸€ä¸ªEncoderå’Œä¸€ä¸ªDecoder</strong>ï¼Œå…¶ä¸­Encoderåƒinputï¼Œå°†å¤„ç†å¥½çš„ç»“æœé€è¿›Decoderï¼ŒDecoderå†³å®šè¾“å‡ºæ˜¯ä»€ä¹ˆseqã€‚seq2seqçš„èµ·æºï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">ã€ŠSequence to Sequence Learning with Neural Networksã€‹</a><br>å½“ä¸‹ï¼Œæœ€å¸¸ç”¨çš„seq2seqæ˜¯transformerï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">attention is all you need</a><br>transformerçš„æ¶æ„å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š</p>
<p><div align="center">
<img src="images/2024/08/4252600933.png" width="50%">
</div></p>
<h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoderåšçš„äº‹æƒ…æ˜¯<strong>åƒä¸€æ’å‘é‡ï¼Œè¾“å‡ºåŒæ ·æ•°é‡çš„ä¸€æ’å‘é‡</strong>ã€‚è¿™æ ·çš„äº‹æƒ…å¾ˆå¤šæ¶æ„éƒ½èƒ½åšåˆ°ï¼Œè€Œ<strong>transformerä¸­åº”ç”¨çš„æ˜¯self-attention</strong>ã€‚<br>å…·ä½“è€Œè¨€ï¼ŒEncoderçš„inputç»è¿‡äº†ä¸€ä¸ªä¸€ä¸ªçš„Blockï¼ˆåŒ…å«å¤šä¸ªhidden layerï¼‰ï¼Œæœ€åå¾—åˆ°outputï¼Œè€Œ<strong>transformerçš„blockä¸ºself-attention+fully-connected</strong>ã€‚<br><img src="images/2024/08/3500171285.png" alt="transformer Encoder block"><br>åœ¨æœ€æ—©æå‡ºçš„transformeræ¶æ„ä¸­ï¼Œè¿˜è¦åŠ ä¸Šä¸€ä¸ªresidual connectionä»¥åŠå„å±‚è¾“å‡ºéƒ½è¦ç»è¿‡normalizationï¼š<br><img src="images/2024/08/1168952771.png" alt="a block in transformer Encoder"><br>è¿™é‡Œçš„normalizationä½¿ç”¨çš„æ˜¯<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p>
<p>æ€»ä¸Šæ‰€è¿°ï¼Œå†çœ‹ä¸€ä¸‹transformerçš„æ¶æ„Encoderéƒ¨åˆ†ï¼š<br><img src="images/2024/08/641826901.png" alt="transformer Encoder"><br>è¿™æ˜¯æœ€åŸå§‹çš„Encoderè®¾è®¡ï¼Œç›®å‰å·²ç»æœ‰äº†å¾ˆå¤šå˜ç§ï¼Œä¾‹å¦‚ï¼š<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">ã€ŠOn Layer Normalization in the Transformer Architectureã€‹</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">ã€ŠPowerNorm: Rethinking Batch Normalization in Transformersã€‹</a></p>
<h2 id="Decoderâ€”â€”Autoregressive-AT"><a href="#Decoderâ€”â€”Autoregressive-AT" class="headerlink" title="Decoderâ€”â€”Autoregressive(AT)"></a>Decoderâ€”â€”Autoregressive(AT)</h2><p>Decoderä¼šè¯»å…¥Encoderçš„outputï¼Œä»ä¸€ä¸ªspecial tokenï¼ˆBEGINï¼‰å¼€å§‹ï¼Œè¾“å‡ºä¸€ä¸ªvectorï¼Œè¿™ä¸ªvectorçš„é•¿åº¦å’Œvocabulary sizeæ˜¯ä¸€æ ·çš„ï¼ˆä»¥ä¸­æ–‡è¯­éŸ³è¾¨è¯†ä¸ºä¾‹ï¼Œvocabularyå°±æ˜¯æ‰€æœ‰çš„å¸¸è§å­—ï¼‰ï¼Œvectorä¸­çš„å€¼æ˜¯ä¸€ä¸ªdistributionï¼Œå…¶ä¸­æ¦‚ç‡æœ€å¤§çš„å°±ä½œä¸ºå½“å‰outputï¼š<br><img src="images/2024/08/1218346287.png" alt="Dncoder"><br>éšåï¼Œå°†æ­¤å‰æ‰€æœ‰çš„outputä½œä¸ºé¢å¤–çš„inputï¼ŒåŠ ä¸ŠBEGINå’ŒEncoderçš„outputå…±åŒä½œä¸ºinputï¼Œè¿›è¡Œä¸‹ä¸€æ¬¡outputã€‚<br><img src="images/2024/08/1102089619.png" alt="Dncoder"><br>Decoderçš„ç»“æ„å¦‚ä¸‹ï¼š<br><img src="images/2024/08/1102159223.png" alt="Decoder structure"><br>å¯¹æ¯”ä¸€ä¸‹Encoderå’ŒDecoderï¼š<br><img src="images/2024/08/3406154327.png" alt="Encoder v.s. Decoder"><br>å¦‚æœå¿½ç•¥è¢«é®æŒ¡éƒ¨åˆ†ï¼Œé‚£ä¹ˆEncoderå’ŒDecoderæ˜¯å®Œå…¨ä¸€æ ·çš„ã€‚<br>[scode type=â€yellowâ€]<strong>Masked Attention</strong><br>ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯self-attentionäº§ç”Ÿoutputçš„æ—¶å€™<strong>åªèƒ½è·å–è‡ªèº«åŠä¹‹å‰çš„ä¿¡æ¯</strong>ã€‚<br><img src="images/2024/08/3141551010.png" alt="Masked Attention"><br>MaskedåŸå› æ˜¯ï¼šDecoderçš„æ—¶å€™ï¼Œoutputæ˜¯ä¸€ä¸ªä¸€ä¸ªäº§ç”Ÿçš„ï¼Œæ‰€ä»¥ä»–åªèƒ½è€ƒè™‘ä¹‹å‰äº§ç”Ÿçš„vector[/scode]<br>Decoderä¼šè®¾ç½®ä¸€ä¸ª<strong>end token</strong>ï¼Œå½“output end tokençš„æ—¶å€™ï¼ŒDecoderå°±ä¼šåœæ­¢ã€‚</p>
<hr>
<p><strong>AT v.s. NATï¼ˆnon-autoregressiveï¼‰</strong><br>ç›¸è¾ƒäºATï¼ŒNATæœ€æ˜¾è‘—çš„åŒºåˆ«æ˜¯åƒä¸€æ’BEGIN tokenï¼ŒåŒæ—¶äº§ç”Ÿoutputã€‚å…¶æ§åˆ¶åºåˆ—é•¿åº¦çš„æ–¹æ³•å¯èƒ½ä¸ºï¼š</p>
<ul>
<li>ç”¨å¦ä¸€ä¸ªmodel predicté•¿åº¦</li>
<li>Aç»™å®šä¸€ä¸ªå¾ˆå¤§çš„é•¿åº¦ï¼Œæˆªå–end tokenä¹‹å‰çš„output</li>
</ul>
<p><img src="images/2024/08/1875163849.png" alt="AT v.s. NAT"><br>NATçš„å¥½å¤„ï¼šå¹³è¡Œè®¡ç®—ï¼Œé€Ÿç‡æ›´å¿«ï¼›å¯æ§åˆ¶çš„outputé•¿åº¦ã€‚ä½†NATçš„performenceä¸å¦‚ATï¼ˆç”±äºMulti-modalityï¼‰ï¼Œå› æ­¤å½“ä¸‹å¯¹NATçš„ä¼˜åŒ–ä»æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚</p>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>Encoderä¸Decoderçš„è¿æ¥éƒ¨åˆ†å«åš<strong>Cross-attention</strong>ã€‚ç”±ä¸Šæ–‡transformeræ¶æ„ä¸­å¯ä»¥å‘ç°ï¼ŒEncoderåå‡ºä¸¤ä¸ªä¸œè¥¿è¢«Decoderæ‰€æ¥æ”¶ï¼Œè¿™ä¸€éƒ¨åˆ†æ‰€è¿›è¡Œçš„æ“ä½œå°±æ˜¯cross-attentionã€‚<br><img src="images/2024/08/3885036104.png" alt="cross-attention"><br>å…¶å…·ä½“çš„è¿ä½œæœºåˆ¶ä¸ºï¼ˆä»¥BEGINä¸ºä¾‹ï¼‰ï¼š<br>BEGINç»è¿‡ä¸€ä¸ªMasked self-attentionå¾—åˆ°ä¸€ä¸ªvectorï¼Œç„¶åä¹˜ä¸Šä¸€ä¸ªçŸ©é˜µå¾—åˆ°$q$ï¼Œç”¨è¿™ä¸ª$q$å»å¯¹Encoderçš„outputæ±‚attention scoreï¼Œåœ¨åšä¸€ä¸ªweighted sumï¼Œå¾—åˆ°çš„è¾“å‡ºé€è¿›Decoderçš„fully-connected layerã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š<br><img src="images/2024/08/2401978987.png" alt="cross-attention"><br>å¯¹äºåç»­äº§ç”Ÿçš„outputï¼Œä¹Ÿæ˜¯åŒç†ï¼š<br><img src="images/2024/08/195258900.png" alt="example"><br>To learn moreï¼Œå¦ä¸€ç§cross-attentionæ–¹å¼ï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08081">ã€ŠRethinking and Improving Natural Language Generation with Layer-Wise Multi-View Decodingã€‹</a></p>
<h2 id="how-to-traing"><a href="#how-to-traing" class="headerlink" title="how to traing"></a>how to traing</h2><p>transformeræ‰€åšçš„äº‹æƒ…å¯ä»¥çœ‹ä½œæ˜¯ä¸€æ¬¡ä¸€æ¬¡çš„classificationï¼Œå› æ­¤å¯ä»¥è®¡ç®—æ¯ä¸ªpredictå’Œtruthçš„cross entropyï¼Œç„¶åminimize cross entropyå³å¯ã€‚ä½†å®é™…<strong>trainçš„æ—¶å€™ï¼ŒDecoderçš„inputæ˜¯æ­£ç¡®çš„labelï¼Œè€Œépredictçš„ç»“æœ</strong>ã€‚è¿™ç§åšæ³•å«åš<strong>teacher forcing</strong>ï¼Œå¯ä»¥åŠ é€Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼Œæé«˜ç”Ÿæˆåºåˆ—çš„è´¨é‡ã€‚<br><img src="images/2024/08/4096320065.png" alt="teacher forcing"><br>å½“ç„¶ï¼Œè¿™æ ·åšä¼šæœ‰ä¸€ä¸ªé—®é¢˜ï¼šåœ¨testingçš„æ—¶å€™æ˜¯æ²¡æœ‰æ­£ç¡®labelçš„ï¼Œå› æ­¤è¿™å°±ä¼šé€ æˆæ¨¡å‹çš„mismatchï¼Œè¿™ç§æƒ…å†µå«åšexposure biasã€‚<br><strong>ä¸€ä¸ªå¯èƒ½çš„è§£å†³æ–¹æ³•</strong>ï¼šScheduled Samplingã€‚ç®€è€Œè¨€ä¹‹ï¼Œå°±æ˜¯ç»™Decoderä¸€äº›é”™è¯¯çš„ä¿¡æ¯ã€‚<br>åŸå§‹çš„Scheduled Samplingï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03099">ã€ŠScheduled Sampling for Sequence Prediction with Recurrent Neural Networksã€‹</a><br>ä¸€äº›æ”¹è¿›ï¼š<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.07651">ã€ŠScheduled Sampling for Transformersã€‹</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.04331">ã€ŠParallel Scheduled Samplingã€‹</a></p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><h3 id="Copy-Mechanism"><a href="#Copy-Mechanism" class="headerlink" title="Copy Mechanism"></a>Copy Mechanism</h3><p>æœ‰äº›æ—¶å€™ï¼Œinput seqä¸­å¯èƒ½ä¼šå‡ºç°ä¸€äº›å¥‡æ€ªçš„ä¸œè¥¿ï¼Œä¾‹å¦‚åœ¨ç¿»è¯‘æˆ–åšchat-botçš„æ—¶å€™ï¼Œäººåã€åœ°åç­‰ã€‚è¿™ç§æƒ…å†µä¸‹æœºå™¨å¾ˆéš¾å­¦åˆ°è¿™äº›å¤æ€ªçš„ä¸œè¥¿ï¼Œå› æ­¤<strong>ç›´æ¥å°†å…¶å¤åˆ¶ä¸‹æ¥åˆ°outputä¸­</strong>ä¹Ÿæ˜¯ä¸€ç§åšæ³•ã€‚<br>æœ€æ—©å®ç°è¿™ç§æ–¹æ³•çš„æ˜¯<strong>Pointer network</strong>ï¼Œä¸‹é¢è¿™ç¯‡æ–‡ç« ä½¿ç”¨äº†pointer networkï¼š<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04368">ã€ŠGet To The Point: Summarization with Pointer-Generator Networksã€‹</a><br>å…¶å˜å½¢ï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">ã€ŠIncorporating Copying Mechanism in Sequence-to-Sequence Learningã€‹</a></p>
<h3 id="Guided-Attention"><a href="#Guided-Attention" class="headerlink" title="Guided Attention"></a>Guided Attention</h3><p>åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæœºå™¨å¯èƒ½ä¼šçŠ¯ä¸€äº›ä¸¥é‡çš„é”™è¯¯ï¼Œæ¯”å¦‚outputç¼ºå¤±ã€é—æ¼æŸä¸ªinputçš„ä¿¡æ¯ã€‚å¯ä»¥ä½¿ç”¨<strong>Guided Attention</strong>ï¼Œç›¸å½“äºè®¤ä¸ºå¼•å¯¼æœºå™¨çš„è¡Œä¸ºã€‚ä¾‹å¦‚åœ¨åšè¯­éŸ³åˆæˆçš„æ—¶å€™ï¼Œattentionæ˜¯ç”±å·¦å‘å³çš„ï¼Œè¿™æ—¶å°±å¯ä»¥è®¤ä¸ºé™åˆ¶attentionçš„è¿‡ç¨‹ã€‚<br>Guided Attentionæ–¹æ³•å…³é”®è¯ï¼šMonotonic Attentionã€Location-aware Attentionã€‚</p>
<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><blockquote>
<p>GPT-4oï¼šBeam Searchæ˜¯ä¸€ç§å¸¸ç”¨çš„å¯å‘å¼æœç´¢ç®—æ³•ï¼Œä¸»è¦ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­çš„åºåˆ—ç”Ÿæˆä»»åŠ¡ï¼Œå¦‚æœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦å’Œå¯¹è¯ç”Ÿæˆç­‰ã€‚ç›¸æ¯”äºè´ªå¿ƒæœç´¢ï¼ˆGreedy Searchï¼‰ï¼ŒBeam Searchåœ¨ç”Ÿæˆåºåˆ—æ—¶å¯ä»¥åŒæ—¶ä¿ç•™å¤šä¸ªå€™é€‰è·¯å¾„ï¼Œä»è€Œæé«˜ç”Ÿæˆç»“æœçš„è´¨é‡ã€‚<br><strong>å·¥ä½œåŸç†</strong><br>Beam Searchçš„ä¸»è¦æ€æƒ³æ˜¯ä¿ç•™å¤šä¸ªå€™é€‰è·¯å¾„ï¼ˆç§°ä¸ºâ€œbeamâ€ï¼‰è€Œä¸æ˜¯åªä¿ç•™ä¸€ä¸ªæœ€ä¼˜è·¯å¾„ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œç®—æ³•ä¼šæ‰©å±•æ‰€æœ‰å½“å‰çš„å€™é€‰è·¯å¾„ï¼Œå¹¶åªä¿ç•™å¾—åˆ†æœ€é«˜çš„ğ‘˜ä¸ªè·¯å¾„ã€‚è¿™é‡Œçš„ğ‘˜ç§°ä¸ºbeam sizeã€‚</p>
</blockquote>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>åœ¨NLPæ–¹é¢ï¼ŒBLEU scoreå¾€å¾€æ˜¯è¯„ä»·ä¸€ä¸ªmodelå¥½åçš„æ ‡å‡†ã€‚</p>
<blockquote>
<p>GPT-4oï¼šBLEUï¼ˆBilingual Evaluation Understudyï¼‰è¯„åˆ†æ˜¯ä¸€ç§ç”¨äºè¯„ä¼°æœºå™¨ç¿»è¯‘å’Œå…¶ä»–è‡ªç„¶è¯­è¨€ç”Ÿæˆæ¨¡å‹çš„è´¨é‡çš„æŒ‡æ ‡ã€‚BLEUè¯„åˆ†é€šè¿‡æ¯”è¾ƒç”Ÿæˆçš„æ–‡æœ¬ä¸ä¸€ä¸ªæˆ–å¤šä¸ªå‚è€ƒæ–‡æœ¬æ¥è¡¡é‡ç¿»è¯‘çš„å‡†ç¡®æ€§å’Œæµç•…æ€§ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯è®¡ç®—ç”Ÿæˆæ–‡æœ¬å’Œå‚è€ƒæ–‡æœ¬ä¹‹é—´çš„ n-gram çš„é‡å ç¨‹åº¦ã€‚</p>
</blockquote>
<p>æˆ‘ä»¬trainçš„æ—¶å€™ï¼Œä½¿ç”¨çš„æ˜¯cross entropyä½œä¸ºlossï¼Œè€Œå®é™…è¯„æµ‹æ ‡å‡†å¯èƒ½æ˜¯BLEU scoreï¼Œå› ä¸ºcross entropyæ˜¯å¯å¾®åˆ†çš„ï¼Œè€ŒBLEU scoreè¾ƒä¸ºå¤æ‚ä¸”ä¸å¯å¾®åˆ†ï¼Œå› æ­¤å¦‚æœä½¿ç”¨BLEU scoreä½œä¸ºlossï¼Œoptimizationå°±ä¼šéš¾ä»¥è¿›è¡Œã€‚<br>é‚£è¯¥æ€ä¹ˆåŠå‘¢ï¼ŸWhen you donâ€™t know how to optimize, just use reinforcement learning!</p>
<h1 id="å„ç§å„æ ·çš„self-attention"><a href="#å„ç§å„æ ·çš„self-attention" class="headerlink" title="å„ç§å„æ ·çš„self-attention"></a>å„ç§å„æ ·çš„self-attention</h1><p>self-attentionçš„å„ç§å˜ä½“å¾€å¾€å–åä¸º<strong>xxxformer</strong>ï¼Œå…ˆæ”¾ä¸€å¼ å„ç§self-attentionçš„å¯¹æ¯”ï¼š<br><img src="images/2024/08/3137948287.png" alt="2024-08-08T08:02:30.png"><br>åœ¨å„ä¸ªModelä¸­ï¼Œself-attentionåªæ˜¯å…¶ä¸­ä¸€ä¸ªéƒ¨åˆ†ã€‚æˆ‘ä»¬çŸ¥é“ï¼Œself-attentionçš„è®¡ç®—å¤æ‚åº¦æ˜¯å’Œseqçš„é•¿åº¦æˆå¹³æ–¹çš„å…³ç³»çš„ï¼Œå› æ­¤å½“seqå¾ˆé•¿æ—¶ï¼Œ<strong>self-attentionå æ®äº†è®¡ç®—çš„å¤§éƒ¨åˆ†æ—¶é—´</strong>ã€‚è¿™äº›ä¼˜åŒ–çš„ç®—æ³•ï¼Œå¤§éƒ¨åˆ†éƒ½æ˜¯<strong>è®¾æ³•å‡å°‘self-attentionçš„è®¡ç®—</strong>ã€‚<br><img src="images/2024/08/1695904727.png" alt=""></p>
<h2 id="Local-Attention-Truncated-Attention"><a href="#Local-Attention-Truncated-Attention" class="headerlink" title="Local Attention/Truncated Attention"></a>Local Attention/Truncated Attention</h2><p>æœ‰äº›æƒ…å†µä¸‹ï¼Œå¯èƒ½<strong>ä¸éœ€è¦äº†è§£æ•´ä¸ªseqçš„ä¿¡æ¯ï¼Œåªéœ€è¦äº†è§£é™„è¿‘vecotrçš„ä¿¡æ¯</strong>ã€‚å…·ä½“åšæ³•æ˜¯ï¼Œ$i,j$æ¥è¿‘çš„æ—¶å€™æ‰å–è®¡ç®—attention scoreï¼Œå…¶ä»–ä½ç½®ç›´æ¥è®¾0ã€‚<br><img src="images/2024/08/2298068304.png" alt="Local Attention/Truncated Attention"><br>ä½†æ˜¯è¿™æ ·çš„è¯ï¼Œå°±å’ŒCNNæ²¡ä»€ä¹ˆä¸¤æ ·ã€‚å› æ­¤è¿™ç§æ–¹æ³•ï¼Œè™½ç„¶å¯ä»¥åŠ é€Ÿè¿ç®—ï¼Œä½†ä¸ä¸€å®šèƒ½æœ‰å¥½çš„æ•ˆæœã€‚</p>
<h2 id="Stride-Attention"><a href="#Stride-Attention" class="headerlink" title="Stride Attention"></a>Stride Attention</h2><p>è€ƒè™‘åˆ°local attentionä¸èƒ½è€ƒè™‘åˆ°è·ç¦»è¿œçš„ä½ç½®ï¼Œä½†è¿˜è¦ç®€åŒ–è®¡ç®—ï¼Œå¯ä»¥ä½¿ç”¨Stride Attentionã€‚å…¶æœºåˆ¶ä¸º<strong>æ¯éš”ä¸€å®šçš„Strideè®¡ç®—attention score</strong>ï¼Œå…¶ä»–è®¾0ã€‚<br><img src="images/2024/08/2379751845.png" alt="Stride Attention"></p>
<h2 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h2><p>ä»¥ä¸Šä¸¤ç§åšæ³•éƒ½æ˜¯ä»¥æŸä¸ªä½ç½®ä¸ºä¸­å¿ƒå»è®¡ç®—ï¼Œå¦‚æœ<strong>æƒ³çŸ¥é“æ•´ä¸ªseqï¼Œå¯ä»¥ä½¿ç”¨Global Attention</strong>ã€‚ç®€å•æ¥è¯´ï¼Œå°±æ˜¯åŠ ä¸Šä¸€ä¸ª<strong>ç‰¹æ®Šçš„tokenï¼Œè¿™ä¸ªtokenè®°å½•äº†æ•´ä¸ªseqçš„ä¿¡æ¯</strong>ï¼šå®ƒä¼šå»attendæ¯ä¸€ä¸ªtokenï¼ŒåŒæ ·ï¼Œå®ƒä¹Ÿä¼šè¢«æ¯ä¸ªtoken attendã€‚<br><img src="images/2024/08/2197013624.png" alt="Global Attention"><br>ä¸Šé¢æ˜¯ä¸¤ç§åšæ³•ï¼š</p>
<ul>
<li>åœ¨<strong>åŸseqä¸­</strong>é€‰æ‹©ç‰¹æ®Šçš„tokenã€‚</li>
<li>åœ¨<strong>åŸseqå¤–</strong>é¢å¤–æ·»åŠ ç‰¹æ®Šçš„tokenã€‚</li>
</ul>
<hr>
<p>é‚£ä¹ˆè¿™ä¹ˆå¤šæ–¹æ³•ï¼Œè¯¥å¦‚ä½•é€‰æ‹©å‘¢ï¼Ÿç­”æ¡ˆæ˜¯ï¼š<strong>å…¨éƒ¨é€‰æ‹©</strong>ã€‚å³ä¸åŒçš„headä½¿ç”¨ä¸åŒçš„attentionï¼Œè¿™æ ·æ•ˆæœå¯èƒ½ä¼šæ›´å¥½ã€‚<br>ä¾‹å¦‚ï¼š<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.05150">ã€ŠLongformer: The Long-Document Transformerã€‹</a>ã€<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.14062">ã€ŠBig Bird: Transformers for Longer Sequencesã€‹</a><br><img src="images/2024/08/3616426535.png" alt="Longformer and BigBird"></p>
<p>ä»¥ä¸Šæ–¹æ³•éƒ½æ˜¯<strong>äººä¸ºå†³å®šè®¡ç®—å“ªäº›attention</strong>ï¼Œè¿˜æœ‰ä¸€äº›ä¸ä¾èµ–äººç›´è§‰çš„åšæ³•ã€‚<br>æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥<strong>å°†attention scoreéå¸¸ä½çš„åœ°æ–¹ç›´æ¥è®¾0</strong>ï¼Œè¿™æ ·å¯¹åŸattention matrixä¸ä¼šäº§ç”Ÿå¾ˆå¤§å½±å“ï¼Œè¿˜èƒ½åŠ é€Ÿè®¡ç®—ã€‚é‚£ä¹ˆé—®é¢˜æ˜¯ï¼š<strong>å¦‚ä½•ä¼°è®¡å‡ºå“ªäº›ä½ç½®å¯èƒ½å¾ˆå°å‘¢ï¼Ÿ</strong></p>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><p>åœ¨ä¸‹é¢ä¸¤ç¯‡æ–‡ç« é‡Œéƒ½ç”¨äº†ç±»ä¼¼çš„æ–¹æ³•Clustering<br><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rkgNKkHtvB">ã€ŠReformer: The Efficient Transformerã€‹</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.05997">Efficient Content-Based Sparse Attention with Routing Transformers</a><br>Clusteringçš„åšæ³•æ˜¯ï¼šå…ˆæŠŠqueryå’Œkeyæ‹¿å‡ºæ¥åšClusteringï¼Œæ¯”è¾ƒæ¥è¿‘çš„å±äºä¸€ä¸ªClusterï¼Œå¦åˆ™å±äºä¸åŒçš„Clusterã€‚è®¡ç®—Clusterçš„æ—¶å€™å¯ä»¥é‡‡ç”¨ä¸€ç§<strong>è™½ç„¶å¯èƒ½ä¸å‡†ç¡®ä½†æ˜¯å¿«é€Ÿçš„ä¼°æµ‹æ–¹æ³•</strong>ï¼Œä¸Šè¿°ä¸¤ç¯‡æ–‡ç« é‡‡ç”¨äº†ä¸åŒçš„è®¡ç®—clusterçš„æ–¹æ³•ï¼Œè¿™æ ·å°±èƒ½åŠ å¿«è®¡ç®—ã€‚<strong>åªæœ‰queryå’Œkeyåœ¨ä¸€ä¸ªclusterä¸­ï¼Œæ‰å»è®¡ç®—attention score</strong>ã€‚<br><img src="images/2024/08/3257410960.png" alt="Clustering"><br><img src="images/2024/08/2526671714.png" alt="calculate attention"></p>
<hr>
<p>ä¸Šè¿°è®¡ç®—æ–¹æ³•éƒ½æ˜¯ä¾é <strong>äººçš„ç†è§£</strong>ï¼ŒClusteringè™½ç„¶æ˜¯è¿‘ä¼¼0ï¼Œä½†æ˜¯â€œç›¸ä¼¼â€çš„æ¦‚å¿µä¹Ÿæ˜¯è®¤ä¸ºå®šä¹‰çš„ã€‚ä¸‹é¢çš„æ–¹æ³•æ˜¯<strong>é€šè¿‡learnçš„æ–¹å¼å†³å®šè®¡ç®—å“ªäº›attention</strong>ã€‚</p>
<h2 id="Learnable-Patternsâ€”â€”Sinkhorn-Sorting-Network"><a href="#Learnable-Patternsâ€”â€”Sinkhorn-Sorting-Network" class="headerlink" title="Learnable Patternsâ€”â€”Sinkhorn Sorting Network"></a>Learnable Patternsâ€”â€”Sinkhorn Sorting Network</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.11296">ã€ŠSparse Sinkhorn Attentionã€‹</a><br>é€šè¿‡learnä¸€ä¸ªmatrixï¼Œå†³å®šå“ªäº›attentionéœ€è¦è®¡ç®—ï¼š<br><img src="images/2024/08/2821130049.png" alt="Sinkhorn Sorting Network"></p>
<h2 id="Linformer"><a href="#Linformer" class="headerlink" title="Linformer"></a>Linformer</h2><p>ä¸Šé¢çš„æ–¹æ³•éƒ½è®¡ç®—äº†NÃ—Nçš„attention matrixï¼Œèƒ½ä¸èƒ½å‡å°attention matrixçš„å¤§å°å‘¢ï¼Ÿ<br>å…·ä½“æ–¹æ³•æ˜¯ï¼šæŒ‘é€‰éƒ¨åˆ†representive keyså’Œrepresentive valuesï¼Œç”¨representive keyså’Œqueryç›¸ä¹˜å¾—åˆ°çš„ç»“æœå’Œrepresentive valuesåšweighted sumå¾—åˆ°attention scoreã€‚<br><img src="images/2024/08/4001522953.png" alt="representive keys and representive values"><br>[scode type=â€yellowâ€]<strong>ä¸ºä»€ä¹ˆä¸é€‰æ‹©representive queryï¼Ÿ</strong><br>output seqçš„lengthä¼šç¼©çŸ­ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ä¼šæœ‰å½±å“ï¼ˆæ¯”å¦‚seqä¸­æ¯ä¸ªvectoréƒ½æœ‰labelï¼‰ã€‚[/scode]<br>ä¸€äº›é€‰æ‹©representive keyçš„æ–¹æ³•ï¼š<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.10198">ã€ŠGenerating Wikipedia by Summarizing Long Sequencesã€‹</a>ã€<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.04768">ã€ŠLinformer: Self-Attention with Linear Complexityã€‹</a><br><img src="images/2024/08/4099759351.png" alt="Reduce Numbers of Key"></p>
<h2 id="k-q-first-â†’-v-k-first"><a href="#k-q-first-â†’-v-k-first" class="headerlink" title="k,q first â†’ v,k first"></a>k,q first â†’ v,k first</h2><p>å›é¡¾ä¸€ä¸‹self-attentionçš„è®¡ç®—ï¼š<br><img src="images/2024/08/1109922326.png" alt="self-attention"><br>æˆ‘ä»¬å‡è®¾æ²¡æœ‰softmaxçš„æ­¥éª¤ï¼ˆå³$A\rightarrow A^\prime$çš„è¿‡ç¨‹ï¼‰ï¼Œé‚£ä¹ˆè®¡ç®—è¿‡ç¨‹å¯ä»¥è¡¨è¾¾ä¸ºï¼š<br><img src="images/2024/08/644508994.png" alt=""><br>åŠ é€Ÿæ–¹æ³•ï¼šå…ˆè®¡ç®—$VK^T$è€Œä¸å…ˆè®¡ç®—$K^TQ$ã€‚<br>åŸç†ï¼šè®¡ç®—é¡ºåºçš„ä¸åŒï¼Œç»“æœç›¸åŒï¼Œä½†ä¹˜æ³•è®¡ç®—æ¬¡æ•°ä¸åŒã€‚<br><img src="images/2024/08/3138545184.png" alt="k,q first v.s. v,k first"><br>æˆ‘ä»¬å‘ç°ï¼Œå…ˆè®¡ç®—$K^TQ$ä¹˜æ³•è¿ç®—æ¬¡æ•°ä¸º$(d+d^prime)N^2$ï¼Œè€Œå…ˆè®¡ç®—$VK^T$ä¹˜æ³•è¿ç®—çš„æ¬¡æ•°ä¸º$2dd^primeN$ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼ŒNçš„å¤§å°æ˜¯å¤§äºdçš„ï¼Œå› æ­¤æ”¹å˜è¿ç®—é¡ºåºä¹Ÿèƒ½åŠ å¿«è¿ç®—é€Ÿåº¦ã€‚<br>ä½†è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼šæˆ‘ä»¬å¿½ç•¥äº†softmaxã€‚ç°åœ¨å°±æ¥è§£å†³softmaxçš„é—®é¢˜ã€‚</p>
<p>å‡è®¾è¦äº§ç”Ÿattention score $b^1$ï¼Œé‚£ä¹ˆæœ‰ï¼š</p>
<script type="math/tex; mode=display">
b^1=\sum\limits_{i=1}^Na_{1,i}v^i=\sum\limits_{i=1}^N\frac{exp(q^1\cdot k^i)}{\sum\limits_{j=1}^Nexp(q^1\cdot k^j)}v^i</script><p>æˆ‘ä»¬å‡è®¾æœ‰ä¸€ä¸ª$\phi$ï¼Œå®ƒçš„ä½œç”¨æ˜¯<strong>è¿‘ä¼¼dot productçš„exponential</strong>ï¼Œå…·ä½“ä¸ºï¼š</p>
<script type="math/tex; mode=display">
exp(q\cdot k)\approx \phi(q)\cdot\phi(k)</script><p>å…¶ä¸­$\phi(q)$çš„ä½œç”¨æ˜¯å°†å…¶è½¬åŒ–ä¸ºå¦ä¸€ä¸ªvectorã€‚<br>é‚£ä¹ˆ:</p>
<script type="math/tex; mode=display">
b^1=\sum\limits_{i=1}^N\frac{exp(q^1\cdot k^i)}{\sum\limits_{j=1}^Nexp(q^1\cdot k^j)}v^i=\sum\limits_{i=1}^N\frac{\phi(q^1)\cdot\phi(k^i)}{\sum\limits_{j=1}^N\phi(q^1)\cdot\phi(k^j)}v^i=\frac{\sum\limits_{i=1}^N[\phi(q^1)\cdot\phi(k^i)]v^i}{\sum\limits_{j=1}^N\phi(q^1)\cdot\phi(k^j)}</script><p>å‡è®¾$\phi(q)$ä¼šå°†$q$è½¬åŒ–ä¸ºä¸€ä¸ªM dimçš„vectorï¼Œé‚£ä¹ˆï¼š</p>
<script type="math/tex; mode=display">
\begin{align} 
    \sum\limits_{i=1}^N[\phi(q^1)\cdot\phi(k^i)]v^i & =[\phi(q^1)\cdot\phi(k^1)]v^1+[\phi(q^1)\cdot\phi(k^2)]v^2+... \label{eq:eq1}
    \\[3pt]
    & = (q_1^1k_1^1+q_2^1k_2^1+...)v^1+(q_1^1k_1^2+q_2^1k_2^2+...)v^2+...  \label{eq:eq2}
    \\[7pt]
    & = q_1^1k_1^1v^1+q_2^1k_2^1v^1+...+q_1^1k_1^2v^2+q_2^1k_2^2v^2+...+...  \label{eq:eq3}
    \\[7pt]
    & = q_1^1(k_1^1v^1+k_1^2v^2+...)+q_2^1(k_2^1v^1+k_2^2v^2+...)+...  \label{eq:eq4}
\end{align}</script><p>å°†$\sum\limits_{j=1}^Nk_1^jv^j$è§†ä½œä¸€ä¸ªvectorï¼Œå¯ä»¥å¾—åˆ°ï¼š<br><img src="images/2024/08/512070726.png" alt=""><br>é‚£ä¹ˆæœ€ç»ˆçš„å¯ä»¥è¡¨è¾¾ä¸ºçŸ©é˜µè¿ç®—ï¼š<br><img src="images/2024/08/1220587946.png" alt=""><br>[scode type=â€greenâ€]è¿™æ ·åšå¸¦æ¥çš„å¥½å¤„æ˜¯ï¼šåœ¨è®¡ç®—æŸä¸ªattentionçš„æ—¶å€™ï¼Œ<strong>åªæœ‰queryæ˜¯å˜åŒ–çš„</strong>ï¼Œå…¶ä»–ä¸œè¥¿ä¸éœ€è¦é‡å¤è®¡ç®—ã€‚[/scode]</p>
<p>ä¸Šè¿°æ¨å¯¼åªæƒ³è¯´æ˜ä¸€ä»¶äº‹ï¼šå¯ä»¥é€šè¿‡ä¸Šè¿°æ–¹æ³•ç®€åŒ–è®¡ç®—ï¼Œè€Œself-attentionçš„ç»“æœä¸ä¼šæœ‰å¾ˆå¤§è¯¯å·®ã€‚è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š<br><img src="images/2024/08/1526852039.png" alt=""><br>é¦–å…ˆé€šè¿‡$k$å’Œ$v$ç®—å‡ºMä¸ªvectorï¼Œç„¶åä¾æ¬¡ä½¿ç”¨queryå»å’Œè¿™Mä¸ªvectoråšweighted sumå³å¯å¾—åˆ°åˆ†å­è¿™ä¸€é¡¹ï¼Œåˆ†æ¯åŒç†ã€‚<br><img src="images/2024/08/3533257680.png" alt=""><br>è¿™ç§åšæ³•å”¯ä¸€çš„é—®é¢˜å°±æ˜¯å¦‚ä½•å®šä¹‰$\phi$ã€‚ä¸åŒçš„paperæœ‰ä¸åŒçš„åšæ³•ï¼š</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01243">ã€ŠEfficient Attention: Attention with Linear Complexitiesã€‹</a></li>
<li><a target="_blank" rel="noopener" href="https://linear-transformers.com/">ã€ŠTransformers are RNNs:Fast Autoregressive Transformers with Linear Attentionã€‹</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.02143">ã€ŠRandom Feature Attentionã€‹</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a><h2 id="Synthesizer"><a href="#Synthesizer" class="headerlink" title="Synthesizer"></a>Synthesizer</h2>è¿˜æœ‰ä¸€ç§åšæ³•ï¼Œä¸éœ€è¦$q,k$è®¡ç®—attention matrixã€‚è¿™ç§æ–¹æ³•é€šè¿‡learnå»å­¦ä¹ attention matrixï¼ŒæŠŠä»–å½“ä½œnetworkçš„ä¸€éƒ¨åˆ†ï¼Œå³Synthesizerã€‚<h2 id="New-Frameworkâ€”â€”Attention-free"><a href="#New-Frameworkâ€”â€”Attention-free" class="headerlink" title="New Frameworkâ€”â€”Attention-free"></a>New Frameworkâ€”â€”Attention-free</h2>ä¸‹é¢è¿™äº›æ–¹æ³•ä¸ä½¿ç”¨attentionå»å¤„ç†seq2seqçš„é—®é¢˜ï¼š</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.03824">ã€ŠFNet: Mixing Tokens with Fourier Transformsã€‹</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.08050">ã€ŠPay Attention to MLPsã€‹</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.01601">ã€ŠMLP-Mixer: An all-MLP Architecture for Visionã€‹</a><h1 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h1><img src="images/2024/08/962417182.png" alt="Summary"><br>æ³¨ï¼šLRAåˆ†æ•°è¶Šé«˜ï¼Œä»£è¡¨attentionè¡¨ç°è¶Šå¥½ã€‚æ¯ä¸ªæ–¹æ³•çš„åœˆåœˆå¤§å°ä»£è¡¨ä½¿ç”¨memoryçš„å¤§å°ã€‚</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/week5-Transformer-and-Variable-Attention.html" data-id="cm59dq8ap0000dk3k4oao8i3y" data-title="[week5]Transformer and Variable Attention" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/articles/week6-Generation.html" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [week6]Generation
        
      </div>
    </a>
  
  
    <a href="/articles/week5-extra-Batch-Normalization.html" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[week5 extra]Batch Normalization</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Env-Setting/">Env Setting</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NTU-ML2022/">NTU-ML2022</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/articles/linux%E9%85%8D%E7%BD%AEgit%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0github.html">linuxé…ç½®gitå¹¶éƒ¨ç½²åˆ°github</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E4%B8%8B%E7%94%A8qemu%E6%A8%A1%E6%8B%9Ffreedos%E7%BC%96%E5%86%9916%E4%BD%8D%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80.html">ubuntuä¸‹ç”¨qemuæ¨¡æ‹Ÿfreedosç¼–å†™16ä½æ±‡ç¼–è¯­è¨€</a>
          </li>
        
          <li>
            <a href="/articles/week12-extra-Q-learning.html">[week12 extra] Q-learning</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%A4%9A%E7%89%88%E6%9C%ACcuda%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%88%87%E6%8D%A2.html">ubuntuç¯å¢ƒä¸‹å¤šç‰ˆæœ¬cudaçš„å®‰è£…ä¸åˆ‡æ¢</a>
          </li>
        
          <li>
            <a href="/articles/week12-Reinforcement-Learning.html">[week12]Reinforcement Learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 zz111y<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
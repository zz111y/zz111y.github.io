<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>[week5]Transformer and Variable Attention | zz111y&#39;s stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Sequence-to-sequence(Seq2seq)seq2seq：input是一个seq，output也是一个seq，且output的seq长度由model决定。seq2seq Model在很多领域都有应用，例如NLP的大部分领域（语音辨识、翻译、语音翻译等）、multi-label classification、Object Detection等。虽然seq2seq Model在这些领域">
<meta property="og:type" content="article">
<meta property="og:title" content="[week5]Transformer and Variable Attention">
<meta property="og:url" content="https://zz111y.github.io/articles/week5-Transformer-and-Variable-Attention.html">
<meta property="og:site_name" content="zz111y&#39;s stack">
<meta property="og:description" content="Sequence-to-sequence(Seq2seq)seq2seq：input是一个seq，output也是一个seq，且output的seq长度由model决定。seq2seq Model在很多领域都有应用，例如NLP的大部分领域（语音辨识、翻译、语音翻译等）、multi-label classification、Object Detection等。虽然seq2seq Model在这些领域">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4252600933.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3500171285.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1168952771.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/641826901.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1218346287.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1102089619.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1102159223.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3406154327.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3141551010.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1875163849.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3885036104.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2401978987.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/195258900.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4096320065.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3137948287.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1695904727.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2298068304.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2379751845.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2197013624.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3616426535.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3257410960.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2526671714.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2821130049.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4001522953.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/4099759351.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1109922326.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/644508994.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3138545184.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/512070726.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1220587946.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1526852039.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3533257680.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/962417182.png">
<meta property="article:published_time" content="2024-08-11T02:56:00.000Z">
<meta property="article:modified_time" content="2024-12-29T08:59:27.069Z">
<meta property="article:author" content="zz111y">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zz111y.github.io/articles/images/2024/08/4252600933.png">
  
    <link rel="alternate" href="/atom.xml" title="zz111y's stack" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zz111y&#39;s stack</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zz111y.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-week5-Transformer-and-Variable-Attention" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/week5-Transformer-and-Variable-Attention.html" class="article-date">
  <time class="dt-published" datetime="2024-08-11T02:56:00.000Z" itemprop="datePublished">2024-08-11</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      [week5]Transformer and Variable Attention
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Sequence-to-sequence-Seq2seq"><a href="#Sequence-to-sequence-Seq2seq" class="headerlink" title="Sequence-to-sequence(Seq2seq)"></a>Sequence-to-sequence(Seq2seq)</h1><p>seq2seq：input是一个seq，output也是一个seq，且output的seq长度由model决定。<br>seq2seq Model在很多领域都有应用，例如NLP的大部分领域（语音辨识、翻译、语音翻译等）、multi-label classification、Object Detection等。虽然seq2seq Model在这些领域都能应用，但<strong>为每个问题客制化定制Model</strong>才能达到更好的效果。</p>
<p>Seq2seq的结构为<strong>一个Encoder和一个Decoder</strong>，其中Encoder吃input，将处理好的结果送进Decoder，Decoder决定输出是什么seq。seq2seq的起源：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">《Sequence to Sequence Learning with Neural Networks》</a><br>当下，最常用的seq2seq是transformer：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">attention is all you need</a><br>transformer的架构如下图所示：</p>
<p><div align="center">
<img src="images/2024/08/4252600933.png" width="50%">
</div></p>
<h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder做的事情是<strong>吃一排向量，输出同样数量的一排向量</strong>。这样的事情很多架构都能做到，而<strong>transformer中应用的是self-attention</strong>。<br>具体而言，Encoder的input经过了一个一个的Block（包含多个hidden layer），最后得到output，而<strong>transformer的block为self-attention+fully-connected</strong>。<br><img src="images/2024/08/3500171285.png" alt="transformer Encoder block"><br>在最早提出的transformer架构中，还要加上一个residual connection以及各层输出都要经过normalization：<br><img src="images/2024/08/1168952771.png" alt="a block in transformer Encoder"><br>这里的normalization使用的是<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p>
<p>总上所述，再看一下transformer的架构Encoder部分：<br><img src="images/2024/08/641826901.png" alt="transformer Encoder"><br>这是最原始的Encoder设计，目前已经有了很多变种，例如：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.04745">《On Layer Normalization in the Transformer Architecture》</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.07845">《PowerNorm: Rethinking Batch Normalization in Transformers》</a></p>
<h2 id="Decoder——Autoregressive-AT"><a href="#Decoder——Autoregressive-AT" class="headerlink" title="Decoder——Autoregressive(AT)"></a>Decoder——Autoregressive(AT)</h2><p>Decoder会读入Encoder的output，从一个special token（BEGIN）开始，输出一个vector，这个vector的长度和vocabulary size是一样的（以中文语音辨识为例，vocabulary就是所有的常见字），vector中的值是一个distribution，其中概率最大的就作为当前output：<br><img src="images/2024/08/1218346287.png" alt="Dncoder"><br>随后，将此前所有的output作为额外的input，加上BEGIN和Encoder的output共同作为input，进行下一次output。<br><img src="images/2024/08/1102089619.png" alt="Dncoder"><br>Decoder的结构如下：<br><img src="images/2024/08/1102159223.png" alt="Decoder structure"><br>对比一下Encoder和Decoder：<br><img src="images/2024/08/3406154327.png" alt="Encoder v.s. Decoder"><br>如果忽略被遮挡部分，那么Encoder和Decoder是完全一样的。<br>[scode type=”yellow”]<strong>Masked Attention</strong><br>简而言之，就是self-attention产生output的时候<strong>只能获取自身及之前的信息</strong>。<br><img src="images/2024/08/3141551010.png" alt="Masked Attention"><br>Masked原因是：Decoder的时候，output是一个一个产生的，所以他只能考虑之前产生的vector[/scode]<br>Decoder会设置一个<strong>end token</strong>，当output end token的时候，Decoder就会停止。</p>
<hr>
<p><strong>AT v.s. NAT（non-autoregressive）</strong><br>相较于AT，NAT最显著的区别是吃一排BEGIN token，同时产生output。其控制序列长度的方法可能为：</p>
<ul>
<li>用另一个model predict长度</li>
<li>A给定一个很大的长度，截取end token之前的output</li>
</ul>
<p><img src="images/2024/08/1875163849.png" alt="AT v.s. NAT"><br>NAT的好处：平行计算，速率更快；可控制的output长度。但NAT的performence不如AT（由于Multi-modality），因此当下对NAT的优化仍是一个问题。</p>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>Encoder与Decoder的连接部分叫做<strong>Cross-attention</strong>。由上文transformer架构中可以发现，Encoder吐出两个东西被Decoder所接收，这一部分所进行的操作就是cross-attention。<br><img src="images/2024/08/3885036104.png" alt="cross-attention"><br>其具体的运作机制为（以BEGIN为例）：<br>BEGIN经过一个Masked self-attention得到一个vector，然后乘上一个矩阵得到$q$，用这个$q$去对Encoder的output求attention score，在做一个weighted sum，得到的输出送进Decoder的fully-connected layer。如下图所示：<br><img src="images/2024/08/2401978987.png" alt="cross-attention"><br>对于后续产生的output，也是同理：<br><img src="images/2024/08/195258900.png" alt="example"><br>To learn more，另一种cross-attention方式：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08081">《Rethinking and Improving Natural Language Generation with Layer-Wise Multi-View Decoding》</a></p>
<h2 id="how-to-traing"><a href="#how-to-traing" class="headerlink" title="how to traing"></a>how to traing</h2><p>transformer所做的事情可以看作是一次一次的classification，因此可以计算每个predict和truth的cross entropy，然后minimize cross entropy即可。但实际<strong>train的时候，Decoder的input是正确的label，而非predict的结果</strong>。这种做法叫做<strong>teacher forcing</strong>，可以加速模型训练过程，提高生成序列的质量。<br><img src="images/2024/08/4096320065.png" alt="teacher forcing"><br>当然，这样做会有一个问题：在testing的时候是没有正确label的，因此这就会造成模型的mismatch，这种情况叫做exposure bias。<br><strong>一个可能的解决方法</strong>：Scheduled Sampling。简而言之，就是给Decoder一些错误的信息。<br>原始的Scheduled Sampling：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.03099">《Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks》</a><br>一些改进：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.07651">《Scheduled Sampling for Transformers》</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.04331">《Parallel Scheduled Sampling》</a></p>
<h2 id="Tips"><a href="#Tips" class="headerlink" title="Tips"></a>Tips</h2><h3 id="Copy-Mechanism"><a href="#Copy-Mechanism" class="headerlink" title="Copy Mechanism"></a>Copy Mechanism</h3><p>有些时候，input seq中可能会出现一些奇怪的东西，例如在翻译或做chat-bot的时候，人名、地名等。这种情况下机器很难学到这些古怪的东西，因此<strong>直接将其复制下来到output中</strong>也是一种做法。<br>最早实现这种方法的是<strong>Pointer network</strong>，下面这篇文章使用了pointer network：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.04368">《Get To The Point: Summarization with Pointer-Generator Networks》</a><br>其变形：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.06393">《Incorporating Copying Mechanism in Sequence-to-Sequence Learning》</a></p>
<h3 id="Guided-Attention"><a href="#Guided-Attention" class="headerlink" title="Guided Attention"></a>Guided Attention</h3><p>在某些情况下，机器可能会犯一些严重的错误，比如output缺失、遗漏某个input的信息。可以使用<strong>Guided Attention</strong>，相当于认为引导机器的行为。例如在做语音合成的时候，attention是由左向右的，这时就可以认为限制attention的过程。<br>Guided Attention方法关键词：Monotonic Attention、Location-aware Attention。</p>
<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><blockquote>
<p>GPT-4o：Beam Search是一种常用的启发式搜索算法，主要用于自然语言处理（NLP）中的序列生成任务，如机器翻译、文本摘要和对话生成等。相比于贪心搜索（Greedy Search），Beam Search在生成序列时可以同时保留多个候选路径，从而提高生成结果的质量。<br><strong>工作原理</strong><br>Beam Search的主要思想是保留多个候选路径（称为“beam”）而不是只保留一个最优路径。在每个时间步，算法会扩展所有当前的候选路径，并只保留得分最高的𝑘个路径。这里的𝑘称为beam size。</p>
</blockquote>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>在NLP方面，BLEU score往往是评价一个model好坏的标准。</p>
<blockquote>
<p>GPT-4o：BLEU（Bilingual Evaluation Understudy）评分是一种用于评估机器翻译和其他自然语言生成模型的质量的指标。BLEU评分通过比较生成的文本与一个或多个参考文本来衡量翻译的准确性和流畅性。其核心思想是计算生成文本和参考文本之间的 n-gram 的重叠程度。</p>
</blockquote>
<p>我们train的时候，使用的是cross entropy作为loss，而实际评测标准可能是BLEU score，因为cross entropy是可微分的，而BLEU score较为复杂且不可微分，因此如果使用BLEU score作为loss，optimization就会难以进行。<br>那该怎么办呢？When you don’t know how to optimize, just use reinforcement learning!</p>
<h1 id="各种各样的self-attention"><a href="#各种各样的self-attention" class="headerlink" title="各种各样的self-attention"></a>各种各样的self-attention</h1><p>self-attention的各种变体往往取名为<strong>xxxformer</strong>，先放一张各种self-attention的对比：<br><img src="images/2024/08/3137948287.png" alt="2024-08-08T08:02:30.png"><br>在各个Model中，self-attention只是其中一个部分。我们知道，self-attention的计算复杂度是和seq的长度成平方的关系的，因此当seq很长时，<strong>self-attention占据了计算的大部分时间</strong>。这些优化的算法，大部分都是<strong>设法减少self-attention的计算</strong>。<br><img src="images/2024/08/1695904727.png" alt=""></p>
<h2 id="Local-Attention-Truncated-Attention"><a href="#Local-Attention-Truncated-Attention" class="headerlink" title="Local Attention/Truncated Attention"></a>Local Attention/Truncated Attention</h2><p>有些情况下，可能<strong>不需要了解整个seq的信息，只需要了解附近vecotr的信息</strong>。具体做法是，$i,j$接近的时候才取计算attention score，其他位置直接设0。<br><img src="images/2024/08/2298068304.png" alt="Local Attention/Truncated Attention"><br>但是这样的话，就和CNN没什么两样。因此这种方法，虽然可以加速运算，但不一定能有好的效果。</p>
<h2 id="Stride-Attention"><a href="#Stride-Attention" class="headerlink" title="Stride Attention"></a>Stride Attention</h2><p>考虑到local attention不能考虑到距离远的位置，但还要简化计算，可以使用Stride Attention。其机制为<strong>每隔一定的Stride计算attention score</strong>，其他设0。<br><img src="images/2024/08/2379751845.png" alt="Stride Attention"></p>
<h2 id="Global-Attention"><a href="#Global-Attention" class="headerlink" title="Global Attention"></a>Global Attention</h2><p>以上两种做法都是以某个位置为中心去计算，如果<strong>想知道整个seq，可以使用Global Attention</strong>。简单来说，就是加上一个<strong>特殊的token，这个token记录了整个seq的信息</strong>：它会去attend每一个token，同样，它也会被每个token attend。<br><img src="images/2024/08/2197013624.png" alt="Global Attention"><br>上面是两种做法：</p>
<ul>
<li>在<strong>原seq中</strong>选择特殊的token。</li>
<li>在<strong>原seq外</strong>额外添加特殊的token。</li>
</ul>
<hr>
<p>那么这么多方法，该如何选择呢？答案是：<strong>全部选择</strong>。即不同的head使用不同的attention，这样效果可能会更好。<br>例如：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2004.05150">《Longformer: The Long-Document Transformer》</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2007.14062">《Big Bird: Transformers for Longer Sequences》</a><br><img src="images/2024/08/3616426535.png" alt="Longformer and BigBird"></p>
<p>以上方法都是<strong>人为决定计算哪些attention</strong>，还有一些不依赖人直觉的做法。<br>比如，我们可以<strong>将attention score非常低的地方直接设0</strong>，这样对原attention matrix不会产生很大影响，还能加速计算。那么问题是：<strong>如何估计出哪些位置可能很小呢？</strong></p>
<h2 id="Clustering"><a href="#Clustering" class="headerlink" title="Clustering"></a>Clustering</h2><p>在下面两篇文章里都用了类似的方法Clustering<br><a target="_blank" rel="noopener" href="https://openreview.net/forum?id=rkgNKkHtvB">《Reformer: The Efficient Transformer》</a><br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2003.05997">Efficient Content-Based Sparse Attention with Routing Transformers</a><br>Clustering的做法是：先把query和key拿出来做Clustering，比较接近的属于一个Cluster，否则属于不同的Cluster。计算Cluster的时候可以采用一种<strong>虽然可能不准确但是快速的估测方法</strong>，上述两篇文章采用了不同的计算cluster的方法，这样就能加快计算。<strong>只有query和key在一个cluster中，才去计算attention score</strong>。<br><img src="images/2024/08/3257410960.png" alt="Clustering"><br><img src="images/2024/08/2526671714.png" alt="calculate attention"></p>
<hr>
<p>上述计算方法都是依靠<strong>人的理解</strong>，Clustering虽然是近似0，但是“相似”的概念也是认为定义的。下面的方法是<strong>通过learn的方式决定计算哪些attention</strong>。</p>
<h2 id="Learnable-Patterns——Sinkhorn-Sorting-Network"><a href="#Learnable-Patterns——Sinkhorn-Sorting-Network" class="headerlink" title="Learnable Patterns——Sinkhorn Sorting Network"></a>Learnable Patterns——Sinkhorn Sorting Network</h2><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.11296">《Sparse Sinkhorn Attention》</a><br>通过learn一个matrix，决定哪些attention需要计算：<br><img src="images/2024/08/2821130049.png" alt="Sinkhorn Sorting Network"></p>
<h2 id="Linformer"><a href="#Linformer" class="headerlink" title="Linformer"></a>Linformer</h2><p>上面的方法都计算了N×N的attention matrix，能不能减小attention matrix的大小呢？<br>具体方法是：挑选部分representive keys和representive values，用representive keys和query相乘得到的结果和representive values做weighted sum得到attention score。<br><img src="images/2024/08/4001522953.png" alt="representive keys and representive values"><br>[scode type=”yellow”]<strong>为什么不选择representive query？</strong><br>output seq的length会缩短，在某些情况下会有影响（比如seq中每个vector都有label）。[/scode]<br>一些选择representive key的方法：<br><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.10198">《Generating Wikipedia by Summarizing Long Sequences》</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.04768">《Linformer: Self-Attention with Linear Complexity》</a><br><img src="images/2024/08/4099759351.png" alt="Reduce Numbers of Key"></p>
<h2 id="k-q-first-→-v-k-first"><a href="#k-q-first-→-v-k-first" class="headerlink" title="k,q first → v,k first"></a>k,q first → v,k first</h2><p>回顾一下self-attention的计算：<br><img src="images/2024/08/1109922326.png" alt="self-attention"><br>我们假设没有softmax的步骤（即$A\rightarrow A^\prime$的过程），那么计算过程可以表达为：<br><img src="images/2024/08/644508994.png" alt=""><br>加速方法：先计算$VK^T$而不先计算$K^TQ$。<br>原理：计算顺序的不同，结果相同，但乘法计算次数不同。<br><img src="images/2024/08/3138545184.png" alt="k,q first v.s. v,k first"><br>我们发现，先计算$K^TQ$乘法运算次数为$(d+d^prime)N^2$，而先计算$VK^T$乘法运算的次数为$2dd^primeN$。一般情况下，N的大小是大于d的，因此改变运算顺序也能加快运算速度。<br>但还有一个问题：我们忽略了softmax。现在就来解决softmax的问题。</p>
<p>假设要产生attention score $b^1$，那么有：</p>
<script type="math/tex; mode=display">
b^1=\sum\limits_{i=1}^Na_{1,i}v^i=\sum\limits_{i=1}^N\frac{exp(q^1\cdot k^i)}{\sum\limits_{j=1}^Nexp(q^1\cdot k^j)}v^i</script><p>我们假设有一个$\phi$，它的作用是<strong>近似dot product的exponential</strong>，具体为：</p>
<script type="math/tex; mode=display">
exp(q\cdot k)\approx \phi(q)\cdot\phi(k)</script><p>其中$\phi(q)$的作用是将其转化为另一个vector。<br>那么:</p>
<script type="math/tex; mode=display">
b^1=\sum\limits_{i=1}^N\frac{exp(q^1\cdot k^i)}{\sum\limits_{j=1}^Nexp(q^1\cdot k^j)}v^i=\sum\limits_{i=1}^N\frac{\phi(q^1)\cdot\phi(k^i)}{\sum\limits_{j=1}^N\phi(q^1)\cdot\phi(k^j)}v^i=\frac{\sum\limits_{i=1}^N[\phi(q^1)\cdot\phi(k^i)]v^i}{\sum\limits_{j=1}^N\phi(q^1)\cdot\phi(k^j)}</script><p>假设$\phi(q)$会将$q$转化为一个M dim的vector，那么：</p>
<script type="math/tex; mode=display">
\begin{align} 
    \sum\limits_{i=1}^N[\phi(q^1)\cdot\phi(k^i)]v^i & =[\phi(q^1)\cdot\phi(k^1)]v^1+[\phi(q^1)\cdot\phi(k^2)]v^2+... \label{eq:eq1}
    \\[3pt]
    & = (q_1^1k_1^1+q_2^1k_2^1+...)v^1+(q_1^1k_1^2+q_2^1k_2^2+...)v^2+...  \label{eq:eq2}
    \\[7pt]
    & = q_1^1k_1^1v^1+q_2^1k_2^1v^1+...+q_1^1k_1^2v^2+q_2^1k_2^2v^2+...+...  \label{eq:eq3}
    \\[7pt]
    & = q_1^1(k_1^1v^1+k_1^2v^2+...)+q_2^1(k_2^1v^1+k_2^2v^2+...)+...  \label{eq:eq4}
\end{align}</script><p>将$\sum\limits_{j=1}^Nk_1^jv^j$视作一个vector，可以得到：<br><img src="images/2024/08/512070726.png" alt=""><br>那么最终的可以表达为矩阵运算：<br><img src="images/2024/08/1220587946.png" alt=""><br>[scode type=”green”]这样做带来的好处是：在计算某个attention的时候，<strong>只有query是变化的</strong>，其他东西不需要重复计算。[/scode]</p>
<p>上述推导只想说明一件事：可以通过上述方法简化计算，而self-attention的结果不会有很大误差。计算方法如下：<br><img src="images/2024/08/1526852039.png" alt=""><br>首先通过$k$和$v$算出M个vector，然后依次使用query去和这M个vector做weighted sum即可得到分子这一项，分母同理。<br><img src="images/2024/08/3533257680.png" alt=""><br>这种做法唯一的问题就是如何定义$\phi$。不同的paper有不同的做法：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.01243">《Efficient Attention: Attention with Linear Complexities》</a></li>
<li><a target="_blank" rel="noopener" href="https://linear-transformers.com/">《Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention》</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.02143">《Random Feature Attention》</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a><h2 id="Synthesizer"><a href="#Synthesizer" class="headerlink" title="Synthesizer"></a>Synthesizer</h2>还有一种做法，不需要$q,k$计算attention matrix。这种方法通过learn去学习attention matrix，把他当作network的一部分，即Synthesizer。<h2 id="New-Framework——Attention-free"><a href="#New-Framework——Attention-free" class="headerlink" title="New Framework——Attention-free"></a>New Framework——Attention-free</h2>下面这些方法不使用attention去处理seq2seq的问题：</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.03824">《FNet: Mixing Tokens with Fourier Transforms》</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.08050">《Pay Attention to MLPs》</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2105.01601">《MLP-Mixer: An all-MLP Architecture for Vision》</a><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><img src="images/2024/08/962417182.png" alt="Summary"><br>注：LRA分数越高，代表attention表现越好。每个方法的圈圈大小代表使用memory的大小。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/week5-Transformer-and-Variable-Attention.html" data-id="cm59dq8ap0000dk3k4oao8i3y" data-title="[week5]Transformer and Variable Attention" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/articles/week6-Generation.html" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [week6]Generation
        
      </div>
    </a>
  
  
    <a href="/articles/week5-extra-Batch-Normalization.html" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[week5 extra]Batch Normalization</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Env-Setting/">Env Setting</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NTU-ML2022/">NTU-ML2022</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/articles/linux%E9%85%8D%E7%BD%AEgit%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0github.html">linux配置git并部署到github</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E4%B8%8B%E7%94%A8qemu%E6%A8%A1%E6%8B%9Ffreedos%E7%BC%96%E5%86%9916%E4%BD%8D%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80.html">ubuntu下用qemu模拟freedos编写16位汇编语言</a>
          </li>
        
          <li>
            <a href="/articles/week12-extra-Q-learning.html">[week12 extra] Q-learning</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%A4%9A%E7%89%88%E6%9C%ACcuda%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%88%87%E6%8D%A2.html">ubuntu环境下多版本cuda的安装与切换</a>
          </li>
        
          <li>
            <a href="/articles/week12-Reinforcement-Learning.html">[week12]Reinforcement Learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 zz111y<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
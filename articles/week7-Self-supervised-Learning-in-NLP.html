<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>[week7]Self-supervised Learning in NLP | zz111y&#39;s stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="相较于supervised learning给出成对的data和label，以及unsupervised learning全部给出无label的data，self-supervised learning更像是两者结合的产物，但由于其没有label，因此也可以算作是一种unsupervised learning。其最早出现于Yann LeCun的facebook： 首先需要介绍一下芝麻街，这是一部动">
<meta property="og:type" content="article">
<meta property="og:title" content="[week7]Self-supervised Learning in NLP">
<meta property="og:url" content="https://zz111y.github.io/articles/week7-Self-supervised-Learning-in-NLP.html">
<meta property="og:site_name" content="zz111y&#39;s stack">
<meta property="og:description" content="相较于supervised learning给出成对的data和label，以及unsupervised learning全部给出无label的data，self-supervised learning更像是两者结合的产物，但由于其没有label，因此也可以算作是一种unsupervised learning。其最早出现于Yann LeCun的facebook： 首先需要介绍一下芝麻街，这是一部动">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1181292986.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/8646709.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3717054905.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1865001741.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1667801115.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2377603511.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1636239827.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/467884839.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2150153133.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1912658737.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3433405695.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/1877988107.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3639479524.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2976768404.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3793866444.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/3314264047.png">
<meta property="og:image" content="https://zz111y.github.io/articles/images/2024/08/2030470944.png">
<meta property="article:published_time" content="2024-08-20T07:50:00.000Z">
<meta property="article:modified_time" content="2024-12-22T09:52:44.770Z">
<meta property="article:author" content="zz111y">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zz111y.github.io/articles/images/2024/08/1181292986.png">
  
    <link rel="alternate" href="/atom.xml" title="zz111y's stack" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zz111y&#39;s stack</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zz111y.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-week7-Self-supervised-Learning-in-NLP" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/week7-Self-supervised-Learning-in-NLP.html" class="article-date">
  <time class="dt-published" datetime="2024-08-20T07:50:00.000Z" itemprop="datePublished">2024-08-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      [week7]Self-supervised Learning in NLP
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>相较于supervised learning给出成对的data和label，以及unsupervised learning全部给出无label的data，self-supervised learning更像是两者结合的产物，但由于其没有label，因此也可以算作是一种unsupervised learning。其最早出现于Yann LeCun的facebook：<br><img src="images/2024/08/1181292986.png" alt="self-supervised learning"></p>
<p>首先需要介绍一下芝麻街，这是一部动画片，self-supervised learning中一部分的Model是以其中人物命名的，包括BERT：<br><img src="images/2024/08/8646709.png" alt="Sesame Street"></p>
<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>首先介绍一下BERT。原始paper：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</a>。它其实就是一个transformer的Encoder，要做的事情就是输入一排vector，输出一排vector。BERT是一个有340M parameters的巨大模型：<br><img src="images/2024/08/3717054905.png" alt="BERT"><br>他的input是Masking Input，简单来说有两种做法：</p>
<ul>
<li>用一个special token随即盖住一些token；</li>
<li>用一些随机的token随即替换一些token。</li>
</ul>
<p>BERT要做的就是<strong>预测这些Masking的token</strong>。盖住部分对应的output经过了linear和softmax后得到一个distribution，训练的目标是<strong>output和原始Masking的部分越接近越好</strong>。</p>
<p>在大多情况下，BERT只是作为一个<strong>Pre-train的Model</strong>，要在上面进行<strong>Fine-tune</strong>（微调），做更多的Downstream Tasks。因为实际上BERT训练的就是一个做“<strong>填空题</strong>”的Model，但进行了fine-tune后可以做更多事。<br><img src="images/2024/08/1865001741.png" alt="Downstream Tasks"></p>
<p>例如，我们可以用BERT判断两个句子是不是连在一起的。我们需要将两个句子拼起来，中间加一个<strong>SEP token</strong>表示分隔，最开始加一个<strong>CLS token</strong>用于生成答案。<br><img src="images/2024/08/1667801115.png" alt="Next Sentence Prediction"></p>
<p>对于BERT的评判，由于其更像是一个“胚胎干细胞”，他的作用就是产生各种各样的Model用作不同的downstream tasks，因此对其的评判需要多种多样。GLUE（General Language Understanding Evaluation）就是一个较官方的<strong>评判集</strong>，有多个任务可以判断Language Model的好坏。<br><img src="images/2024/08/2377603511.png" alt="The General Language Understanding Evaluation (GLUE)"><br>下图是一些类似BERT的Model的GLUE score的对比图：<br><img src="images/2024/08/1636239827.png" alt="BERT and its family"><br>来源：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.00537">《SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems》</a></p>
<h2 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h2><p>下面是一些BERT应用的举例</p>
<h3 id="Sentiment-analysis"><a href="#Sentiment-analysis" class="headerlink" title="Sentiment analysis"></a>Sentiment analysis</h3><p>输入一个seq，输出一个class。例如Sentiment analysis。<br><img src="images/2024/08/467884839.png" alt=""><br>我们需要在CLS的output上加一些额外的东西，作为我们的fine-tune。这样一个大的Model是我们要去train的，但其中BERT的部分是pre-train的，额外加的部分是随即初始化的。</p>
<p>[scode type=”yellow”]<strong>为什么要Pre-train</strong><br><img src="images/2024/08/2150153133.png" alt="《Visualizing and Understanding the Effectiveness of BERT》"><br>不难看出，<strong>pre-train</strong>的Model效果明显要好于随机初始化BERT的。[/scode]</p>
<h3 id="POS-tagging"><a href="#POS-tagging" class="headerlink" title="POS tagging"></a>POS tagging</h3><p>输入一个seq，输出相同数量class，例如POS tagging。<br><img src="images/2024/08/1912658737.png" alt=""></p>
<h3 id="Natural-Language-Inference-NLI"><a href="#Natural-Language-Inference-NLI" class="headerlink" title="Natural Language Inference(NLI)"></a>Natural Language Inference(NLI)</h3><p>输入两个seq，输出一个class。例如NLI。以下是一个例子，输入一个句子作为前提，一个句子作为假设，判断两个句子是否矛盾。<br><img src="images/2024/08/3433405695.png" alt=""><br><img src="images/2024/08/1877988107.png" alt=""></p>
<h3 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h3><p>我们还可以输入一篇文章，再输入一个问题，最后输出问题的答案。<br><img src="images/2024/08/3639479524.png" alt=""><br><img src="images/2024/08/2976768404.png" alt=""><br>但这个做法有些局限性，回答是出自文章的原句，不会自己产生其他的回答。</p>
<h2 id="other-knowledge"><a href="#other-knowledge" class="headerlink" title="other knowledge"></a>other knowledge</h2><ul>
<li>BERT实际上是很难训练的，因为其Model非常大。这篇paper讲了BERT训练的过程，探究了在训练过程中<strong>BERT究竟学到了什么</strong>：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.02480">《Pretrained Language Model Embryology: The Birth of ALBERT》</a></li>
<li>BERT实际上是pre-train了一个transformer的Encoder，我们也可以pre-train Encoder和Decoder，也就是一个会做填空题的seq2seq的Model。<br><img src="images/2024/08/3793866444.png" alt="Pre-train a seq2seq Model"><br>实际上，对于pre-train seq2seq，masking的方法有很多。这篇paper介绍了多种MASS的方法：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1905.02450">《MASS: Masked Sequence to Sequence Pre-training for Language Generation》</a>，这篇paper使用了上述方法pre-train了一个seq2seq：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.13461">《BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension》</a></li>
<li>惊人的发现，用一种语言fine-tune BERT，然后用另一种语言去test，居然也能在一定程度上起效果：<br><img src="images/2024/08/3314264047.png" alt=""><br>来源：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1909.09587">《Zero-shot Reading Comprehension by Cross-lingual Transfer Learning with Multi-lingual Language Representation Model》</a><h2 id="Why-does-BERT-work"><a href="#Why-does-BERT-work" class="headerlink" title="Why does BERT work?"></a>Why does BERT work?</h2>在BERT中，每一个token都被处理为了一个vector，这个vector叫做embedding，代表了这个token的意思。相近意思的token，其embedding也更加接近：<br><img src="images/2024/08/2030470944.png" alt="embedding"><br>可以看到，同一个token在不同的语境下，其embedding也不相同。这种考虑了语境（即上下文）的embedding叫做contextualized word embedding。一种可能的解释为，<strong>BERT在训练的过程中学到了token的意思</strong>，但一篇paper中用BERT做了蛋白质的分类，也起到了很好的效果，因此BERT在train中做的事情可能不止于此：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.07162">《Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models’ Transferability》</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/week7-Self-supervised-Learning-in-NLP.html" data-id="cm4zg7wze000lwe3k8ltkczkb" data-title="[week7]Self-supervised Learning in NLP" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/articles/week12-Reinforcement-Learning.html" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          [week12]Reinforcement Learning
        
      </div>
    </a>
  
  
    <a href="/articles/week6-Generation.html" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">[week6]Generation</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Algorithm/">Algorithm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Env-Setting/">Env Setting</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NTU-ML2022/">NTU-ML2022</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">December 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">October 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/articles/linux%E9%85%8D%E7%BD%AEgit%E5%B9%B6%E9%83%A8%E7%BD%B2%E5%88%B0github.html">linux配置git并部署到github</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E4%B8%8B%E7%94%A8qemu%E6%A8%A1%E6%8B%9Ffreedos%E7%BC%96%E5%86%9916%E4%BD%8D%E6%B1%87%E7%BC%96%E8%AF%AD%E8%A8%80.html">ubuntu下用qemu模拟freedos编写16位汇编语言</a>
          </li>
        
          <li>
            <a href="/articles/week12-extra-Q-learning.html">[week12 extra] Q-learning</a>
          </li>
        
          <li>
            <a href="/articles/ubuntu%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%A4%9A%E7%89%88%E6%9C%ACcuda%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E5%88%87%E6%8D%A2.html">ubuntu环境下多版本cuda的安装与切换</a>
          </li>
        
          <li>
            <a href="/articles/week12-Reinforcement-Learning.html">[week12]Reinforcement Learning</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 zz111y<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>zz111y&#39;s stack</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ">
<meta property="og:type" content="website">
<meta property="og:title" content="zz111y&#39;s stack">
<meta property="og:url" content="https://zz111y.github.io/index.html">
<meta property="og:site_name" content="zz111y&#39;s stack">
<meta property="og:description" content="ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="zz111y">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="zz111y's stack" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">zz111y&#39;s stack</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">ㄅㄆㄇㄈㄉㄊㄋㄌ巜ㄎㄏ</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://zz111y.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-Week1-Introduction-of-Deep-Learning" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/Week1-Introduction-of-Deep-Learning.html" class="article-date">
  <time class="dt-published" datetime="2024-07-12T16:07:00.000Z" itemprop="datePublished">2024-07-13</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/NTU-ML2022/">NTU-ML2022</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/Week1-Introduction-of-Deep-Learning.html">[Week1] Introduction of Deep Learning</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Machine-Learning"><a href="#Machine-Learning" class="headerlink" title="Machine Learning"></a>Machine Learning</h1><p>ML就是让机器<strong>找到一个function</strong>。<br>例如：向function输入一段语音，输出语音对应的文字；向function输入一张图片，输出图片是什么；向function输入一张棋盘，输出下一步最佳落子位置……</p>
<p>根据function的不同，ML主要分为两种：<strong>Regression</strong>和<strong>Classification</strong></p>
<ul>
<li>Regression：function的output是一个<strong>scalar</strong>；</li>
<li>Classification：function的output是一个给定的<strong>option(class)</strong>。</li>
</ul>
<p>但在简单的Regression和Classification之外，ML还有更为广泛的领域：<strong>Structured Learning</strong>，也即让机器学会创作，产生一个<strong>有结构的事物</strong>，例如一篇文章，一张图……</p>
<h2 id="如何实现ML"><a href="#如何实现ML" class="headerlink" title="如何实现ML"></a>如何实现ML</h2><p>要实现ML，通常需要以下步骤：</p>
<h3 id="一、构建function"><a href="#一、构建function" class="headerlink" title="一、构建function"></a>一、构建function</h3><p>我们往往需要根据对应领域的知识，构建出一个<strong>带有未知parameters的function</strong>。以$y=b+wx_1$为例，该functino就叫做一个<strong>Model</strong>，其中$x_1$作为已知量输入，叫做<strong>feature</strong>，w（weight)和b（bias）就是未知的parameters。</p>
<h3 id="二、定义Loss"><a href="#二、定义Loss" class="headerlink" title="二、定义Loss"></a>二、定义Loss</h3><p>Loss是一个<strong>关于parameters的function</strong>。Loss通过比较Model的output与真实值之间的差异，来表示某一组parameter的好坏。常见的Loss定义为：$L=\frac{1}{N}\sum\limits_ne_n$，其中e可以是$|y-\hat{y}|$（此时L叫做mean absolute error，MAE）或$(y-\hat{y})^2$（此时L叫做mean square error，MSE）等，需要根据具体情况进行选择。例如如果$y$和$\hat{y}$是一个概率分布的时候，往往选择Cross-entorpy。</p>
<h3 id="三、Optimization"><a href="#三、Optimization" class="headerlink" title="三、Optimization"></a>三、Optimization</h3><ul>
<li>目标：找到$w^<em>,b^</em>=\mathop{\arg\min}\limits_{w,b}L$。</li>
<li>方法：<strong>Gradient Descent</strong>（暂不考虑local optimum）。</li>
<li>步骤（以$w$为例）：<ol>
<li>随机选取一组随机初始值$w^0,b^0$；</li>
<li>计算$\frac{\partial L}{\partial w}|_{w=w^0}$；</li>
<li>为了让Loss的值变小，则更新$w$为：$w^1=w^0-\eta \frac{\partial L}{\partial w}|_{w=w^0}$，其中$\eta$叫做<strong>learning rate</strong>，是一个<strong>Hyperparameter</strong></li>
<li>迭代进行2、3两步，直到终止（不想继续进行了，或者计算出$\frac{\partial L}{\partial w}|_{w=w^0}=0$）</li>
</ol>
</li>
</ul>
<p><strong>Hyperparameter</strong>：人为设置的参数，不是机器需要学习的parameter</p>
<h2 id="更加复杂的情况"><a href="#更加复杂的情况" class="headerlink" title="更加复杂的情况"></a>更加复杂的情况</h2><h3 id="Sophisticated-Model"><a href="#Sophisticated-Model" class="headerlink" title="Sophisticated Model"></a>Sophisticated Model</h3><p>实际中，function往往并非线性，而是十分复杂的，这时，应用简单的线性Model并不能解决问题，我们需要更加复杂的Model。在ML中，解决方法一般如下：<br><img src="images/2024/07/3743548032.png" alt="sophisticated Model"><br>可以看到，要获得一个如图所示的红色曲线，需要三个类似形状的<strong>蓝色曲线</strong>（hard-sigmoid function）和一个<strong>常数项</strong>相加得到。以此为例，不难得出：任何一个复杂的函数，都可以由多个如图所示的蓝色曲线和一个常数项相加<strong>近似得到</strong>。</p>
<h3 id="sigmoid-function"><a href="#sigmoid-function" class="headerlink" title="sigmoid function"></a>sigmoid function</h3><p>可是，由于蓝色曲线并非光滑的，所以对其进行Optimization非常困难，因此，这里采用<strong>sigmoid函数</strong>去逼近蓝色曲线。<strong>sigmoid函数</strong>如下图所示：<br><img src="images/2024/07/548121614.png" alt="Structure of CIFAR10-quick model"><br>其函数表达式为：$y=c\frac{1}{1+e^{-b+wx_1}}$，简单记作$y=c sigmoid(b+wx_1)$。<br>那么，上面的sophisticated Model就可以转化为：$y=b+\sum\limits_ic_isigmoid(b_i+w_ix_1)$<br>对于一个sigmoid function来说，改变其参数可以得到不同形状的sigmoid function：<br><img src="images/2024/07/1242558349.png" alt="different sigmoid function"><br>有了这些<strong>不同的sigmoid function</strong>，将其叠加后就可以<strong>逼近不同的non-linear Model</strong>。</p>
<h3 id="multi-feature-Model"><a href="#multi-feature-Model" class="headerlink" title="multi-feature Model"></a>multi-feature Model</h3><p>到目前为止，这样的Model还有一个不足：无法应对多feature的情况。对此，我们将其进行改进，给Model<strong>添加多个feature</strong>后，得到如下Model：$y=b+\sum\limits_ic_isigmoid(b_i+\sum\limits_jw_{ij}x_{j})$，其中$j$为feature的编号。<br>假设某Model需要三个sigmoid function，那么我们可以通过下图<strong>直观的观察到其中的关系</strong>：<br><img src="images/2024/07/4222411239.png" alt=""><br>接下来，使用<strong>矩阵</strong>来<strong>简洁的表达其中的运算</strong>：</p>
<script type="math/tex; mode=display">
\begin{equation}
\begin{bmatrix}
 r_1 \\ r_2 \\ r_3
\end{bmatrix}
=
\begin{bmatrix}
 b_1 \\ b_2 \\ b_3
\end{bmatrix}
+
\begin{bmatrix}
 w_{11} & w_{12} & w_{13} \\ w_{21} & w_{22} & w_{23} \\ w_{31} & w_{32} & w_{33}
\end{bmatrix}
\begin{bmatrix}
 x_1 \\ x_2 \\ x_3
\end{bmatrix}
\end{equation}</script><p>用符号表示为：$r=b+Wx$<br>整个Model由下图给出：<br><img src="images/2024/07/257902148.png" alt=""></p>
<h3 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h3><p>对于$y=b+c^Ta$中所有未知的parameter（包括$b、c、W、b$），将其中元素拼成一个列向量</p>
<script type="math/tex; mode=display">
\begin{equation}
\theta=
\begin{bmatrix}
 \theta_1 \\ \theta_2 \\ \vdots
\end{bmatrix}
\end{equation}</script><p>要进行Optimization，即找到$\theta^*=\mathop{\arg\min}\limits_\theta L$。步骤和最基本的Model类似，先随机选取一个初始值$\theta^0$，然后计算</p>
<script type="math/tex; mode=display">
\begin{equation}
g=
\begin{bmatrix}
\frac{\partial L}{\partial \theta_1}|_{\theta=\theta^0} \\
\frac{\partial L}{\partial \theta_2}|_{\theta=\theta^0} \\
\vdots
\end{bmatrix}
\end{equation}</script><p>记作$g=\nabla L(\theta^0)$。最后更新参数即可：$\theta^1=\theta^0-\eta g$</p>
<h3 id="train-as-batch"><a href="#train-as-batch" class="headerlink" title="train as batch"></a>train as batch</h3><p>实际Optimization的过程中，往往将train data<strong>分为多个batch</strong>，记每个batch对parameter的一次Optimization为<strong>一次update</strong>，所有batch串行进行一次update叫做<strong>一个epoch</strong>。如下图所示：<br><img src="images/2024/07/425677454.png" alt="epoch-update"><br>那么<strong>batch size和epoch</strong>也就成为了<strong>Hyperparameter</strong>。</p>
<h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>这样的Model有一个变体，我们使用<strong>ReLU</strong>（Rectified Linear Unit）来代替sigmoid function，ReLU如下图所示：<br><img src="images/2024/07/2218813359.png" alt="ReLU"><br>只要将两个ReLU叠加，就可以得到hard-sigmoid funccion：<br><img src="images/2024/07/2679937844.png" alt=""><br>那么对于上述Model，就可以表示为：$y=b+\sum\limits_{2i}c_imax(b_i+\sum\limits_jw_{ij}x_{j})$</p>
<p>对于sigmoid function和ReLU这样的函数，我们统称为<strong>Activate function</strong>。</p>
<h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><p>我们可以将上述Model进行套娃，如下图所示：<br><img src="images/2024/07/548161856.png" alt=""><br>每个Activate function叫做一个<strong>Neuron</strong>，所有Neuron的总和叫做<strong>Neural Network</strong>；每一列Neuron叫做一个<strong>hidden layer</strong>，<strong>包含多个hidden layer的Model</strong>即是<strong>Deep Learning</strong>。<br><img src="images/2024/07/1753846494.png" alt="Deep Learning"><br>其中，$x_1,x_2,…$叫做<strong>Input Layer</strong>，Output前的最后一层叫做<strong>Output Layer</strong>。</p>
<p>大多数情况下，Deep Learning的表现要比单层的Model更好。但hidden layer过多会导致<strong>over fitness</strong>。</p>
<p>network的structure是非常重要的，需要自己设计好stucture，才能获得好的效果。设计的依据：试错+直觉。</p>
<hr>
<p>那么，相对于Machine Learning，Deep Learning究竟做了什么呢？<br>其实就是<strong>把一个问题转化为了另一个问题</strong>。对于ML，往往需要<strong>找一组好的feature</strong>，这在有些场景下是非常困难的。但在DL中，只需要将一组不是那么好的feature放到一个<strong>设计好的structure</strong>中，就能得到较好的效果。因此问题从<strong>find feature</strong>转化为了<strong>design a network</strong>，而DL和ML的优劣，就归结于<strong>这两个问题哪个较为容易解决</strong>（例如在NLP领域，抽一组好的feature往往是更容易的；而对于影像辨识和语音辨识，抽一组好的feature是比较困难的）。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/Week1-Introduction-of-Deep-Learning.html" data-id="cm4zd1vkq00006r3kf3ul4lw2" data-title="[Week1] Introduction of Deep Learning" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-test" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/articles/test.html" class="article-date">
  <time class="dt-published" datetime="2024-06-29T03:20:00.000Z" itemprop="datePublished">2024-06-29</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/DeepLearning/">DeepLearning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/articles/test.html">训练一个简单网络模型的方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>以torchvision中的CIFAR10数据集为例，介绍一个简单网络模型的训练方法。</p>
<h2 id="CIFAR10介绍"><a href="#CIFAR10介绍" class="headerlink" title="CIFAR10介绍"></a>CIFAR10介绍</h2><p>CIFAR10是torchvision中给出的数据集，包含了60000张32x32的3通道彩色图像，这些图片被分为了10类，每类有6000张。其中，train集有50000张，test集有10000张。<a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~kriz/cifar.html">官方文档</a></p>
<h2 id="网络模型构建"><a href="#网络模型构建" class="headerlink" title="网络模型构建"></a>网络模型构建</h2><p>这里以现有模型结构进行构建，以下是现有模型结构的示意图（来源：<a target="_blank" rel="noopener" href="https://www.researchgate.net/figure/Structure-of-CIFAR10-quick-model_fig2_312170477">Structure of CIFAR10-quick model.</a>）<br><img src="images/2024/06/3798127767.png" alt="Structure of CIFAR10-quick model"><br>在此模型中，输入以3channel 32x32的格式输入，依次经过以下变化过程：</p>
<ol>
<li>经过一个<strong>5x5 kernel的Convolution</strong>，转化为32channel 32x32；</li>
<li>经过一个<strong>2x2 kernel的Max-Pooling</strong>，转化为32channel 16x16；</li>
<li>经过一个<strong>5x5 kernel的Convolution</strong>，转化为32channel 32x32；</li>
<li>经过一个<strong>2x2 kernel的Max-Pooling</strong>，转化为32channel 8x8；</li>
<li>经过一个<strong>5x5 kernel的Convolution</strong>，转化为64channel 8x8；</li>
<li>经过一个<strong>2x2 kernel的Max-Pooling</strong>，转化为64channel 4x4；</li>
<li>经过<strong>flatten</strong>操作后，展平为一个64x4x4的线性数据；</li>
<li>经过一个<strong>Linear</strong>，输出为64；</li>
<li>经过一个<strong>Linear</strong>，输出为10（也即<strong>对应类别的概率</strong>）。</li>
</ol>
<p>根据以上模型的构建，可以定义一个model类（需要继承nn.module)，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">testModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(testModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><br>[scode type=”yellow”]模型的输入必须是一个<strong>3 channel 32x32的batch</strong>。如果不满足尺寸要求或不是一个batch，都将报错！[/scode]</p>
<h2 id="数据集的获取"><a href="#数据集的获取" class="headerlink" title="数据集的获取"></a>数据集的获取</h2><p>使用torchvision中的datasets可以获取指定模型。代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>对参数的理解为：</p>
<ul>
<li>root：数据集存放的位置；</li>
<li>train：为True时加载train数据集，为False时加载test数据集；</li>
<li>transform：对数据集中每个元素进行的初始化操作，这里将每个元素转化为了Tensor；</li>
<li>download：为True时，当本地不存在数据集时，会自动下载；为False时，不会自动下载。</li>
</ul>
<p>然后，将数据集分为多个batch，代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><br>至此，数据集初始化部分完成。</p>
<h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><p>首先定义简单模型参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">model = testModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数——交叉熵</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 学习率和优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 当前epoch和总epoch</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义tensorboard，记录训练过程</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br></pre></td></tr></table></figure><br>模型训练的步骤如下（对于每个epoch）：<br>首先将模型调整为train模式，对于某些层（如Dropout和BatchNorm）在train模式和eval模式下作为会有所不同，因此好的习惯是不管包含哪些层，都注意train模式和eval模式的改变<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br></pre></td></tr></table></figure><br>然后对训练集dataLoader中的每一个batch进行训练：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">    <span class="comment"># 取出一个batch的数据</span></span><br><span class="line">    imgs, label = data</span><br><span class="line">    <span class="comment"># 将batch传给模型，得到batch的输出</span></span><br><span class="line">    outputs = model(imgs)</span><br><span class="line">    <span class="comment"># 计算此batch的损失</span></span><br><span class="line">    loss = loss_fn(outputs, label)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 清空梯度，防止梯度积累</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 反向传播，计算损失对模型参数的梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 根据计算得到的梯度更新模型参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 写入tensorboard</span></span><br><span class="line">    total_train_step += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;, train loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, epochs, loss.item()))</span><br><span class="line">        writer.add_scalar(<span class="string">&quot;loss&quot;</span>, loss.item(), total_train_step)</span><br></pre></td></tr></table></figure><br>模型验证的步骤如下（对于每个epoch）：<br>首先将模型调整为train模式：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure><br>然后对测试集dataloader中每一个batch进行验证：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定当前batch总损失和准确率</span></span><br><span class="line">total_test_loss = <span class="number">0</span></span><br><span class="line">total_test_accuracy = <span class="number">0</span></span><br><span class="line"><span class="comment"># 验证时不需要反向传播，禁用梯度计算，加快训练速度</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        img, label = data</span><br><span class="line">        outputs = model(img)</span><br><span class="line">        loss = loss_fn(outputs, label)</span><br><span class="line">        total_test_loss += loss.item()</span><br><span class="line">        <span class="comment"># 计算当前批次中预测正确的样本数</span></span><br><span class="line">        accuracy = (outputs.argmax(<span class="number">1</span>) == label).<span class="built_in">sum</span>()</span><br><span class="line">        total_test_accuracy += accuracy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入tensorboard</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total test loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;total test accuracy: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_accuracy))</span><br><span class="line">writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_train_step)</span><br><span class="line">writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_test_accuracy, total_train_step)</span><br><span class="line">total_test_step += <span class="number">1</span></span><br></pre></td></tr></table></figure><br>最后，保存模型。这里我们每个epoch保存一个模型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, <span class="string">&quot;test_model&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;save model&quot;</span>)</span><br></pre></td></tr></table></figure><br>至此，一个简单模型的训练结束。下面给出整体代码：<br>model.py文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">testModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(testModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.model = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.model(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = testModel()</span><br><span class="line">    _<span class="built_in">input</span> = torch.ones((<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">    output = model(_<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure><br>train.py文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">True</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                          download=<span class="literal">True</span>)</span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./data&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                         download=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Train data size:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test data size:&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line">train_loader = DataLoader(train_data, batch_size=<span class="number">64</span>)</span><br><span class="line">test_loader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">model = testModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning_rate = <span class="number">1e-2</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;./logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, epochs))</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_loader:</span><br><span class="line">        imgs, label = data</span><br><span class="line">        outputs = model(imgs)</span><br><span class="line">        loss = loss_fn(outputs, label)</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_train_step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Epoch &#123;&#125;/&#123;&#125;, train loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(i + <span class="number">1</span>, epochs, loss.item()))</span><br><span class="line">            writer.add_scalar(<span class="string">&quot;loss&quot;</span>, loss.item(), total_train_step)</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_test_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            img, label = data</span><br><span class="line">            outputs = model(img)</span><br><span class="line">            loss = loss_fn(outputs, label)</span><br><span class="line">            total_test_loss += loss.item()</span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == label).<span class="built_in">sum</span>()</span><br><span class="line">            total_test_accuracy += accuracy</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total test loss: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total test accuracy: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_accuracy))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>, total_test_loss, total_train_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>, total_test_accuracy, total_train_step)</span><br><span class="line">    total_test_step += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    torch.save(model, <span class="string">&quot;test_model&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;save model&quot;</span>)</span><br><span class="line"></span><br><span class="line">writer.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="利用gpu进行训练"><a href="#利用gpu进行训练" class="headerlink" title="利用gpu进行训练"></a>利用gpu进行训练</h2><p>运行上面程序效率较低，这是因为我们单纯使用cpu进行计算。cpu对于处理大规模简单计算的效率远远不如gpu，因此接下来介绍使用gpu的训练方法。此方法需要设备配备nvidia显卡、安装cuda以及cudnn。<br>检测当前环境是否能使用gpu进行训练，使用以下语句：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><br>支持gpu训练，返回True，否则返回False。</p>
<h3 id="方法1"><a href="#方法1" class="headerlink" title="方法1"></a>方法1</h3><p>要使用gpu进行训练，需要对上述代码作出以下修改：<br>模型初始化部分原始代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = testModel()</span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><br>利用gpu版本的代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model = testModel()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    loss_fn = loss_fn.cuda()</span><br></pre></td></tr></table></figure><br>模型训练部分及验证部分原始代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img, label = data</span><br></pre></td></tr></table></figure><br>利用gpu版本代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">imgs, label = data</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    imgs = imgs.cuda()</span><br><span class="line">    label = label.cuda()</span><br></pre></td></tr></table></figure></p>
<h3 id="方法2"><a href="#方法2" class="headerlink" title="方法2"></a>方法2</h3><p>首先指定训练设备：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span>)</span><br></pre></td></tr></table></figure><br>然后在对应部分加上如下代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型定义部分</span></span><br><span class="line">model = model.to(device)</span><br><span class="line"><span class="comment"># 损失函数部分</span></span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"><span class="comment"># 获取dataloader中batch部分</span></span><br><span class="line">imgs = imgs.to(device)</span><br><span class="line">label = label.to(device)</span><br></pre></td></tr></table></figure><br>使用gpu加速训练后，可以发现，训练效率明显加速。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://zz111y.github.io/articles/test.html" data-id="cm4zannya0000ip3k3zj3547u" data-title="训练一个简单网络模型的方法" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NTU-ML2022/">NTU-ML2022</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/07/">July 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/06/">June 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/articles/Week1-Introduction-of-Deep-Learning.html">[Week1] Introduction of Deep Learning</a>
          </li>
        
          <li>
            <a href="/articles/test.html">训练一个简单网络模型的方法</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 zz111y<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>